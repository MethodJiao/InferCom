{"prompt": "import asyncio\n\nfrom app.logger import logger\n\n\nasync def main():\n    # Create and initialize Manus agent\n    agent = await ", "metadata": {"task_id": "OpenManus/1", "ground_truth": "Manus.create()\n", "fpath_tuple": ["OpenManus", "main.py"], "context_start_lineno": 0, "line_no": 8, "col_no": 18, "import_no": [2, 2]}}
{"prompt": "import asyncio\nimport time\n\nfrom app.agent.manus import Manus\nfrom app.config import config\nfrom app.flow.flow_factory import FlowFactory, FlowType\nfrom app.logger import logger\n\n\nasync def run_flow():\n    agents = {\n        \"manus\": Manus(),\n    }\n    if config.run_flow_config.use_data_analysis_agent:\n", "metadata": {"task_id": "OpenManus/2", "ground_truth": "        agents[\"data_analysis\"] = DataAnalysis()\n", "fpath_tuple": ["OpenManus", "run_flow.py"], "context_start_lineno": 0, "line_no": 15, "col_no": -1, "import_no": [3, 3]}}
{"prompt": "            except KeyError:\n                # If the model is not in tiktoken's presets, use cl100k_base as default\n                self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n\n            if self.api_type == \"azure\":\n                self.client = AsyncAzureOpenAI(\n                    base_url=self.base_url,\n                    api_key=self.api_key,\n                    api_version=self.api_version,\n                )\n            elif self.api_type == \"aws\":\n                self.client = BedrockClient()\n            else:\n                self.client = AsyncOpenAI(api_key=self.api_key, base_url=self.base_url)\n\n            self.token_counter = TokenCounter(self.tokenizer)\n\n    def count_tokens(self, text: str) -> int:\n        \"\"\"Calculate the number of tokens in a text\"\"\"\n        if not text:\n            return 0\n        return len(self.tokenizer.encode(text))\n\n    def count_message_tokens(self, messages: List[dict]) -> int:\n        return self.token_counter.count_message_tokens(messages)\n\n    def update_token_count(self, input_tokens: int, completion_tokens: int = 0) -> None:\n        \"\"\"Update token counts\"\"\"\n        # Only track tokens if max_input_tokens is set\n        self.total_input_tokens += input_tokens\n        self.total_completion_tokens += completion_tokens\n        logger.info(\n            f\"Token usage: Input={input_tokens}, Completion={completion_tokens}, \"\n            f\"Cumulative Input={self.total_input_tokens}, Cumulative Completion={self.total_completion_tokens}, \"\n            f\"Total={input_tokens + completion_tokens}, Cumulative Total={self.total_input_tokens + self.total_completion_tokens}\"\n        )\n\n    def check_token_limit(self, input_tokens: int) -> bool:\n        \"\"\"Check if token limits are exceeded\"\"\"\n        if self.max_input_tokens is not None:\n            return (self.total_input_tokens + input_tokens) <= self.max_input_tokens\n        # If max_input_tokens is not set, always return True\n        return True\n\n    def get_limit_error_message(self, input_tokens: int) -> str:\n        \"\"\"Generate error message for token limit exceeded\"\"\"\n        if (\n            self.max_input_tokens is not None\n            and (self.total_input_tokens + input_tokens) > self.max_input_tokens\n        ):\n            return f\"Request may exceed input token limit (Current: {self.total_input_tokens}, Needed: {input_tokens}, Max: {self.max_input_tokens})\"\n\n        return \"Token limit exceeded\"\n\n    @staticmethod\n    def format_messages(\n        messages: List[Union[dict, Message]], supports_images: bool = False\n    ) -> List[dict]:\n        \"\"\"\n        Format messages for LLM by converting them to OpenAI message format.\n\n        Args:\n            messages: List of messages that can be either dict or Message objects\n            supports_images: Flag indicating if the target model supports image inputs\n\n        Returns:\n            List[dict]: List of formatted messages in OpenAI format\n\n        Raises:\n            ValueError: If messages are invalid or missing required fields\n            TypeError: If unsupported message types are provided\n\n        Examples:\n            >>> msgs = [\n            ...     Message.system_message(\"You are a helpful assistant\"),\n            ...     {\"role\": \"user\", \"content\": \"Hello\"},\n            ...     Message.user_message(\"How are you?\")\n            ... ]\n            >>> formatted = LLM.format_messages(msgs)\n        \"\"\"\n        formatted_messages = []\n\n        for message in messages:\n            # Convert Message objects to dictionaries\n            if isinstance(message, Message):\n                message = message.to_dict()\n\n            if isinstance(message, dict):\n                # If message is a dict, ensure it has required fields\n                if \"role\" not in message:\n                    raise ValueError(\"Message dict must contain 'role' field\")\n\n                # Process base64 images if present and model supports images\n                if supports_images and message.get(\"base64_image\"):\n                    # Initialize or convert content to appropriate format\n                    if not message.get(\"content\"):\n                        message[\"content\"] = []\n                    elif isinstance(message[\"content\"], str):\n                        message[\"content\"] = [\n                            {\"type\": \"text\", \"text\": message[\"content\"]}\n                        ]\n                    elif isinstance(message[\"content\"], list):\n                        # Convert string items to proper text objects\n                        message[\"content\"] = [\n                            (\n                                {\"type\": \"text\", \"text\": item}\n                                if isinstance(item, str)\n                                else item\n                            )\n                            for item in message[\"content\"]\n                        ]\n\n                    # Add the image to content\n                    message[\"content\"].append(\n                        {\n                            \"type\": \"image_url\",\n                            \"image_url\": {\n                                \"url\": f\"data:image/jpeg;base64,{message['base64_image']}\"\n                            },\n                        }\n                    )\n\n                    # Remove the base64_image field\n                    del message[\"base64_image\"]\n                # If model doesn't support images but message has base64_image, handle gracefully\n                elif not supports_images and message.get(\"base64_image\"):\n                    # Just remove the base64_image field and keep the text content\n                    del message[\"base64_image\"]\n\n                if \"content\" in message or \"tool_calls\" in message:\n                    formatted_messages.append(message)\n                # else: do not include the message\n            else:\n                raise TypeError(f\"Unsupported message type: {type(message)}\")\n\n        # Validate all messages have required fields\n        for msg in formatted_messages:\n            if msg[\"role\"] not in ROLE_VALUES:\n                raise ValueError(f\"Invalid role: {msg['role']}\")\n\n        return formatted_messages\n\n    @retry(\n        wait=wait_random_exponential(min=1, max=60),\n        stop=stop_after_attempt(6),\n        retry=retry_if_exception_type(\n            (OpenAIError, Exception, ValueError)\n        ),  # Don't retry TokenLimitExceeded\n    )\n    async def ask(\n        self,\n        messages: List[Union[dict, Message]],\n        system_msgs: Optional[List[Union[dict, Message]]] = None,\n        stream: bool = True,\n        temperature: Optional[float] = None,\n    ) -> str:\n        \"\"\"\n        Send a prompt to the LLM and get the response.\n\n        Args:\n            messages: List of conversation messages\n            system_msgs: Optional system messages to prepend\n            stream (bool): Whether to stream the response\n            temperature (float): Sampling temperature for the response\n\n        Returns:\n            str: The generated response\n\n        Raises:\n            TokenLimitExceeded: If token limits are exceeded\n            ValueError: If messages are invalid or response is empty\n            OpenAIError: If API call fails after retries\n            Exception: For unexpected errors\n        \"\"\"\n        try:\n            # Check if the model supports images\n            supports_images = self.model in MULTIMODAL_MODELS\n\n            # Format system and user messages with image support check\n            if system_msgs:\n                system_msgs = self.format_messages(system_msgs, supports_images)\n                messages = system_msgs + self.format_messages(messages, supports_images)\n            else:\n                messages = self.format_messages(messages, supports_images)\n\n            # Calculate input token count\n            input_tokens = self.count_message_tokens(messages)\n\n            # Check if token limits are exceeded\n            if not self.check_token_limit(input_tokens):\n                error_message = self.get_limit_error_message(input_tokens)\n                # Raise a special exception that won't be retried\n                raise ", "metadata": {"task_id": "OpenManus/3", "ground_truth": "TokenLimitExceeded(error_message)\n", "fpath_tuple": ["OpenManus", "app", "llm.py"], "context_start_lineno": 210, "line_no": 403, "col_no": 22, "import_no": [22, 22]}}
{"prompt": "import asyncio\nfrom typing import Any, Dict, List, Optional\n\nimport requests\nfrom bs4 import BeautifulSoup\nfrom pydantic import BaseModel, ConfigDict, Field, model_validator\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nfrom app.config import config\nfrom app.logger import logger\nfrom app.tool.base import BaseTool, ToolResult\nfrom app.tool.search.base import SearchItem\n\n\nclass SearchResult(BaseModel):\n    \"\"\"Represents a single search result returned by a search engine.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    position: int = Field(description=\"Position in search results\")\n    url: str = Field(description=\"URL of the search result\")\n    title: str = Field(default=\"\", description=\"Title of the search result\")\n    description: str = Field(\n        default=\"\", description=\"Description or snippet of the search result\"\n    )\n    source: str = Field(description=\"The search engine that provided this result\")\n    raw_content: Optional[str] = Field(\n        default=None, description=\"Raw content from the search result page if available\"\n    )\n\n    def __str__(self) -> str:\n        \"\"\"String representation of a search result.\"\"\"\n        return f\"{self.title} ({self.url})\"\n\n\nclass SearchMetadata(BaseModel):\n    \"\"\"Metadata about the search operation.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    total_results: int = Field(description=\"Total number of results found\")\n    language: str = Field(description=\"Language code used for the search\")\n    country: str = Field(description=\"Country code used for the search\")\n\n\nclass SearchResponse(ToolResult):\n    \"\"\"Structured response from the web search tool, inheriting ToolResult.\"\"\"\n\n    query: str = Field(description=\"The search query that was executed\")\n    results: List[SearchResult] = Field(\n        default_factory=list, description=\"List of search results\"\n    )\n    metadata: Optional[SearchMetadata] = Field(\n        default=None, description=\"Metadata about the search\"\n    )\n\n    @model_validator(mode=\"after\")\n    def populate_output(self) -> \"SearchResponse\":\n        \"\"\"Populate output or error fields based on search results.\"\"\"\n        if self.error:\n            return self\n\n        result_text = [f\"Search results for '{self.query}':\"]\n\n        for i, result in enumerate(self.results, 1):\n            # Add title with position number\n            title = result.title.strip() or \"No title\"\n            result_text.append(f\"\\n{i}. {title}\")\n\n            # Add URL with proper indentation\n            result_text.append(f\"   URL: {result.url}\")\n\n            # Add description if available\n            if result.description.strip():\n                result_text.append(f\"   Description: {result.description}\")\n\n            # Add content preview if available\n            if result.raw_content:\n                content_preview = result.raw_content[:1000].replace(\"\\n\", \" \").strip()\n                if len(result.raw_content) > 1000:\n                    content_preview += \"...\"\n                result_text.append(f\"   Content: {content_preview}\")\n\n        # Add metadata at the bottom if available\n        if self.metadata:\n            result_text.extend(\n                [\n                    f\"\\nMetadata:\",\n                    f\"- Total results: {self.metadata.total_results}\",\n                    f\"- Language: {self.metadata.language}\",\n                    f\"- Country: {self.metadata.country}\",\n                ]\n            )\n\n        self.output = \"\\n\".join(result_text)\n        return self\n\n\nclass WebContentFetcher:\n    \"\"\"Utility class for fetching web content.\"\"\"\n\n    @staticmethod\n    async def fetch_content(url: str, timeout: int = 10) -> Optional[str]:\n        \"\"\"\n        Fetch and extract the main content from a webpage.\n\n        Args:\n            url: The URL to fetch content from\n            timeout: Request timeout in seconds\n\n        Returns:\n            Extracted text content or None if fetching fails\n        \"\"\"\n        headers = {\n            \"WebSearch\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n        }\n\n        try:\n            # Use asyncio to run requests in a thread pool\n            response = await asyncio.get_event_loop().run_in_executor(\n                None, lambda: requests.get(url, headers=headers, timeout=timeout)\n            )\n\n            if response.status_code != 200:\n                logger.warning(\n                    f\"Failed to fetch content from {url}: HTTP {response.status_code}\"\n                )\n                return None\n\n            # Parse HTML with BeautifulSoup\n            soup = BeautifulSoup(response.text, \"html.parser\")\n\n            # Remove script and style elements\n            for script in soup([\"script\", \"style\", \"header\", \"footer\", \"nav\"]):\n                script.extract()\n\n            # Get text content\n            text = soup.get_text(separator=\"\\n\", strip=True)\n\n            # Clean up whitespace and limit size (100KB max)\n            text = \" \".join(text.split())\n            return text[:10000] if text else None\n\n        except Exception as e:\n            logger.warning(f\"Error fetching content from {url}: {e}\")\n            return None\n\n\nclass WebSearch(BaseTool):\n    \"\"\"Search the web for information using various search engines.\"\"\"\n\n    name: str = \"web_search\"\n    description: str = \"\"\"Search the web for real-time information about any topic.\n    This tool returns comprehensive search results with relevant information, URLs, titles, and descriptions.\n    If the primary search engine fails, it automatically falls back to alternative engines.\"\"\"\n    parameters: dict = {\n        \"type\": \"object\",\n        \"properties\": {\n            \"query\": {\n                \"type\": \"string\",\n                \"description\": \"(required) The search query to submit to the search engine.\",\n            },\n            \"num_results\": {\n                \"type\": \"integer\",\n                \"description\": \"(optional) The number of search results to return. Default is 5.\",\n                \"default\": 5,\n            },\n            \"lang\": {\n                \"type\": \"string\",\n                \"description\": \"(optional) Language code for search results (default: en).\",\n                \"default\": \"en\",\n            },\n            \"country\": {\n                \"type\": \"string\",\n                \"description\": \"(optional) Country code for search results (default: us).\",\n                \"default\": \"us\",\n            },\n            \"fetch_content\": {\n                \"type\": \"boolean\",\n                \"description\": \"(optional) Whether to fetch full content from result pages. Default is false.\",\n                \"default\": False,\n            },\n        },\n        \"required\": [\"query\"],\n    }\n    _search_engine: dict[str, WebSearchEngine] = {\n        \"google\": GoogleSearchEngine(),\n", "metadata": {"task_id": "OpenManus/4", "ground_truth": "        \"baidu\": BaiduSearchEngine(),\n", "fpath_tuple": ["OpenManus", "app", "tool", "web_search.py"], "context_start_lineno": 0, "line_no": 194, "col_no": -1, "import_no": [11, 17]}}
{"prompt": "\"\"\"Collection classes for managing multiple tools.\"\"\"\nfrom typing import Any, Dict, List\n\nfrom app.exceptions import ToolError\nfrom app.logger import logger\n\n\nclass ToolCollection:\n    \"\"\"A collection of defined tools.\"\"\"\n\n    class Config:\n        arbitrary_types_allowed = True\n\n    def __init__(self, *tools: BaseTool):\n        self.tools = tools\n        self.tool_map = {tool.name: tool for tool in tools}\n\n    def __iter__(self):\n        return iter(self.tools)\n\n    def to_params(self) -> List[Dict[str, Any]]:\n        return [tool.to_param() for tool in self.tools]\n\n    async def execute(\n        self, *, name: str, tool_input: Dict[str, Any] = None\n    ) -> ToolResult:\n        tool = self.tool_map.get(name)\n        if not tool:\n            return ", "metadata": {"task_id": "OpenManus/5", "ground_truth": "ToolFailure(error=f\"Tool {name} is invalid\")\n", "fpath_tuple": ["OpenManus", "app", "tool", "tool_collection.py"], "context_start_lineno": 0, "line_no": 29, "col_no": 19, "import_no": [5, 5]}}
{"prompt": "from contextlib import AsyncExitStack\nfrom typing import Dict, List, Optional\n\nfrom mcp.client.sse import sse_client\nfrom mcp.client.stdio import stdio_client\nfrom mcp.types import ListToolsResult, TextContent\n\nfrom app.logger import logger\nfrom app.tool.base import BaseTool, ToolResult\nfrom app.tool.tool_collection import ToolCollection\n\n\nclass MCPClientTool(BaseTool):\n    \"\"\"Represents a tool proxy that can be called on the MCP server from the client side.\"\"\"\n\n    session: Optional[ClientSession] = None\n    server_id: str = \"\"  # Add server identifier\n    original_name: str = \"\"\n\n    async def execute(self, **kwargs) -> ToolResult:\n        \"\"\"Execute the tool by making a remote call to the MCP server.\"\"\"\n        if not self.session:\n            return ToolResult(error=\"Not connected to MCP server\")\n\n        try:\n            logger.info(f\"Executing tool: {self.original_name}\")\n            result = await self.session.call_tool(self.original_name, kwargs)\n            content_str = \", \".join(\n                item.text for item in result.content if isinstance(item, TextContent)\n            )\n            return ToolResult(output=content_str or \"No output returned.\")\n        except Exception as e:\n            return ToolResult(error=f\"Error executing tool: {str(e)}\")\n\n\nclass MCPClients(ToolCollection):\n    \"\"\"\n    A collection of tools that connects to multiple MCP servers and manages available tools through the Model Context Protocol.\n    \"\"\"\n\n    sessions: Dict[str, ClientSession] = {}\n    exit_stacks: Dict[str, AsyncExitStack] = {}\n    description: str = \"MCP client tools for server interaction\"\n\n    def __init__(self):\n        super().__init__()  # Initialize with empty tools list\n        self.name = \"mcp\"  # Keep name for backward compatibility\n\n    async def connect_sse(self, server_url: str, server_id: str = \"\") -> None:\n        \"\"\"Connect to an MCP server using SSE transport.\"\"\"\n        if not server_url:\n            raise ValueError(\"Server URL is required.\")\n\n        server_id = server_id or server_url\n\n        # Always ensure clean disconnection before new connection\n        if server_id in self.sessions:\n            await self.disconnect(server_id)\n\n        exit_stack = AsyncExitStack()\n        self.exit_stacks[server_id] = exit_stack\n\n        streams_context = sse_client(url=server_url)\n        streams = await exit_stack.enter_async_context(streams_context)\n        session = await exit_stack.enter_async_context(ClientSession(*streams))\n        self.sessions[server_id] = session\n\n        await self._initialize_and_list_tools(server_id)\n\n    async def connect_stdio(\n        self, command: str, args: List[str], server_id: str = \"\"\n    ) -> None:\n        \"\"\"Connect to an MCP server using stdio transport.\"\"\"\n        if not command:\n            raise ValueError(\"Server command is required.\")\n\n        server_id = server_id or command\n\n        # Always ensure clean disconnection before new connection\n        if server_id in self.sessions:\n            await self.disconnect(server_id)\n\n        exit_stack = AsyncExitStack()\n        self.exit_stacks[server_id] = exit_stack\n\n", "metadata": {"task_id": "OpenManus/6", "ground_truth": "        server_params = StdioServerParameters(command=command, args=args)\n", "fpath_tuple": ["OpenManus", "app", "tool", "mcp.py"], "context_start_lineno": 0, "line_no": 86, "col_no": -1, "import_no": [10, 10]}}
{"prompt": "\"\"\"File operation interfaces and implementations for local and sandbox environments.\"\"\"\n\nimport asyncio\nfrom pathlib import Path\nfrom typing import Optional, Protocol, Tuple, Union, runtime_checkable\n\nfrom app.config import SandboxSettings\nfrom app.exceptions import ToolError\n\n\nPathLike = Union[str, Path]\n\n\n@runtime_checkable\nclass FileOperator(Protocol):\n    \"\"\"Interface for file operations in different environments.\"\"\"\n\n    async def read_file(self, path: PathLike) -> str:\n        \"\"\"Read content from a file.\"\"\"\n        ...\n\n    async def write_file(self, path: PathLike, content: str) -> None:\n        \"\"\"Write content to a file.\"\"\"\n        ...\n\n    async def is_directory(self, path: PathLike) -> bool:\n        \"\"\"Check if path points to a directory.\"\"\"\n        ...\n\n    async def exists(self, path: PathLike) -> bool:\n        \"\"\"Check if path exists.\"\"\"\n        ...\n\n    async def run_command(\n        self, cmd: str, timeout: Optional[float] = 120.0\n    ) -> Tuple[int, str, str]:\n        \"\"\"Run a shell command and return (return_code, stdout, stderr).\"\"\"\n        ...\n\n\nclass LocalFileOperator(FileOperator):\n    \"\"\"File operations implementation for local filesystem.\"\"\"\n\n    encoding: str = \"utf-8\"\n\n    async def read_file(self, path: PathLike) -> str:\n        \"\"\"Read content from a local file.\"\"\"\n        try:\n            return Path(path).read_text(encoding=self.encoding)\n        except Exception as e:\n            raise ToolError(f\"Failed to read {path}: {str(e)}\") from None\n\n    async def write_file(self, path: PathLike, content: str) -> None:\n        \"\"\"Write content to a local file.\"\"\"\n        try:\n            Path(path).write_text(content, encoding=self.encoding)\n        except Exception as e:\n            raise ToolError(f\"Failed to write to {path}: {str(e)}\") from None\n\n    async def is_directory(self, path: PathLike) -> bool:\n        \"\"\"Check if path points to a directory.\"\"\"\n        return Path(path).is_dir()\n\n    async def exists(self, path: PathLike) -> bool:\n        \"\"\"Check if path exists.\"\"\"\n        return Path(path).exists()\n\n    async def run_command(\n        self, cmd: str, timeout: Optional[float] = 120.0\n    ) -> Tuple[int, str, str]:\n        \"\"\"Run a shell command locally.\"\"\"\n        process = await asyncio.create_subprocess_shell(\n            cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE\n        )\n\n        try:\n            stdout, stderr = await asyncio.wait_for(\n                process.communicate(), timeout=timeout\n            )\n            return (\n                process.returncode or 0,\n                stdout.decode(),\n                stderr.decode(),\n            )\n        except asyncio.TimeoutError as exc:\n            try:\n                process.kill()\n            except ProcessLookupError:\n                pass\n            raise TimeoutError(\n                f\"Command '{cmd}' timed out after {timeout} seconds\"\n            ) from exc\n\n\nclass SandboxFileOperator(FileOperator):\n    \"\"\"File operations implementation for sandbox environment.\"\"\"\n\n    def __init__(self):\n        self.sandbox_client = SANDBOX_CLIENT\n\n    async def _ensure_sandbox_initialized(self):\n        \"\"\"Ensure sandbox is initialized.\"\"\"\n        if not self.sandbox_client.sandbox:\n            await ", "metadata": {"task_id": "OpenManus/7", "ground_truth": "self.sandbox_client.create(config=SandboxSettings())\n", "fpath_tuple": ["OpenManus", "app", "tool", "file_operators.py"], "context_start_lineno": 0, "line_no": 104, "col_no": 18, "import_no": [8, 8]}}
{"prompt": "import json\nimport time\nfrom enum import Enum\nfrom typing import Dict, List, Optional, Union\n\nfrom pydantic import Field\n\nfrom app.agent.base import BaseAgent\nfrom app.flow.base import BaseFlow\nfrom app.llm import LLM\nfrom app.logger import logger\nfrom app.tool import PlanningTool\n\n\nclass PlanStepStatus(str, Enum):\n    \"\"\"Enum class defining possible statuses of a plan step\"\"\"\n\n    NOT_STARTED = \"not_started\"\n    IN_PROGRESS = \"in_progress\"\n    COMPLETED = \"completed\"\n    BLOCKED = \"blocked\"\n\n    @classmethod\n    def get_all_statuses(cls) -> list[str]:\n        \"\"\"Return a list of all possible step status values\"\"\"\n        return [status.value for status in cls]\n\n    @classmethod\n    def get_active_statuses(cls) -> list[str]:\n        \"\"\"Return a list of values representing active statuses (not started or in progress)\"\"\"\n        return [cls.NOT_STARTED.value, cls.IN_PROGRESS.value]\n\n    @classmethod\n    def get_status_marks(cls) -> Dict[str, str]:\n        \"\"\"Return a mapping of statuses to their marker symbols\"\"\"\n        return {\n            cls.COMPLETED.value: \"[\u2713]\",\n            cls.IN_PROGRESS.value: \"[\u2192]\",\n            cls.BLOCKED.value: \"[!]\",\n            cls.NOT_STARTED.value: \"[ ]\",\n        }\n\n\nclass PlanningFlow(BaseFlow):\n    \"\"\"A flow that manages planning and execution of tasks using agents.\"\"\"\n\n    llm: LLM = Field(default_factory=lambda: LLM())\n    planning_tool: PlanningTool = Field(default_factory=PlanningTool)\n    executor_keys: List[str] = Field(default_factory=list)\n    active_plan_id: str = Field(default_factory=lambda: f\"plan_{int(time.time())}\")\n    current_step_index: Optional[int] = None\n\n    def __init__(\n        self, agents: Union[BaseAgent, List[BaseAgent], Dict[str, BaseAgent]], **data\n    ):\n        # Set executor keys before super().__init__\n        if \"executors\" in data:\n            data[\"executor_keys\"] = data.pop(\"executors\")\n\n        # Set plan ID if provided\n        if \"plan_id\" in data:\n            data[\"active_plan_id\"] = data.pop(\"plan_id\")\n\n        # Initialize the planning tool if not provided\n        if \"planning_tool\" not in data:\n            planning_tool = PlanningTool()\n            data[\"planning_tool\"] = planning_tool\n\n        # Call parent's init with the processed data\n        super().__init__(agents, **data)\n\n        # Set executor_keys to all agent keys if not specified\n        if not self.executor_keys:\n            self.executor_keys = list(self.agents.keys())\n\n    def get_executor(self, step_type: Optional[str] = None) -> BaseAgent:\n        \"\"\"\n        Get an appropriate executor agent for the current step.\n        Can be extended to select agents based on step type/requirements.\n        \"\"\"\n        # If step type is provided and matches an agent key, use that agent\n        if step_type and step_type in self.agents:\n            return self.agents[step_type]\n\n        # Otherwise use the first available executor or fall back to primary agent\n        for key in self.executor_keys:\n            if key in self.agents:\n                return self.agents[key]\n\n        # Fallback to primary agent\n        return self.primary_agent\n\n    async def execute(self, input_text: str) -> str:\n        \"\"\"Execute the planning flow with agents.\"\"\"\n        try:\n            if not self.primary_agent:\n                raise ValueError(\"No primary agent available\")\n\n            # Create initial plan if input provided\n            if input_text:\n                await self._create_initial_plan(input_text)\n\n                # Verify plan was created successfully\n                if self.active_plan_id not in self.planning_tool.plans:\n                    logger.error(\n                        f\"Plan creation failed. Plan ID {self.active_plan_id} not found in planning tool.\"\n                    )\n                    return f\"Failed to create plan for: {input_text}\"\n\n            result = \"\"\n            while True:\n                # Get current step to execute\n                self.current_step_index, step_info = await self._get_current_step_info()\n\n                # Exit if no more steps or plan completed\n                if self.current_step_index is None:\n                    result += await self._finalize_plan()\n                    break\n\n                # Execute current step with appropriate agent\n                step_type = step_info.get(\"type\") if step_info else None\n                executor = self.get_executor(step_type)\n                step_result = await self._execute_step(executor, step_info)\n                result += step_result + \"\\n\"\n\n                # Check if agent wants to terminate\n                if hasattr(executor, \"state\") and executor.state == AgentState.FINISHED:\n                    break\n\n            return result\n        except Exception as e:\n            logger.error(f\"Error in PlanningFlow: {str(e)}\")\n            return f\"Execution failed: {str(e)}\"\n\n    async def _create_initial_plan(self, request: str) -> None:\n        \"\"\"Create an initial plan based on the request using the flow's LLM and PlanningTool.\"\"\"\n        logger.info(f\"Creating initial plan with ID: {self.active_plan_id}\")\n\n        system_message_content = (\n            \"You are a planning assistant. Create a concise, actionable plan with clear steps. \"\n            \"Focus on key milestones rather than detailed sub-steps. \"\n            \"Optimize for clarity and efficiency.\"\n        )\n        agents_description = []\n        for key in self.executor_keys:\n            if key in self.agents:\n                agents_description.append(\n                    {\n                        \"name\": key.upper(),\n                        \"description\": self.agents[key].description,\n                    }\n                )\n        if len(agents_description) > 1:\n            # Add description of agents to select\n            system_message_content += (\n                f\"\\nNow we have {agents_description} agents. \"\n                f\"The infomation of them are below: {json.dumps(agents_description)}\\n\"\n                \"When creating steps in the planning tool, please specify the agent names using the format '[agent_name]'.\"\n            )\n\n        # Create a system message for plan creation\n", "metadata": {"task_id": "OpenManus/8", "ground_truth": "        system_message = Message.system_message(system_message_content)\n", "fpath_tuple": ["OpenManus", "app", "flow", "planning.py"], "context_start_lineno": 0, "line_no": 162, "col_no": -1, "import_no": [11, 11]}}
{"prompt": "import asyncio\nimport json\nfrom typing import Any, List, Optional, Union\n\nfrom pydantic import Field\n\nfrom app.agent.react import ReActAgent\nfrom app.exceptions import TokenLimitExceeded\nfrom app.logger import logger\nfrom app.prompt.toolcall import NEXT_STEP_PROMPT, SYSTEM_PROMPT\nfrom app.tool import CreateChatCompletion, Terminate, ToolCollection\n\n\nTOOL_CALL_REQUIRED = \"Tool calls required but none provided\"\n\n\nclass ToolCallAgent(ReActAgent):\n    \"\"\"Base agent class for handling tool/function calls with enhanced abstraction\"\"\"\n\n    name: str = \"toolcall\"\n    description: str = \"an agent that can execute tool calls.\"\n\n    system_prompt: str = SYSTEM_PROMPT\n    next_step_prompt: str = NEXT_STEP_PROMPT\n\n    available_tools: ToolCollection = ToolCollection(\n        CreateChatCompletion(), Terminate()\n    )\n    tool_choices: TOOL_CHOICE_TYPE = ToolChoice.AUTO  # type: ignore\n    special_tool_names: List[str] = Field(default_factory=lambda: [Terminate().name])\n\n    tool_calls: List[ToolCall] = Field(default_factory=list)\n    _current_base64_image: Optional[str] = None\n\n    max_steps: int = 30\n    max_observe: Optional[Union[int, bool]] = None\n\n    async def think(self) -> bool:\n        \"\"\"Process current state and decide next actions using tools\"\"\"\n        if self.next_step_prompt:\n            user_msg = ", "metadata": {"task_id": "OpenManus/9", "ground_truth": "Message.user_message(self.next_step_prompt)\n", "fpath_tuple": ["OpenManus", "app", "agent", "toolcall.py"], "context_start_lineno": 0, "line_no": 41, "col_no": 23, "import_no": [10, 10]}}
{"prompt": "from typing import Dict, List, Optional\n\nfrom pydantic import Field, model_validator\n\nfrom app.agent.toolcall import ToolCallAgent\nfrom app.config import config\nfrom app.logger import logger\nfrom app.prompt.manus import NEXT_STEP_PROMPT, SYSTEM_PROMPT\nfrom app.tool import Terminate, ToolCollection\nfrom app.tool.ask_human import AskHuman\nfrom app.tool.browser_use_tool import BrowserUseTool\nfrom app.tool.mcp import MCPClients, MCPClientTool\nfrom app.tool.python_execute import PythonExecute\nfrom app.tool.str_replace_editor import StrReplaceEditor\n\n\nclass Manus(ToolCallAgent):\n    \"\"\"A versatile general-purpose agent with support for both local and MCP tools.\"\"\"\n\n    name: str = \"Manus\"\n    description: str = \"A versatile agent that can solve various tasks using multiple tools including MCP-based tools\"\n\n    system_prompt: str = SYSTEM_PROMPT.format(directory=config.workspace_root)\n    next_step_prompt: str = NEXT_STEP_PROMPT\n\n    max_observe: int = 10000\n    max_steps: int = 20\n\n    # MCP clients for remote tool access\n    mcp_clients: MCPClients = Field(default_factory=MCPClients)\n\n    # Add general-purpose tools to the tool collection\n    available_tools: ToolCollection = Field(\n        default_factory=lambda: ToolCollection(\n            PythonExecute(),\n            BrowserUseTool(),\n            StrReplaceEditor(),\n            AskHuman(),\n            Terminate(),\n        )\n    )\n\n    special_tool_names: list[str] = Field(default_factory=lambda: [Terminate().name])\n    browser_context_helper: Optional[BrowserContextHelper] = None\n\n    # Track connected MCP servers\n    connected_servers: Dict[str, str] = Field(\n        default_factory=dict\n    )  # server_id -> url/command\n    _initialized: bool = False\n\n    @model_validator(mode=\"after\")\n    def initialize_helper(self) -> \"Manus\":\n        \"\"\"Initialize basic components synchronously.\"\"\"\n        self.browser_context_helper = BrowserContextHelper(self)\n        return self\n\n    @classmethod\n    async def create(cls, **kwargs) -> \"Manus\":\n        \"\"\"Factory method to create and properly initialize a Manus instance.\"\"\"\n        instance = cls(**kwargs)\n        await instance.initialize_mcp_servers()\n        instance._initialized = True\n        return instance\n\n    async def initialize_mcp_servers(self) -> None:\n        \"\"\"Initialize connections to configured MCP servers.\"\"\"\n        for server_id, server_config in config.mcp_config.servers.items():\n            try:\n                if server_config.type == \"sse\":\n                    if server_config.url:\n                        await self.connect_mcp_server(server_config.url, server_id)\n                        logger.info(\n                            f\"Connected to MCP server {server_id} at {server_config.url}\"\n                        )\n                elif server_config.type == \"stdio\":\n                    if server_config.command:\n                        await self.connect_mcp_server(\n                            server_config.command,\n                            server_id,\n                            use_stdio=True,\n                            stdio_args=server_config.args,\n                        )\n                        logger.info(\n                            f\"Connected to MCP server {server_id} using command {server_config.command}\"\n                        )\n            except Exception as e:\n                logger.error(f\"Failed to connect to MCP server {server_id}: {e}\")\n\n    async def connect_mcp_server(\n        self,\n        server_url: str,\n        server_id: str = \"\",\n        use_stdio: bool = False,\n        stdio_args: List[str] = None,\n    ) -> None:\n        \"\"\"Connect to an MCP server and add its tools.\"\"\"\n        if use_stdio:\n            await self.mcp_clients.connect_stdio(\n                server_url, stdio_args or [], server_id\n            )\n            self.connected_servers[server_id or server_url] = server_url\n        else:\n            await self.mcp_clients.connect_sse(server_url, server_id)\n            self.connected_servers[server_id or server_url] = server_url\n\n        # Update available tools with only the new tools from this server\n        new_tools = [\n            tool for tool in self.mcp_clients.tools if tool.server_id == server_id\n        ]\n        self.available_tools.add_tools(*new_tools)\n\n    async def disconnect_mcp_server(self, server_id: str = \"\") -> None:\n        \"\"\"Disconnect from an MCP server and remove its tools.\"\"\"\n        await self.mcp_clients.disconnect(server_id)\n        if server_id:\n            self.connected_servers.pop(server_id, None)\n        else:\n            self.connected_servers.clear()\n\n        # Rebuild available tools without the disconnected server's tools\n        base_tools = [\n            tool\n            for tool in self.available_tools.tools\n            if not isinstance(tool, MCPClientTool)\n        ]\n        self.available_tools = ToolCollection(*base_tools)\n        self.available_tools.add_tools(*self.mcp_clients.tools)\n\n    async def cleanup(self):\n        \"\"\"Clean up Manus agent resources.\"\"\"\n        if self.browser_context_helper:\n            await self.browser_context_helper.cleanup_browser()\n        # Disconnect from all MCP servers only if we were initialized\n        if self._initialized:\n            await self.disconnect_mcp_server()\n            self._initialized = False\n\n    async def think(self) -> bool:\n        \"\"\"Process current state and decide next actions with appropriate context.\"\"\"\n        if not self._initialized:\n            await self.initialize_mcp_servers()\n            self._initialized = True\n\n        original_prompt = self.next_step_prompt\n        recent_messages = self.memory.messages[-3:] if self.memory.messages else []\n        browser_in_use = any(\n            tc.function.name == BrowserUseTool().name\n            for msg in recent_messages\n            if msg.tool_calls\n            for tc in msg.tool_calls\n        )\n\n        if browser_in_use:\n            self.next_step_prompt = (\n", "metadata": {"task_id": "OpenManus/10", "ground_truth": "                await self.browser_context_helper.format_next_step_prompt()\n", "fpath_tuple": ["OpenManus", "app", "agent", "manus.py"], "context_start_lineno": 0, "line_no": 156, "col_no": -1, "import_no": [4, 4]}}
{"prompt": "import numpy as np\nimport argparse\nimport math\n\nfrom PIL import Image\nfrom diffusers import AutoencoderKLHunyuanVideo\nfrom transformers import LlamaModel, CLIPTextModel, LlamaTokenizerFast, CLIPTokenizer\nfrom diffusers_helper.hunyuan import encode_prompt_conds, vae_decode, vae_encode, vae_decode_fake\nfrom diffusers_helper.utils import save_bcthw_as_mp4, crop_or_pad_yield_mask, soft_append_bcthw, resize_and_center_crop, state_dict_weighted_merge, state_dict_offset_merge, generate_timestamp\nfrom diffusers_helper.models.hunyuan_video_packed import HunyuanVideoTransformer3DModelPacked\nfrom diffusers_helper.pipelines.k_diffusion_hunyuan import sample_hunyuan\nfrom diffusers_helper.memory import cpu, gpu, get_cuda_free_memory_gb, move_model_to_device_with_memory_preservation, offload_model_from_device_for_memory_preservation, fake_diffusers_current_device, DynamicSwapInstaller, unload_complete_models, load_model_as_complete\nfrom diffusers_helper.thread_utils import AsyncStream, async_run\nfrom diffusers_helper.gradio.progress_bar import make_progress_bar_css, make_progress_bar_html\nfrom transformers import SiglipImageProcessor, SiglipVisionModel\nfrom diffusers_helper.clip_vision import hf_clip_vision_encode\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--share', action='store_true')\nparser.add_argument(\"--server\", type=str, default='0.0.0.0')\nparser.add_argument(\"--port\", type=int, required=False)\nparser.add_argument(\"--inbrowser\", action='store_true')\nargs = parser.parse_args()\n\n# for win desktop probably use --server 127.0.0.1 --inbrowser\n# For linux server probably use --server 127.0.0.1 or do not use any cmd flags\n\nprint(args)\n\nfree_mem_gb = get_cuda_free_memory_gb(gpu)\nhigh_vram = free_mem_gb > 60\n\nprint(f'Free VRAM {free_mem_gb} GB')\nprint(f'High-VRAM Mode: {high_vram}')\n\ntext_encoder = LlamaModel.from_pretrained(\"hunyuanvideo-community/HunyuanVideo\", subfolder='text_encoder', torch_dtype=torch.float16).cpu()\ntext_encoder_2 = CLIPTextModel.from_pretrained(\"hunyuanvideo-community/HunyuanVideo\", subfolder='text_encoder_2', torch_dtype=torch.float16).cpu()\ntokenizer = LlamaTokenizerFast.from_pretrained(\"hunyuanvideo-community/HunyuanVideo\", subfolder='tokenizer')\ntokenizer_2 = CLIPTokenizer.from_pretrained(\"hunyuanvideo-community/HunyuanVideo\", subfolder='tokenizer_2')\nvae = AutoencoderKLHunyuanVideo.from_pretrained(\"hunyuanvideo-community/HunyuanVideo\", subfolder='vae', torch_dtype=torch.float16).cpu()\n\nfeature_extractor = SiglipImageProcessor.from_pretrained(\"lllyasviel/flux_redux_bfl\", subfolder='feature_extractor')\nimage_encoder = SiglipVisionModel.from_pretrained(\"lllyasviel/flux_redux_bfl\", subfolder='image_encoder', torch_dtype=torch.float16).cpu()\n\ntransformer = HunyuanVideoTransformer3DModelPacked.from_pretrained('lllyasviel/FramePack_F1_I2V_HY_20250503', torch_dtype=torch.bfloat16).cpu()\n\nvae.eval()\ntext_encoder.eval()\ntext_encoder_2.eval()\nimage_encoder.eval()\ntransformer.eval()\n\nif not high_vram:\n    vae.enable_slicing()\n    vae.enable_tiling()\n\ntransformer.high_quality_fp32_output_for_inference = True\nprint('transformer.high_quality_fp32_output_for_inference = True')\n\ntransformer.to(dtype=torch.bfloat16)\nvae.to(dtype=torch.float16)\nimage_encoder.to(dtype=torch.float16)\ntext_encoder.to(dtype=torch.float16)\ntext_encoder_2.to(dtype=torch.float16)\n\nvae.requires_grad_(False)\ntext_encoder.requires_grad_(False)\ntext_encoder_2.requires_grad_(False)\nimage_encoder.requires_grad_(False)\ntransformer.requires_grad_(False)\n\nif not high_vram:\n    # DynamicSwapInstaller is same as huggingface's enable_sequential_offload but 3x faster\n    DynamicSwapInstaller.install_model(transformer, device=gpu)\n    DynamicSwapInstaller.install_model(text_encoder, device=gpu)\nelse:\n    text_encoder.to(gpu)\n    text_encoder_2.to(gpu)\n    image_encoder.to(gpu)\n    vae.to(gpu)\n    transformer.to(gpu)\n\nstream = AsyncStream()\n\noutputs_folder = './outputs/'\nos.makedirs(outputs_folder, exist_ok=True)\n\n\n@torch.no_grad()\ndef worker(input_image, prompt, n_prompt, seed, total_second_length, latent_window_size, steps, cfg, gs, rs, gpu_memory_preservation, use_teacache, mp4_crf):\n    total_latent_sections = (total_second_length * 30) / (latent_window_size * 4)\n    total_latent_sections = int(max(round(total_latent_sections), 1))\n\n    job_id = generate_timestamp()\n\n    stream.output_queue.push(('progress', (None, '', make_progress_bar_html(0, 'Starting ...'))))\n\n    try:\n        # Clean GPU\n        if not high_vram:\n            unload_complete_models(\n                text_encoder, text_encoder_2, image_encoder, vae, transformer\n            )\n\n        # Text encoding\n\n        stream.output_queue.push(('progress', (None, '', make_progress_bar_html(0, 'Text encoding ...'))))\n\n        if not high_vram:\n            fake_diffusers_current_device(text_encoder, gpu)  # since we only encode one text - that is one model move and one encode, offload is same time consumption since it is also one load and one encode.\n            load_model_as_complete(text_encoder_2, target_device=gpu)\n\n        llama_vec, clip_l_pooler = encode_prompt_conds(prompt, text_encoder, text_encoder_2, tokenizer, tokenizer_2)\n\n        if cfg == 1:\n            llama_vec_n, clip_l_pooler_n = torch.zeros_like(llama_vec), torch.zeros_like(clip_l_pooler)\n        else:\n            llama_vec_n, clip_l_pooler_n = encode_prompt_conds(n_prompt, text_encoder, text_encoder_2, tokenizer, tokenizer_2)\n\n        llama_vec, llama_attention_mask = crop_or_pad_yield_mask(llama_vec, length=512)\n        llama_vec_n, llama_attention_mask_n = crop_or_pad_yield_mask(llama_vec_n, length=512)\n\n        # Processing input image\n\n        stream.output_queue.push(('progress', (None, '', make_progress_bar_html(0, 'Image processing ...'))))\n\n        H, W, C = input_image.shape\n        height, width = ", "metadata": {"task_id": "FramePack/1", "ground_truth": "find_nearest_bucket(H, W, resolution=640)\n", "fpath_tuple": ["FramePack", "demo_gradio_f1.py"], "context_start_lineno": 11, "line_no": 140, "col_no": 24, "import_no": [27, 27]}}
{"prompt": "from diffusers_helper.thread_utils import AsyncStream, async_run\nfrom diffusers_helper.gradio.progress_bar import make_progress_bar_css, make_progress_bar_html\nfrom transformers import SiglipImageProcessor, SiglipVisionModel\nfrom diffusers_helper.bucket_tools import find_nearest_bucket\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--share', action='store_true')\nparser.add_argument(\"--server\", type=str, default='0.0.0.0')\nparser.add_argument(\"--port\", type=int, required=False)\nparser.add_argument(\"--inbrowser\", action='store_true')\nargs = parser.parse_args()\n\n# for win desktop probably use --server 127.0.0.1 --inbrowser\n# For linux server probably use --server 127.0.0.1 or do not use any cmd flags\n\nprint(args)\n\nfree_mem_gb = get_cuda_free_memory_gb(gpu)\nhigh_vram = free_mem_gb > 60\n\nprint(f'Free VRAM {free_mem_gb} GB')\nprint(f'High-VRAM Mode: {high_vram}')\n\ntext_encoder = LlamaModel.from_pretrained(\"hunyuanvideo-community/HunyuanVideo\", subfolder='text_encoder', torch_dtype=torch.float16).cpu()\ntext_encoder_2 = CLIPTextModel.from_pretrained(\"hunyuanvideo-community/HunyuanVideo\", subfolder='text_encoder_2', torch_dtype=torch.float16).cpu()\ntokenizer = LlamaTokenizerFast.from_pretrained(\"hunyuanvideo-community/HunyuanVideo\", subfolder='tokenizer')\ntokenizer_2 = CLIPTokenizer.from_pretrained(\"hunyuanvideo-community/HunyuanVideo\", subfolder='tokenizer_2')\nvae = AutoencoderKLHunyuanVideo.from_pretrained(\"hunyuanvideo-community/HunyuanVideo\", subfolder='vae', torch_dtype=torch.float16).cpu()\n\nfeature_extractor = SiglipImageProcessor.from_pretrained(\"lllyasviel/flux_redux_bfl\", subfolder='feature_extractor')\nimage_encoder = SiglipVisionModel.from_pretrained(\"lllyasviel/flux_redux_bfl\", subfolder='image_encoder', torch_dtype=torch.float16).cpu()\n\ntransformer = HunyuanVideoTransformer3DModelPacked.from_pretrained('lllyasviel/FramePack_F1_I2V_HY_20250503', torch_dtype=torch.bfloat16).cpu()\n\nvae.eval()\ntext_encoder.eval()\ntext_encoder_2.eval()\nimage_encoder.eval()\ntransformer.eval()\n\nif not high_vram:\n    vae.enable_slicing()\n    vae.enable_tiling()\n\ntransformer.high_quality_fp32_output_for_inference = True\nprint('transformer.high_quality_fp32_output_for_inference = True')\n\ntransformer.to(dtype=torch.bfloat16)\nvae.to(dtype=torch.float16)\nimage_encoder.to(dtype=torch.float16)\ntext_encoder.to(dtype=torch.float16)\ntext_encoder_2.to(dtype=torch.float16)\n\nvae.requires_grad_(False)\ntext_encoder.requires_grad_(False)\ntext_encoder_2.requires_grad_(False)\nimage_encoder.requires_grad_(False)\ntransformer.requires_grad_(False)\n\nif not high_vram:\n    # DynamicSwapInstaller is same as huggingface's enable_sequential_offload but 3x faster\n    DynamicSwapInstaller.install_model(transformer, device=gpu)\n    DynamicSwapInstaller.install_model(text_encoder, device=gpu)\nelse:\n    text_encoder.to(gpu)\n    text_encoder_2.to(gpu)\n    image_encoder.to(gpu)\n    vae.to(gpu)\n    transformer.to(gpu)\n\nstream = AsyncStream()\n\noutputs_folder = './outputs/'\nos.makedirs(outputs_folder, exist_ok=True)\n\n\n@torch.no_grad()\ndef worker(input_image, prompt, n_prompt, seed, total_second_length, latent_window_size, steps, cfg, gs, rs, gpu_memory_preservation, use_teacache, mp4_crf):\n    total_latent_sections = (total_second_length * 30) / (latent_window_size * 4)\n    total_latent_sections = int(max(round(total_latent_sections), 1))\n\n    job_id = generate_timestamp()\n\n    stream.output_queue.push(('progress', (None, '', make_progress_bar_html(0, 'Starting ...'))))\n\n    try:\n        # Clean GPU\n        if not high_vram:\n            unload_complete_models(\n                text_encoder, text_encoder_2, image_encoder, vae, transformer\n            )\n\n        # Text encoding\n\n        stream.output_queue.push(('progress', (None, '', make_progress_bar_html(0, 'Text encoding ...'))))\n\n        if not high_vram:\n            fake_diffusers_current_device(text_encoder, gpu)  # since we only encode one text - that is one model move and one encode, offload is same time consumption since it is also one load and one encode.\n            load_model_as_complete(text_encoder_2, target_device=gpu)\n\n        llama_vec, clip_l_pooler = encode_prompt_conds(prompt, text_encoder, text_encoder_2, tokenizer, tokenizer_2)\n\n        if cfg == 1:\n            llama_vec_n, clip_l_pooler_n = torch.zeros_like(llama_vec), torch.zeros_like(clip_l_pooler)\n        else:\n            llama_vec_n, clip_l_pooler_n = encode_prompt_conds(n_prompt, text_encoder, text_encoder_2, tokenizer, tokenizer_2)\n\n        llama_vec, llama_attention_mask = crop_or_pad_yield_mask(llama_vec, length=512)\n        llama_vec_n, llama_attention_mask_n = crop_or_pad_yield_mask(llama_vec_n, length=512)\n\n        # Processing input image\n\n        stream.output_queue.push(('progress', (None, '', make_progress_bar_html(0, 'Image processing ...'))))\n\n        H, W, C = input_image.shape\n        height, width = find_nearest_bucket(H, W, resolution=640)\n        input_image_np = resize_and_center_crop(input_image, target_width=width, target_height=height)\n\n        Image.fromarray(input_image_np).save(os.path.join(outputs_folder, f'{job_id}.png'))\n\n        input_image_pt = torch.from_numpy(input_image_np).float() / 127.5 - 1\n        input_image_pt = input_image_pt.permute(2, 0, 1)[None, :, None]\n\n        # VAE encoding\n\n        stream.output_queue.push(('progress', (None, '', make_progress_bar_html(0, 'VAE encoding ...'))))\n\n        if not high_vram:\n            load_model_as_complete(vae, target_device=gpu)\n\n        start_latent = vae_encode(input_image_pt, vae)\n\n        # CLIP Vision\n\n        stream.output_queue.push(('progress', (None, '', make_progress_bar_html(0, 'CLIP Vision encoding ...'))))\n\n        if not high_vram:\n            load_model_as_complete(image_encoder, target_device=gpu)\n\n", "metadata": {"task_id": "FramePack/2", "ground_truth": "        image_encoder_output = hf_clip_vision_encode(input_image_np, feature_extractor, image_encoder)\n", "fpath_tuple": ["FramePack", "demo_gradio_f1.py"], "context_start_lineno": 23, "line_no": 164, "col_no": -1, "import_no": [26, 26]}}
{"prompt": "from diffusers_helper.hf_login import login\n\nimport os\n\nos.environ['HF_HOME'] = os.path.abspath(os.path.realpath(os.path.join(os.path.dirname(__file__), './hf_download')))\n\nimport gradio as gr\nimport torch\nimport traceback\nimport einops\nimport safetensors.torch as sf\nimport numpy as np\nimport argparse\nimport math\n\nfrom PIL import Image\nfrom diffusers import AutoencoderKLHunyuanVideo\nfrom transformers import LlamaModel, CLIPTextModel, LlamaTokenizerFast, CLIPTokenizer\nfrom diffusers_helper.hunyuan import encode_prompt_conds, vae_decode, vae_encode, vae_decode_fake\nfrom diffusers_helper.utils import save_bcthw_as_mp4, crop_or_pad_yield_mask, soft_append_bcthw, resize_and_center_crop, state_dict_weighted_merge, state_dict_offset_merge, generate_timestamp\nfrom diffusers_helper.models.hunyuan_video_packed import HunyuanVideoTransformer3DModelPacked\nfrom diffusers_helper.pipelines.k_diffusion_hunyuan import sample_hunyuan\nfrom diffusers_helper.thread_utils import AsyncStream, async_run\nfrom diffusers_helper.gradio.progress_bar import make_progress_bar_css, make_progress_bar_html\nfrom transformers import SiglipImageProcessor, SiglipVisionModel\nfrom diffusers_helper.clip_vision import hf_clip_vision_encode\nfrom diffusers_helper.bucket_tools import find_nearest_bucket\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--share', action='store_true')\nparser.add_argument(\"--server\", type=str, default='0.0.0.0')\nparser.add_argument(\"--port\", type=int, required=False)\nparser.add_argument(\"--inbrowser\", action='store_true')\nargs = parser.parse_args()\n\n# for win desktop probably use --server 127.0.0.1 --inbrowser\n# For linux server probably use --server 127.0.0.1 or do not use any cmd flags\n\nprint(args)\n\nfree_mem_gb = ", "metadata": {"task_id": "FramePack/3", "ground_truth": "get_cuda_free_memory_gb(gpu)\n", "fpath_tuple": ["FramePack", "demo_gradio.py"], "context_start_lineno": 0, "line_no": 42, "col_no": 14, "import_no": [22, 22]}}
{"prompt": "from diffusers_helper.hf_login import login\n\nimport os\n\nos.environ['HF_HOME'] = os.path.abspath(os.path.realpath(os.path.join(os.path.dirname(__file__), './hf_download')))\n\nimport gradio as gr\nimport torch\nimport traceback\nimport einops\nimport safetensors.torch as sf\nimport numpy as np\nimport argparse\nimport math\n\nfrom PIL import Image\nfrom diffusers import AutoencoderKLHunyuanVideo\nfrom transformers import LlamaModel, CLIPTextModel, LlamaTokenizerFast, CLIPTokenizer\nfrom diffusers_helper.hunyuan import encode_prompt_conds, vae_decode, vae_encode, vae_decode_fake\nfrom diffusers_helper.models.hunyuan_video_packed import HunyuanVideoTransformer3DModelPacked\nfrom diffusers_helper.pipelines.k_diffusion_hunyuan import sample_hunyuan\nfrom diffusers_helper.memory import cpu, gpu, get_cuda_free_memory_gb, move_model_to_device_with_memory_preservation, offload_model_from_device_for_memory_preservation, fake_diffusers_current_device, DynamicSwapInstaller, unload_complete_models, load_model_as_complete\nfrom diffusers_helper.thread_utils import AsyncStream, async_run\nfrom diffusers_helper.gradio.progress_bar import make_progress_bar_css, make_progress_bar_html\nfrom transformers import SiglipImageProcessor, SiglipVisionModel\nfrom diffusers_helper.clip_vision import hf_clip_vision_encode\nfrom diffusers_helper.bucket_tools import find_nearest_bucket\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--share', action='store_true')\nparser.add_argument(\"--server\", type=str, default='0.0.0.0')\nparser.add_argument(\"--port\", type=int, required=False)\nparser.add_argument(\"--inbrowser\", action='store_true')\nargs = parser.parse_args()\n\n# for win desktop probably use --server 127.0.0.1 --inbrowser\n# For linux server probably use --server 127.0.0.1 or do not use any cmd flags\n\nprint(args)\n\nfree_mem_gb = get_cuda_free_memory_gb(gpu)\nhigh_vram = free_mem_gb > 60\n\nprint(f'Free VRAM {free_mem_gb} GB')\nprint(f'High-VRAM Mode: {high_vram}')\n\ntext_encoder = LlamaModel.from_pretrained(\"hunyuanvideo-community/HunyuanVideo\", subfolder='text_encoder', torch_dtype=torch.float16).cpu()\ntext_encoder_2 = CLIPTextModel.from_pretrained(\"hunyuanvideo-community/HunyuanVideo\", subfolder='text_encoder_2', torch_dtype=torch.float16).cpu()\ntokenizer = LlamaTokenizerFast.from_pretrained(\"hunyuanvideo-community/HunyuanVideo\", subfolder='tokenizer')\ntokenizer_2 = CLIPTokenizer.from_pretrained(\"hunyuanvideo-community/HunyuanVideo\", subfolder='tokenizer_2')\nvae = AutoencoderKLHunyuanVideo.from_pretrained(\"hunyuanvideo-community/HunyuanVideo\", subfolder='vae', torch_dtype=torch.float16).cpu()\n\nfeature_extractor = SiglipImageProcessor.from_pretrained(\"lllyasviel/flux_redux_bfl\", subfolder='feature_extractor')\nimage_encoder = SiglipVisionModel.from_pretrained(\"lllyasviel/flux_redux_bfl\", subfolder='image_encoder', torch_dtype=torch.float16).cpu()\n\ntransformer = HunyuanVideoTransformer3DModelPacked.from_pretrained('lllyasviel/FramePackI2V_HY', torch_dtype=torch.bfloat16).cpu()\n\nvae.eval()\ntext_encoder.eval()\ntext_encoder_2.eval()\nimage_encoder.eval()\ntransformer.eval()\n\nif not high_vram:\n    vae.enable_slicing()\n    vae.enable_tiling()\n\ntransformer.high_quality_fp32_output_for_inference = True\nprint('transformer.high_quality_fp32_output_for_inference = True')\n\ntransformer.to(dtype=torch.bfloat16)\nvae.to(dtype=torch.float16)\nimage_encoder.to(dtype=torch.float16)\ntext_encoder.to(dtype=torch.float16)\ntext_encoder_2.to(dtype=torch.float16)\n\nvae.requires_grad_(False)\ntext_encoder.requires_grad_(False)\ntext_encoder_2.requires_grad_(False)\nimage_encoder.requires_grad_(False)\ntransformer.requires_grad_(False)\n\nif not high_vram:\n    # DynamicSwapInstaller is same as huggingface's enable_sequential_offload but 3x faster\n    DynamicSwapInstaller.install_model(transformer, device=gpu)\n    DynamicSwapInstaller.install_model(text_encoder, device=gpu)\nelse:\n    text_encoder.to(gpu)\n    text_encoder_2.to(gpu)\n    image_encoder.to(gpu)\n    vae.to(gpu)\n    transformer.to(gpu)\n\nstream = AsyncStream()\n\noutputs_folder = './outputs/'\nos.makedirs(outputs_folder, exist_ok=True)\n\n\n@torch.no_grad()\ndef worker(input_image, prompt, n_prompt, seed, total_second_length, latent_window_size, steps, cfg, gs, rs, gpu_memory_preservation, use_teacache, mp4_crf):\n    total_latent_sections = (total_second_length * 30) / (latent_window_size * 4)\n    total_latent_sections = int(max(round(total_latent_sections), 1))\n\n    job_id = generate_timestamp()\n\n    stream.output_queue.push(('progress', (None, '', make_progress_bar_html(0, 'Starting ...'))))\n\n    try:\n        # Clean GPU\n        if not high_vram:\n            unload_complete_models(\n                text_encoder, text_encoder_2, image_encoder, vae, transformer\n            )\n\n        # Text encoding\n\n        stream.output_queue.push(('progress', (None, '', make_progress_bar_html(0, 'Text encoding ...'))))\n\n        if not high_vram:\n            fake_diffusers_current_device(text_encoder, gpu)  # since we only encode one text - that is one model move and one encode, offload is same time consumption since it is also one load and one encode.\n            load_model_as_complete(text_encoder_2, target_device=gpu)\n\n        llama_vec, clip_l_pooler = encode_prompt_conds(prompt, text_encoder, text_encoder_2, tokenizer, tokenizer_2)\n\n        if cfg == 1:\n            llama_vec_n, clip_l_pooler_n = torch.zeros_like(llama_vec), torch.zeros_like(clip_l_pooler)\n        else:\n            llama_vec_n, clip_l_pooler_n = encode_prompt_conds(n_prompt, text_encoder, text_encoder_2, tokenizer, tokenizer_2)\n\n", "metadata": {"task_id": "FramePack/4", "ground_truth": "        llama_vec, llama_attention_mask = crop_or_pad_yield_mask(llama_vec, length=512)\n", "fpath_tuple": ["FramePack", "demo_gradio.py"], "context_start_lineno": 0, "line_no": 132, "col_no": -1, "import_no": [19, 19]}}
{"prompt": "\n        if not high_vram:\n            load_model_as_complete(image_encoder, target_device=gpu)\n\n        image_encoder_output = hf_clip_vision_encode(input_image_np, feature_extractor, image_encoder)\n        image_encoder_last_hidden_state = image_encoder_output.last_hidden_state\n\n        # Dtype\n\n        llama_vec = llama_vec.to(transformer.dtype)\n        llama_vec_n = llama_vec_n.to(transformer.dtype)\n        clip_l_pooler = clip_l_pooler.to(transformer.dtype)\n        clip_l_pooler_n = clip_l_pooler_n.to(transformer.dtype)\n        image_encoder_last_hidden_state = image_encoder_last_hidden_state.to(transformer.dtype)\n\n        # Sampling\n\n        stream.output_queue.push(('progress', (None, '', make_progress_bar_html(0, 'Start sampling ...'))))\n\n        rnd = torch.Generator(\"cpu\").manual_seed(seed)\n        num_frames = latent_window_size * 4 - 3\n\n        history_latents = torch.zeros(size=(1, 16, 1 + 2 + 16, height // 8, width // 8), dtype=torch.float32).cpu()\n        history_pixels = None\n        total_generated_latent_frames = 0\n\n        latent_paddings = reversed(range(total_latent_sections))\n\n        if total_latent_sections > 4:\n            # In theory the latent_paddings should follow the above sequence, but it seems that duplicating some\n            # items looks better than expanding it when total_latent_sections > 4\n            # One can try to remove below trick and just\n            # use `latent_paddings = list(reversed(range(total_latent_sections)))` to compare\n            latent_paddings = [3] + [2] * (total_latent_sections - 3) + [1, 0]\n\n        for latent_padding in latent_paddings:\n            is_last_section = latent_padding == 0\n            latent_padding_size = latent_padding * latent_window_size\n\n            if stream.input_queue.top() == 'end':\n                stream.output_queue.push(('end', None))\n                return\n\n            print(f'latent_padding_size = {latent_padding_size}, is_last_section = {is_last_section}')\n\n            indices = torch.arange(0, sum([1, latent_padding_size, latent_window_size, 1, 2, 16])).unsqueeze(0)\n            clean_latent_indices_pre, blank_indices, latent_indices, clean_latent_indices_post, clean_latent_2x_indices, clean_latent_4x_indices = indices.split([1, latent_padding_size, latent_window_size, 1, 2, 16], dim=1)\n            clean_latent_indices = torch.cat([clean_latent_indices_pre, clean_latent_indices_post], dim=1)\n\n            clean_latents_pre = start_latent.to(history_latents)\n            clean_latents_post, clean_latents_2x, clean_latents_4x = history_latents[:, :, :1 + 2 + 16, :, :].split([1, 2, 16], dim=2)\n            clean_latents = torch.cat([clean_latents_pre, clean_latents_post], dim=2)\n\n            if not high_vram:\n                unload_complete_models()\n                move_model_to_device_with_memory_preservation(transformer, target_device=gpu, preserved_memory_gb=gpu_memory_preservation)\n\n            if use_teacache:\n                transformer.initialize_teacache(enable_teacache=True, num_steps=steps)\n            else:\n                transformer.initialize_teacache(enable_teacache=False)\n\n            def callback(d):\n                preview = d['denoised']\n                preview = vae_decode_fake(preview)\n\n                preview = (preview * 255.0).detach().cpu().numpy().clip(0, 255).astype(np.uint8)\n                preview = einops.rearrange(preview, 'b c t h w -> (b h) (t w) c')\n\n                if stream.input_queue.top() == 'end':\n                    stream.output_queue.push(('end', None))\n                    raise KeyboardInterrupt('User ends the task.')\n\n                current_step = d['i'] + 1\n                percentage = int(100.0 * current_step / steps)\n                hint = f'Sampling {current_step}/{steps}'\n                desc = f'Total generated frames: {int(max(0, total_generated_latent_frames * 4 - 3))}, Video length: {max(0, (total_generated_latent_frames * 4 - 3) / 30) :.2f} seconds (FPS-30). The video is being extended now ...'\n                stream.output_queue.push(('progress', (preview, desc, make_progress_bar_html(percentage, hint))))\n                return\n\n            generated_latents = sample_hunyuan(\n                transformer=transformer,\n                sampler='unipc',\n                width=width,\n                height=height,\n                frames=num_frames,\n                real_guidance_scale=cfg,\n                distilled_guidance_scale=gs,\n                guidance_rescale=rs,\n                # shift=3.0,\n                num_inference_steps=steps,\n                generator=rnd,\n                prompt_embeds=llama_vec,\n                prompt_embeds_mask=llama_attention_mask,\n                prompt_poolers=clip_l_pooler,\n                negative_prompt_embeds=llama_vec_n,\n                negative_prompt_embeds_mask=llama_attention_mask_n,\n                negative_prompt_poolers=clip_l_pooler_n,\n                device=gpu,\n                dtype=torch.bfloat16,\n                image_embeddings=image_encoder_last_hidden_state,\n                latent_indices=latent_indices,\n                clean_latents=clean_latents,\n                clean_latent_indices=clean_latent_indices,\n                clean_latents_2x=clean_latents_2x,\n                clean_latent_2x_indices=clean_latent_2x_indices,\n                clean_latents_4x=clean_latents_4x,\n                clean_latent_4x_indices=clean_latent_4x_indices,\n                callback=callback,\n            )\n\n            if is_last_section:\n                generated_latents = torch.cat([start_latent.to(generated_latents), generated_latents], dim=2)\n\n            total_generated_latent_frames += int(generated_latents.shape[2])\n            history_latents = torch.cat([generated_latents.to(history_latents), history_latents], dim=2)\n\n            if not high_vram:\n                offload_model_from_device_for_memory_preservation(transformer, target_device=gpu, preserved_memory_gb=8)\n                load_model_as_complete(vae, target_device=gpu)\n\n            real_history_latents = history_latents[:, :, :total_generated_latent_frames, :, :]\n\n            if history_pixels is None:\n                history_pixels = vae_decode(real_history_latents, vae).cpu()\n            else:\n                section_latent_frames = (latent_window_size * 2 + 1) if is_last_section else (latent_window_size * 2)\n                overlapped_frames = latent_window_size * 4 - 3\n\n                current_pixels = vae_decode(real_history_latents[:, :, :section_latent_frames], vae).cpu()\n                history_pixels = ", "metadata": {"task_id": "FramePack/5", "ground_truth": "soft_append_bcthw(current_pixels, history_pixels, overlapped_frames)\n", "fpath_tuple": ["FramePack", "demo_gradio.py"], "context_start_lineno": 159, "line_no": 290, "col_no": 33, "import_no": [19, 19]}}
{"prompt": "        image_encoder_last_hidden_state = image_encoder_output.last_hidden_state\n\n        # Dtype\n\n        llama_vec = llama_vec.to(transformer.dtype)\n        llama_vec_n = llama_vec_n.to(transformer.dtype)\n        clip_l_pooler = clip_l_pooler.to(transformer.dtype)\n        clip_l_pooler_n = clip_l_pooler_n.to(transformer.dtype)\n        image_encoder_last_hidden_state = image_encoder_last_hidden_state.to(transformer.dtype)\n\n        # Sampling\n\n        stream.output_queue.push(('progress', (None, '', make_progress_bar_html(0, 'Start sampling ...'))))\n\n        rnd = torch.Generator(\"cpu\").manual_seed(seed)\n        num_frames = latent_window_size * 4 - 3\n\n        history_latents = torch.zeros(size=(1, 16, 1 + 2 + 16, height // 8, width // 8), dtype=torch.float32).cpu()\n        history_pixels = None\n        total_generated_latent_frames = 0\n\n        latent_paddings = reversed(range(total_latent_sections))\n\n        if total_latent_sections > 4:\n            # In theory the latent_paddings should follow the above sequence, but it seems that duplicating some\n            # items looks better than expanding it when total_latent_sections > 4\n            # One can try to remove below trick and just\n            # use `latent_paddings = list(reversed(range(total_latent_sections)))` to compare\n            latent_paddings = [3] + [2] * (total_latent_sections - 3) + [1, 0]\n\n        for latent_padding in latent_paddings:\n            is_last_section = latent_padding == 0\n            latent_padding_size = latent_padding * latent_window_size\n\n            if stream.input_queue.top() == 'end':\n                stream.output_queue.push(('end', None))\n                return\n\n            print(f'latent_padding_size = {latent_padding_size}, is_last_section = {is_last_section}')\n\n            indices = torch.arange(0, sum([1, latent_padding_size, latent_window_size, 1, 2, 16])).unsqueeze(0)\n            clean_latent_indices_pre, blank_indices, latent_indices, clean_latent_indices_post, clean_latent_2x_indices, clean_latent_4x_indices = indices.split([1, latent_padding_size, latent_window_size, 1, 2, 16], dim=1)\n            clean_latent_indices = torch.cat([clean_latent_indices_pre, clean_latent_indices_post], dim=1)\n\n            clean_latents_pre = start_latent.to(history_latents)\n            clean_latents_post, clean_latents_2x, clean_latents_4x = history_latents[:, :, :1 + 2 + 16, :, :].split([1, 2, 16], dim=2)\n            clean_latents = torch.cat([clean_latents_pre, clean_latents_post], dim=2)\n\n            if not high_vram:\n                unload_complete_models()\n                move_model_to_device_with_memory_preservation(transformer, target_device=gpu, preserved_memory_gb=gpu_memory_preservation)\n\n            if use_teacache:\n                transformer.initialize_teacache(enable_teacache=True, num_steps=steps)\n            else:\n                transformer.initialize_teacache(enable_teacache=False)\n\n            def callback(d):\n                preview = d['denoised']\n                preview = vae_decode_fake(preview)\n\n                preview = (preview * 255.0).detach().cpu().numpy().clip(0, 255).astype(np.uint8)\n                preview = einops.rearrange(preview, 'b c t h w -> (b h) (t w) c')\n\n                if stream.input_queue.top() == 'end':\n                    stream.output_queue.push(('end', None))\n                    raise KeyboardInterrupt('User ends the task.')\n\n                current_step = d['i'] + 1\n                percentage = int(100.0 * current_step / steps)\n                hint = f'Sampling {current_step}/{steps}'\n                desc = f'Total generated frames: {int(max(0, total_generated_latent_frames * 4 - 3))}, Video length: {max(0, (total_generated_latent_frames * 4 - 3) / 30) :.2f} seconds (FPS-30). The video is being extended now ...'\n                stream.output_queue.push(('progress', (preview, desc, make_progress_bar_html(percentage, hint))))\n                return\n\n            generated_latents = sample_hunyuan(\n                transformer=transformer,\n                sampler='unipc',\n                width=width,\n                height=height,\n                frames=num_frames,\n                real_guidance_scale=cfg,\n                distilled_guidance_scale=gs,\n                guidance_rescale=rs,\n                # shift=3.0,\n                num_inference_steps=steps,\n                generator=rnd,\n                prompt_embeds=llama_vec,\n                prompt_embeds_mask=llama_attention_mask,\n                prompt_poolers=clip_l_pooler,\n                negative_prompt_embeds=llama_vec_n,\n                negative_prompt_embeds_mask=llama_attention_mask_n,\n                negative_prompt_poolers=clip_l_pooler_n,\n                device=gpu,\n                dtype=torch.bfloat16,\n                image_embeddings=image_encoder_last_hidden_state,\n                latent_indices=latent_indices,\n                clean_latents=clean_latents,\n                clean_latent_indices=clean_latent_indices,\n                clean_latents_2x=clean_latents_2x,\n                clean_latent_2x_indices=clean_latent_2x_indices,\n                clean_latents_4x=clean_latents_4x,\n                clean_latent_4x_indices=clean_latent_4x_indices,\n                callback=callback,\n            )\n\n            if is_last_section:\n                generated_latents = torch.cat([start_latent.to(generated_latents), generated_latents], dim=2)\n\n            total_generated_latent_frames += int(generated_latents.shape[2])\n            history_latents = torch.cat([generated_latents.to(history_latents), history_latents], dim=2)\n\n            if not high_vram:\n                offload_model_from_device_for_memory_preservation(transformer, target_device=gpu, preserved_memory_gb=8)\n                load_model_as_complete(vae, target_device=gpu)\n\n            real_history_latents = history_latents[:, :, :total_generated_latent_frames, :, :]\n\n            if history_pixels is None:\n                history_pixels = vae_decode(real_history_latents, vae).cpu()\n            else:\n                section_latent_frames = (latent_window_size * 2 + 1) if is_last_section else (latent_window_size * 2)\n                overlapped_frames = latent_window_size * 4 - 3\n\n                current_pixels = vae_decode(real_history_latents[:, :, :section_latent_frames], vae).cpu()\n                history_pixels = soft_append_bcthw(current_pixels, history_pixels, overlapped_frames)\n\n            if not high_vram:\n                unload_complete_models()\n\n            output_filename = os.path.join(outputs_folder, f'{job_id}_{total_generated_latent_frames}.mp4')\n\n", "metadata": {"task_id": "FramePack/6", "ground_truth": "            save_bcthw_as_mp4(history_pixels, output_filename, fps=30, crf=mp4_crf)\n", "fpath_tuple": ["FramePack", "demo_gradio.py"], "context_start_lineno": 164, "line_no": 297, "col_no": -1, "import_no": [19, 19]}}
{"prompt": "import torch\nimport math\n\nfrom diffusers_helper.k_diffusion.uni_pc_fm import sample_unipc\nfrom diffusers_helper.k_diffusion.wrapper import fm_wrapper\n\n\ndef flux_time_shift(t, mu=1.15, sigma=1.0):\n    return math.exp(mu) / (math.exp(mu) + (1 / t - 1) ** sigma)\n\n\ndef calculate_flux_mu(context_length, x1=256, y1=0.5, x2=4096, y2=1.15, exp_max=7.0):\n    k = (y2 - y1) / (x2 - x1)\n    b = y1 - k * x1\n    mu = k * context_length + b\n    mu = min(mu, math.log(exp_max))\n    return mu\n\n\ndef get_flux_sigmas_from_mu(n, mu):\n    sigmas = torch.linspace(1, 0, steps=n + 1)\n    sigmas = flux_time_shift(sigmas, mu=mu)\n    return sigmas\n\n\n@torch.inference_mode()\ndef sample_hunyuan(\n        transformer,\n        sampler='unipc',\n        initial_latent=None,\n        concat_latent=None,\n        strength=1.0,\n        width=512,\n        height=512,\n        frames=16,\n        real_guidance_scale=1.0,\n        distilled_guidance_scale=6.0,\n        guidance_rescale=0.0,\n        shift=None,\n        num_inference_steps=25,\n        batch_size=None,\n        generator=None,\n        prompt_embeds=None,\n        prompt_embeds_mask=None,\n        prompt_poolers=None,\n        negative_prompt_embeds=None,\n        negative_prompt_embeds_mask=None,\n        negative_prompt_poolers=None,\n        dtype=torch.bfloat16,\n        device=None,\n        negative_kwargs=None,\n        callback=None,\n        **kwargs,\n):\n    device = device or transformer.device\n\n    if batch_size is None:\n        batch_size = int(prompt_embeds.shape[0])\n\n    latents = torch.randn((batch_size, 16, (frames + 3) // 4, height // 8, width // 8), generator=generator, device=generator.device).to(device=device, dtype=torch.float32)\n\n    B, C, T, H, W = latents.shape\n    seq_length = T * H * W // 4\n\n    if shift is None:\n        mu = calculate_flux_mu(seq_length, exp_max=7.0)\n    else:\n        mu = math.log(shift)\n\n    sigmas = get_flux_sigmas_from_mu(num_inference_steps, mu).to(device)\n\n    k_model = fm_wrapper(transformer)\n\n    if initial_latent is not None:\n        sigmas = sigmas * strength\n        first_sigma = sigmas[0].to(device=device, dtype=torch.float32)\n        initial_latent = initial_latent.to(device=device, dtype=torch.float32)\n        latents = initial_latent.float() * (1.0 - first_sigma) + latents.float() * first_sigma\n\n    if concat_latent is not None:\n        concat_latent = concat_latent.to(latents)\n\n    distilled_guidance = torch.tensor([distilled_guidance_scale * 1000.0] * batch_size).to(device=device, dtype=dtype)\n\n    prompt_embeds = ", "metadata": {"task_id": "FramePack/7", "ground_truth": "repeat_to_batch_size(prompt_embeds, batch_size)\n", "fpath_tuple": ["FramePack", "diffusers_helper", "pipelines", "k_diffusion_hunyuan.py"], "context_start_lineno": 0, "line_no": 85, "col_no": 20, "import_no": [5, 5]}}
{"prompt": "import torch\nimport math\n\nfrom diffusers_helper.k_diffusion.wrapper import fm_wrapper\nfrom diffusers_helper.utils import repeat_to_batch_size\n\n\ndef flux_time_shift(t, mu=1.15, sigma=1.0):\n    return math.exp(mu) / (math.exp(mu) + (1 / t - 1) ** sigma)\n\n\ndef calculate_flux_mu(context_length, x1=256, y1=0.5, x2=4096, y2=1.15, exp_max=7.0):\n    k = (y2 - y1) / (x2 - x1)\n    b = y1 - k * x1\n    mu = k * context_length + b\n    mu = min(mu, math.log(exp_max))\n    return mu\n\n\ndef get_flux_sigmas_from_mu(n, mu):\n    sigmas = torch.linspace(1, 0, steps=n + 1)\n    sigmas = flux_time_shift(sigmas, mu=mu)\n    return sigmas\n\n\n@torch.inference_mode()\ndef sample_hunyuan(\n        transformer,\n        sampler='unipc',\n        initial_latent=None,\n        concat_latent=None,\n        strength=1.0,\n        width=512,\n        height=512,\n        frames=16,\n        real_guidance_scale=1.0,\n        distilled_guidance_scale=6.0,\n        guidance_rescale=0.0,\n        shift=None,\n        num_inference_steps=25,\n        batch_size=None,\n        generator=None,\n        prompt_embeds=None,\n        prompt_embeds_mask=None,\n        prompt_poolers=None,\n        negative_prompt_embeds=None,\n        negative_prompt_embeds_mask=None,\n        negative_prompt_poolers=None,\n        dtype=torch.bfloat16,\n        device=None,\n        negative_kwargs=None,\n        callback=None,\n        **kwargs,\n):\n    device = device or transformer.device\n\n    if batch_size is None:\n        batch_size = int(prompt_embeds.shape[0])\n\n    latents = torch.randn((batch_size, 16, (frames + 3) // 4, height // 8, width // 8), generator=generator, device=generator.device).to(device=device, dtype=torch.float32)\n\n    B, C, T, H, W = latents.shape\n    seq_length = T * H * W // 4\n\n    if shift is None:\n        mu = calculate_flux_mu(seq_length, exp_max=7.0)\n    else:\n        mu = math.log(shift)\n\n    sigmas = get_flux_sigmas_from_mu(num_inference_steps, mu).to(device)\n\n    k_model = fm_wrapper(transformer)\n\n    if initial_latent is not None:\n        sigmas = sigmas * strength\n        first_sigma = sigmas[0].to(device=device, dtype=torch.float32)\n        initial_latent = initial_latent.to(device=device, dtype=torch.float32)\n        latents = initial_latent.float() * (1.0 - first_sigma) + latents.float() * first_sigma\n\n    if concat_latent is not None:\n        concat_latent = concat_latent.to(latents)\n\n    distilled_guidance = torch.tensor([distilled_guidance_scale * 1000.0] * batch_size).to(device=device, dtype=dtype)\n\n    prompt_embeds = repeat_to_batch_size(prompt_embeds, batch_size)\n    prompt_embeds_mask = repeat_to_batch_size(prompt_embeds_mask, batch_size)\n    prompt_poolers = repeat_to_batch_size(prompt_poolers, batch_size)\n    negative_prompt_embeds = repeat_to_batch_size(negative_prompt_embeds, batch_size)\n    negative_prompt_embeds_mask = repeat_to_batch_size(negative_prompt_embeds_mask, batch_size)\n    negative_prompt_poolers = repeat_to_batch_size(negative_prompt_poolers, batch_size)\n    concat_latent = repeat_to_batch_size(concat_latent, batch_size)\n\n    sampler_kwargs = dict(\n        dtype=dtype,\n        cfg_scale=real_guidance_scale,\n        cfg_rescale=guidance_rescale,\n        concat_latent=concat_latent,\n        positive=dict(\n            pooled_projections=prompt_poolers,\n            encoder_hidden_states=prompt_embeds,\n            encoder_attention_mask=prompt_embeds_mask,\n            guidance=distilled_guidance,\n            **kwargs,\n        ),\n        negative=dict(\n            pooled_projections=negative_prompt_poolers,\n            encoder_hidden_states=negative_prompt_embeds,\n            encoder_attention_mask=negative_prompt_embeds_mask,\n            guidance=distilled_guidance,\n            **(kwargs if negative_kwargs is None else {**kwargs, **negative_kwargs}),\n        )\n    )\n\n    if sampler == 'unipc':\n", "metadata": {"task_id": "FramePack/8", "ground_truth": "        results = sample_unipc(k_model, latents, sigmas, extra_args=sampler_kwargs, disable=False, callback=callback)\n", "fpath_tuple": ["FramePack", "diffusers_helper", "pipelines", "k_diffusion_hunyuan.py"], "context_start_lineno": 0, "line_no": 115, "col_no": -1, "import_no": [3, 3]}}
{"prompt": "import torch\nimport math\n\nfrom diffusers_helper.k_diffusion.uni_pc_fm import sample_unipc\nfrom diffusers_helper.utils import repeat_to_batch_size\n\n\ndef flux_time_shift(t, mu=1.15, sigma=1.0):\n    return math.exp(mu) / (math.exp(mu) + (1 / t - 1) ** sigma)\n\n\ndef calculate_flux_mu(context_length, x1=256, y1=0.5, x2=4096, y2=1.15, exp_max=7.0):\n    k = (y2 - y1) / (x2 - x1)\n    b = y1 - k * x1\n    mu = k * context_length + b\n    mu = min(mu, math.log(exp_max))\n    return mu\n\n\ndef get_flux_sigmas_from_mu(n, mu):\n    sigmas = torch.linspace(1, 0, steps=n + 1)\n    sigmas = flux_time_shift(sigmas, mu=mu)\n    return sigmas\n\n\n@torch.inference_mode()\ndef sample_hunyuan(\n        transformer,\n        sampler='unipc',\n        initial_latent=None,\n        concat_latent=None,\n        strength=1.0,\n        width=512,\n        height=512,\n        frames=16,\n        real_guidance_scale=1.0,\n        distilled_guidance_scale=6.0,\n        guidance_rescale=0.0,\n        shift=None,\n        num_inference_steps=25,\n        batch_size=None,\n        generator=None,\n        prompt_embeds=None,\n        prompt_embeds_mask=None,\n        prompt_poolers=None,\n        negative_prompt_embeds=None,\n        negative_prompt_embeds_mask=None,\n        negative_prompt_poolers=None,\n        dtype=torch.bfloat16,\n        device=None,\n        negative_kwargs=None,\n        callback=None,\n        **kwargs,\n):\n    device = device or transformer.device\n\n    if batch_size is None:\n        batch_size = int(prompt_embeds.shape[0])\n\n    latents = torch.randn((batch_size, 16, (frames + 3) // 4, height // 8, width // 8), generator=generator, device=generator.device).to(device=device, dtype=torch.float32)\n\n    B, C, T, H, W = latents.shape\n    seq_length = T * H * W // 4\n\n    if shift is None:\n        mu = calculate_flux_mu(seq_length, exp_max=7.0)\n    else:\n        mu = math.log(shift)\n\n    sigmas = get_flux_sigmas_from_mu(num_inference_steps, mu).to(device)\n\n    k_model = ", "metadata": {"task_id": "FramePack/9", "ground_truth": "fm_wrapper(transformer)\n", "fpath_tuple": ["FramePack", "diffusers_helper", "pipelines", "k_diffusion_hunyuan.py"], "context_start_lineno": 0, "line_no": 72, "col_no": 14, "import_no": [4, 4]}}
{"prompt": "        key = attn.norm_k(key)\n\n        query = apply_rotary_emb_transposed(query, image_rotary_emb)\n        key = apply_rotary_emb_transposed(key, image_rotary_emb)\n\n        encoder_query = attn.add_q_proj(encoder_hidden_states)\n        encoder_key = attn.add_k_proj(encoder_hidden_states)\n        encoder_value = attn.add_v_proj(encoder_hidden_states)\n\n        encoder_query = encoder_query.unflatten(2, (attn.heads, -1))\n        encoder_key = encoder_key.unflatten(2, (attn.heads, -1))\n        encoder_value = encoder_value.unflatten(2, (attn.heads, -1))\n\n        encoder_query = attn.norm_added_q(encoder_query)\n        encoder_key = attn.norm_added_k(encoder_key)\n\n        query = torch.cat([query, encoder_query], dim=1)\n        key = torch.cat([key, encoder_key], dim=1)\n        value = torch.cat([value, encoder_value], dim=1)\n\n        hidden_states = attn_varlen_func(query, key, value, cu_seqlens_q, cu_seqlens_kv, max_seqlen_q, max_seqlen_kv)\n        hidden_states = hidden_states.flatten(-2)\n\n        txt_length = encoder_hidden_states.shape[1]\n        hidden_states, encoder_hidden_states = hidden_states[:, :-txt_length], hidden_states[:, -txt_length:]\n\n        hidden_states = attn.to_out[0](hidden_states)\n        hidden_states = attn.to_out[1](hidden_states)\n        encoder_hidden_states = attn.to_add_out(encoder_hidden_states)\n\n        return hidden_states, encoder_hidden_states\n\n\nclass HunyuanAttnProcessorFlashAttnSingle:\n    def __call__(self, attn, hidden_states, encoder_hidden_states, attention_mask, image_rotary_emb):\n        cu_seqlens_q, cu_seqlens_kv, max_seqlen_q, max_seqlen_kv = attention_mask\n\n        hidden_states = torch.cat([hidden_states, encoder_hidden_states], dim=1)\n\n        query = attn.to_q(hidden_states)\n        key = attn.to_k(hidden_states)\n        value = attn.to_v(hidden_states)\n\n        query = query.unflatten(2, (attn.heads, -1))\n        key = key.unflatten(2, (attn.heads, -1))\n        value = value.unflatten(2, (attn.heads, -1))\n\n        query = attn.norm_q(query)\n        key = attn.norm_k(key)\n\n        txt_length = encoder_hidden_states.shape[1]\n\n        query = torch.cat([apply_rotary_emb_transposed(query[:, :-txt_length], image_rotary_emb), query[:, -txt_length:]], dim=1)\n        key = torch.cat([apply_rotary_emb_transposed(key[:, :-txt_length], image_rotary_emb), key[:, -txt_length:]], dim=1)\n\n        hidden_states = attn_varlen_func(query, key, value, cu_seqlens_q, cu_seqlens_kv, max_seqlen_q, max_seqlen_kv)\n        hidden_states = hidden_states.flatten(-2)\n\n        hidden_states, encoder_hidden_states = hidden_states[:, :-txt_length], hidden_states[:, -txt_length:]\n\n        return hidden_states, encoder_hidden_states\n\n\nclass CombinedTimestepGuidanceTextProjEmbeddings(nn.Module):\n    def __init__(self, embedding_dim, pooled_projection_dim):\n        super().__init__()\n\n        self.time_proj = Timesteps(num_channels=256, flip_sin_to_cos=True, downscale_freq_shift=0)\n        self.timestep_embedder = TimestepEmbedding(in_channels=256, time_embed_dim=embedding_dim)\n        self.guidance_embedder = TimestepEmbedding(in_channels=256, time_embed_dim=embedding_dim)\n        self.text_embedder = PixArtAlphaTextProjection(pooled_projection_dim, embedding_dim, act_fn=\"silu\")\n\n    def forward(self, timestep, guidance, pooled_projection):\n        timesteps_proj = self.time_proj(timestep)\n        timesteps_emb = self.timestep_embedder(timesteps_proj.to(dtype=pooled_projection.dtype))\n\n        guidance_proj = self.time_proj(guidance)\n        guidance_emb = self.guidance_embedder(guidance_proj.to(dtype=pooled_projection.dtype))\n\n        time_guidance_emb = timesteps_emb + guidance_emb\n\n        pooled_projections = self.text_embedder(pooled_projection)\n        conditioning = time_guidance_emb + pooled_projections\n\n        return conditioning\n\n\nclass CombinedTimestepTextProjEmbeddings(nn.Module):\n    def __init__(self, embedding_dim, pooled_projection_dim):\n        super().__init__()\n\n        self.time_proj = Timesteps(num_channels=256, flip_sin_to_cos=True, downscale_freq_shift=0)\n        self.timestep_embedder = TimestepEmbedding(in_channels=256, time_embed_dim=embedding_dim)\n        self.text_embedder = PixArtAlphaTextProjection(pooled_projection_dim, embedding_dim, act_fn=\"silu\")\n\n    def forward(self, timestep, pooled_projection):\n        timesteps_proj = self.time_proj(timestep)\n        timesteps_emb = self.timestep_embedder(timesteps_proj.to(dtype=pooled_projection.dtype))\n\n        pooled_projections = self.text_embedder(pooled_projection)\n\n        conditioning = timesteps_emb + pooled_projections\n\n        return conditioning\n\n\nclass HunyuanVideoAdaNorm(nn.Module):\n    def __init__(self, in_features: int, out_features: Optional[int] = None) -> None:\n        super().__init__()\n\n        out_features = out_features or 2 * in_features\n        self.linear = nn.Linear(in_features, out_features)\n        self.nonlinearity = nn.SiLU()\n\n    def forward(\n        self, temb: torch.Tensor\n    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n        temb = self.linear(self.nonlinearity(temb))\n        gate_msa, gate_mlp = temb.chunk(2, dim=-1)\n        gate_msa, gate_mlp = gate_msa.unsqueeze(1), gate_mlp.unsqueeze(1)\n        return gate_msa, gate_mlp\n\n\nclass HunyuanVideoIndividualTokenRefinerBlock(nn.Module):\n    def __init__(\n        self,\n        num_attention_heads: int,\n        attention_head_dim: int,\n        mlp_width_ratio: str = 4.0,\n        mlp_drop_rate: float = 0.0,\n        attention_bias: bool = True,\n    ) -> None:\n        super().__init__()\n\n        hidden_size = num_attention_heads * attention_head_dim\n\n        self.norm1 = ", "metadata": {"task_id": "FramePack/10", "ground_truth": "LayerNorm(hidden_size, elementwise_affine=True, eps=1e-6)\n", "fpath_tuple": ["FramePack", "diffusers_helper", "models", "hunyuan_video_packed.py"], "context_start_lineno": 154, "line_no": 291, "col_no": 21, "import_no": [16, 16]}}
{"prompt": "import argparse\nimport os\nimport random\n\nimport numpy as np\nimport soundfile as sf\nimport torch\n\n\n\ndef set_seed(seed: int):\n    \"\"\"Sets the random seed for reproducibility.\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    # Ensure deterministic behavior for cuDNN (if used)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Generate audio using the Dia model.\")\n\n    parser.add_argument(\"text\", type=str, help=\"Input text for speech generation.\")\n    parser.add_argument(\n        \"--output\", type=str, required=True, help=\"Path to save the generated audio file (e.g., output.wav).\"\n    )\n\n    parser.add_argument(\n        \"--repo-id\",\n        type=str,\n        default=\"nari-labs/Dia-1.6B\",\n        help=\"Hugging Face repository ID (e.g., nari-labs/Dia-1.6B).\",\n    )\n    parser.add_argument(\n        \"--local-paths\", action=\"store_true\", help=\"Load model from local config and checkpoint files.\"\n    )\n\n    parser.add_argument(\n        \"--config\", type=str, help=\"Path to local config.json file (required if --local-paths is set).\"\n    )\n    parser.add_argument(\n        \"--checkpoint\", type=str, help=\"Path to local model checkpoint .pth file (required if --local-paths is set).\"\n    )\n    parser.add_argument(\n        \"--audio-prompt\", type=str, default=None, help=\"Path to an optional audio prompt WAV file for voice cloning.\"\n    )\n\n    gen_group = parser.add_argument_group(\"Generation Parameters\")\n    gen_group.add_argument(\n        \"--max-tokens\",\n        type=int,\n        default=None,\n        help=\"Maximum number of audio tokens to generate (defaults to config value).\",\n    )\n    gen_group.add_argument(\n        \"--cfg-scale\", type=float, default=3.0, help=\"Classifier-Free Guidance scale (default: 3.0).\"\n    )\n    gen_group.add_argument(\n        \"--temperature\", type=float, default=1.3, help=\"Sampling temperature (higher is more random, default: 0.7).\"\n    )\n    gen_group.add_argument(\"--top-p\", type=float, default=0.95, help=\"Nucleus sampling probability (default: 0.95).\")\n\n    infra_group = parser.add_argument_group(\"Infrastructure\")\n    infra_group.add_argument(\"--seed\", type=int, default=None, help=\"Random seed for reproducibility.\")\n    infra_group.add_argument(\n        \"--device\",\n        type=str,\n        default=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n        help=\"Device to run inference on (e.g., 'cuda', 'cpu', default: auto).\",\n    )\n\n    args = parser.parse_args()\n\n    # Validation for local paths\n    if args.local_paths:\n        if not args.config:\n            parser.error(\"--config is required when --local-paths is set.\")\n        if not args.checkpoint:\n            parser.error(\"--checkpoint is required when --local-paths is set.\")\n        if not os.path.exists(args.config):\n            parser.error(f\"Config file not found: {args.config}\")\n        if not os.path.exists(args.checkpoint):\n            parser.error(f\"Checkpoint file not found: {args.checkpoint}\")\n\n    # Set seed if provided\n    if args.seed is not None:\n        set_seed(args.seed)\n        print(f\"Using random seed: {args.seed}\")\n\n    # Determine device\n    device = torch.device(args.device)\n    print(f\"Using device: {device}\")\n\n    # Load model\n    print(\"Loading model...\")\n    if args.local_paths:\n        print(f\"Loading from local paths: config='{args.config}', checkpoint='{args.checkpoint}'\")\n        try:\n", "metadata": {"task_id": "dia/1", "ground_truth": "            model = Dia.from_local(args.config, args.checkpoint, device=device)\n", "fpath_tuple": ["dia", "cli.py"], "context_start_lineno": 0, "line_no": 103, "col_no": -1, "import_no": [8, 8]}}
{"prompt": "\n\nmodel = Dia.from_pretrained(\"nari-labs/Dia-1.6B\", compute_dtype=\"float16\")\n\ntext = \"[S1] Dia is an open weights text to dialogue model. [S2] You get full control over scripts and voices. [S1] Wow. Amazing. (laughs) [S2] Try it now on Git hub or Hugging Face.\"\ntexts = [text for _ in range(10)]\n\noutput = model.generate(texts, use_torch_compile=True, verbose=True, max_tokens=1500)\n\nfor i, o in enumerate(output):\n", "metadata": {"task_id": "dia/2", "ground_truth": "    model.save_audio(f\"simple_{i}.mp3\", o)\n", "fpath_tuple": ["dia", "example", "simple_batch.py"], "context_start_lineno": 0, "line_no": 11, "col_no": -1, "import_no": [0, 0]}}
{"prompt": "\n\nmodel = Dia.from_pretrained(\"nari-labs/Dia-1.6B\", compute_dtype=\"float16\")\n\n# You should put the transcript of the voice you want to clone\n# We will use the audio created by running simple.py as an example.\n# Note that you will be REQUIRED TO RUN simple.py for the script to work as-is.\nclone_from_text = \"[S1] Dia is an open weights text to dialogue model. [S2] You get full control over scripts and voices. [S1] Wow. Amazing. (laughs) [S2] Try it now on Git hub or Hugging Face.\"\n\n# For your custom needs, replace above with below and add your audio file to this directory:\n# clone_from_text = \"[S1] ... [S2] ... [S1] ... corresponding to your_audio_name.mp3\"\n# clone_from_audio = \"your_audio_name.mp3\"\n\n# Text to generate\ntext_to_generate = \"[S1] Dia is an open weights text to dialogue model. [S2] You get full control over scripts and voices. [S1] Wow. Amazing. (laughs) [S2] Try it now on Git hub or Hugging Face.\"\n\nclone_from_audios = [f\"simple_{i}.mp3\" for i in range(10)]\n\ntexts = [clone_from_text + text_to_generate for _ in range(10)]\n\n# It will only return the audio from the text_to_generate\noutput = ", "metadata": {"task_id": "dia/3", "ground_truth": "model.generate(texts, audio_prompt=clone_from_audios, use_torch_compile=True, verbose=True, max_tokens=2000)\n", "fpath_tuple": ["dia", "example", "voice_clone_batch.py"], "context_start_lineno": 0, "line_no": 22, "col_no": 9, "import_no": [0, 0]}}
{"prompt": "        first_part = first_half * cos - second_half * sin\n        second_part = second_half * cos + first_half * sin\n        return torch.cat((first_part.to(self.compute_dtype), second_part.to(self.compute_dtype)), dim=-1)\n\n\ndef custom_scaled_dot_product_attention(\n    query: torch.Tensor,\n    key: torch.Tensor,\n    value: torch.Tensor,\n    attn_mask: torch.Tensor | None = None,\n    scale: float = 1.0,\n    is_causal: bool = False,\n    num_gqa_groups: int = 1,\n) -> torch.Tensor:\n    \"\"\"\n    Custom scaled dot-product attention with GQA support for MPS compatibility.\n\n    Args:\n        query: (B, N_q, T, H) - Query tensor, N_q = num_query_heads\n        key: (B, N_kv, S, H) - Key tensor, N_kv = num_kv_heads\n        value: (B, N_kv, S, H) - Value tensor\n        attn_mask: (B, 1, T, S) - Attention mask, optional\n        scale: Scaling factor for attention scores\n        is_causal: If True, apply causal masking\n        num_gqa_groups: Number of query groups per KV head (N_q / N_kv)\n\n    Returns:\n        output: (B, N_q, T, H) - Attention output\n    \"\"\"\n    B, N_q, T, H = query.shape\n    _, N_kv, S, _ = key.shape\n\n    # For GQA, repeat key and value tensors to match query heads\n    if num_gqa_groups > 1:\n        key = key.repeat_interleave(num_gqa_groups, dim=1)  # (B, N_q, S, H)\n        value = value.repeat_interleave(num_gqa_groups, dim=1)  # (B, N_q, S, H)\n\n    # Compute attention scores: (B, N_q, T, H) @ (B, N_q, H, S) -> (B, N_q, T, S)\n    scores = torch.matmul(query, key.transpose(-1, -2)) * scale\n\n    # Apply causal mask if needed\n    if is_causal:\n        causal_mask = torch.tril(torch.ones(T, S, dtype=torch.bool, device=query.device))\n        scores = scores.masked_fill(~causal_mask, float(\"-inf\"))\n\n    # Apply attention mask if provided\n    if attn_mask is not None:\n        scores = scores.masked_fill(~attn_mask, float(\"-inf\"))\n\n    # Softmax over the last dimension (S)\n    attn_weights = F.softmax(scores, dim=-1)\n\n    # Compute output: (B, N_q, T, S) @ (B, N_q, S, H) -> (B, N_q, T, H)\n    output = torch.matmul(attn_weights, value)\n\n    return output\n\n\nclass CrossAttention(nn.Module):\n    \"\"\"Cross-Attention using DenseGeneral.\"\"\"\n\n    def __init__(\n        self,\n        config: DiaConfig,\n        q_embed_dim: int,\n        kv_embed_dim: int,\n        num_query_heads: int,\n        num_kv_heads: int,\n        head_dim: int,\n        compute_dtype: torch.dtype,\n        out_embed_dim: int | None = None,\n    ):\n        super().__init__()\n        self.num_query_heads = num_query_heads\n        self.num_kv_heads = num_kv_heads\n        self.head_dim = head_dim\n        self.output_dim = out_embed_dim if out_embed_dim is not None else q_embed_dim\n        self.projected_query_dim = num_query_heads * head_dim\n        if num_query_heads % num_kv_heads != 0:\n            raise ValueError(f\"num_query_heads ({num_query_heads}) must be divisible by num_kv_heads ({num_kv_heads})\")\n        self.num_gqa_groups = num_query_heads // num_kv_heads\n\n        # --- Projection Layers using DenseGeneral ---\n        self.q_proj = DenseGeneral(\n            in_shapes=(q_embed_dim,),\n            out_features=(num_query_heads, head_dim),\n            axis=(-1,),\n            weight_dtype=compute_dtype,\n        )\n        self.k_proj = DenseGeneral(\n            in_shapes=(kv_embed_dim,),\n            out_features=(num_kv_heads, head_dim),\n            axis=(-1,),\n            weight_dtype=compute_dtype,\n        )\n        self.v_proj = DenseGeneral(\n            in_shapes=(kv_embed_dim,),\n            out_features=(num_kv_heads, head_dim),\n            axis=(-1,),\n            weight_dtype=compute_dtype,\n        )\n        self.o_proj = DenseGeneral(\n            in_shapes=(num_query_heads, head_dim),\n            out_features=(self.output_dim,),\n            axis=(-2, -1),\n            weight_dtype=compute_dtype,\n        )\n\n        # --- Rotary Embedding ---\n        self.rotary_emb = RotaryEmbedding(\n            embedding_dims=self.head_dim,\n            min_timescale=config.model.rope_min_timescale,\n            max_timescale=config.model.rope_max_timescale,\n            dtype=compute_dtype,\n        )\n\n    def forward(\n        self,\n        Xq: torch.Tensor,  # (B, T, D) T = 1 in AR generation\n        q_positions: torch.Tensor,  # (B, T)\n        kv_positions: torch.Tensor | None = None,  # (B, S)\n        attn_mask: torch.Tensor | None = None,  # None in Decoder Self Attention, Valid mask in Others\n        cache: KVCache | None = None,  # None in Encoder, KVCache in Decoder\n        is_causal: bool = False,\n    ) -> tuple[torch.Tensor, tuple[torch.Tensor, torch.Tensor] | None]:\n        \"\"\"\n        Performs attention calculation with optional KV caching.\n\n        Args:\n            Xq: Query tensor (B, T, D). T=1 during single-step decoding.\n            Xkv: Key/Value source tensor (B, S, E). S=1 during single-step decoding for self-attn.\n            q_positions: Positions for queries (B, T).\n            kv_positions: Positions for keys/values (B, S). If None, uses q_positions.\n            attn_mask: Attention mask.\n            cache: KVCache.\n\n        Returns:\n            A tuple containing:\n            - output: The attention output tensor (B, T, output_dim).\n            - present_kv: The K/V state to be cached for the next step ((B, N, S_new, H), (B, N, S_new, H)). For self-attn, S_new = S_past + S. For cross-attn, S_new = S_kv.\n        \"\"\"\n        if kv_positions is None:\n            kv_positions = q_positions\n        original_dtype = Xq.dtype\n\n        Xq_BxTxNxH = self.q_proj(Xq)\n        Xq_BxTxNxH = self.rotary_emb(Xq_BxTxNxH, position=q_positions)\n        Xq_BxNxTxH = Xq_BxTxNxH.transpose(1, 2)\n\n        attn_k: torch.Tensor | None = None\n        attn_v: torch.Tensor | None = None\n\n", "metadata": {"task_id": "dia/4", "ground_truth": "        attn_k, attn_v = cache.k, cache.v\n", "fpath_tuple": ["dia", "dia", "layers.py"], "context_start_lineno": 131, "line_no": 284, "col_no": -1, "import_no": [8, 8]}}
{"prompt": "        if is_mps:\n            attn_output = custom_scaled_dot_product_attention(\n                query=Xq_BxNxTxH,\n                key=attn_k,\n                value=attn_v,\n                attn_mask=attn_mask if not is_causal else None,\n                scale=1.0,\n                is_causal=is_causal,\n                num_gqa_groups=self.num_gqa_groups,\n            )\n        else:\n            attn_output = F.scaled_dot_product_attention(\n                Xq_BxNxTxH,\n                attn_k,\n                attn_v,\n                attn_mask=attn_mask if not is_causal else None,\n                scale=1.0,\n                enable_gqa=self.num_gqa_groups > 1,\n                is_causal=is_causal,\n            )\n\n        attn_output = attn_output.transpose(1, 2).contiguous()  # (B, T, N, H)\n        output = self.o_proj(attn_output)\n\n        return output.to(original_dtype)\n\n\nclass EncoderLayer(nn.Module):\n    \"\"\"Transformer Encoder Layer using DenseGeneral.\"\"\"\n\n    def __init__(self, config: DiaConfig, compute_dtype: torch.dtype):\n        super().__init__()\n        self.config = config\n        model_config = config.model\n        enc_config = config.model.encoder\n        embed_dim = enc_config.n_embd\n        self.compute_dtype = compute_dtype\n\n        self.pre_sa_norm = RMSNorm(\n            embed_dim,\n            eps=model_config.normalization_layer_epsilon,\n            dtype=torch.float32,\n        )\n        self.self_attention = SelfAttention(\n            config,\n            q_embed_dim=embed_dim,\n            kv_embed_dim=embed_dim,\n            num_query_heads=enc_config.n_head,\n            num_kv_heads=enc_config.n_head,\n            head_dim=enc_config.head_dim,\n            compute_dtype=compute_dtype,\n            is_cross_attn=False,\n            out_embed_dim=embed_dim,\n        )\n        self.post_sa_norm = RMSNorm(\n            embed_dim,\n            eps=model_config.normalization_layer_epsilon,\n            dtype=torch.float32,\n        )\n        self.mlp = MlpBlock(embed_dim=embed_dim, intermediate_dim=enc_config.n_hidden, compute_dtype=compute_dtype)\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        state: EncoderInferenceState,\n    ) -> torch.Tensor:\n        residual = x\n        x_norm = self.pre_sa_norm(x).to(self.compute_dtype)\n\n        sa_out = self.self_attention(\n            X=x_norm,\n            q_positions=state.positions,\n            kv_positions=state.positions,\n            attn_mask=state.attn_mask,\n        )\n        x = residual + sa_out\n\n        residual = x\n        x_norm = self.post_sa_norm(x).to(self.compute_dtype)\n        mlp_out = self.mlp(x_norm)\n        x = residual + mlp_out\n\n        return x\n\n\nclass Encoder(nn.Module):\n    \"\"\"Transformer Encoder Stack using DenseGeneral.\"\"\"\n\n    def __init__(self, config: DiaConfig, compute_dtype: torch.dtype):\n        super().__init__()\n        self.config = config\n        model_config = config.model\n        enc_config = config.model.encoder\n        self.compute_dtype = compute_dtype\n\n        self.embedding = nn.Embedding(\n            model_config.src_vocab_size,\n            enc_config.n_embd,\n            dtype=compute_dtype,\n        )\n        self.layers = nn.ModuleList([EncoderLayer(config, compute_dtype) for _ in range(enc_config.n_layer)])\n        self.norm = RMSNorm(\n            enc_config.n_embd,\n            eps=model_config.normalization_layer_epsilon,\n            dtype=torch.float32,\n        )\n\n    def forward(\n        self,\n        x_ids: torch.Tensor,\n        state: EncoderInferenceState,\n    ) -> torch.Tensor:\n        x = self.embedding(x_ids)\n\n        for layer in self.layers:\n            x = layer(x, state)\n\n        x = self.norm(x).to(self.compute_dtype)\n        return x\n\n\nclass DecoderLayer(nn.Module):\n    \"\"\"Transformer Decoder Layer using DenseGeneral.\"\"\"\n\n    def __init__(self, config: DiaConfig, compute_dtype: torch.dtype):\n        super().__init__()\n        self.config = config\n        model_config = config.model\n        dec_config = config.model.decoder\n        enc_config = config.model.encoder\n        dec_embed_dim = dec_config.n_embd\n        enc_embed_dim = enc_config.n_embd\n        self.compute_dtype = compute_dtype\n\n        # Norms\n        self.pre_sa_norm = RMSNorm(\n            dec_embed_dim,\n            eps=model_config.normalization_layer_epsilon,\n            dtype=torch.float32,\n        )\n        self.pre_ca_norm = RMSNorm(\n            dec_embed_dim,\n            eps=model_config.normalization_layer_epsilon,\n            dtype=torch.float32,\n        )\n        self.pre_mlp_norm = RMSNorm(\n            dec_embed_dim,\n            eps=model_config.normalization_layer_epsilon,\n            dtype=torch.float32,\n        )\n\n        # Self-Attention (GQA) with Causal Masking\n        self.self_attention = SelfAttention(\n            config,\n            q_embed_dim=dec_embed_dim,\n            kv_embed_dim=dec_embed_dim,\n            num_query_heads=dec_config.gqa_query_heads,\n            num_kv_heads=dec_config.kv_heads,\n            head_dim=dec_config.gqa_head_dim,\n            compute_dtype=compute_dtype,\n            is_cross_attn=False,\n            out_embed_dim=dec_embed_dim,\n        )\n        # Cross-Attention (MHA)\n        self.cross_attention = CrossAttention(\n            config=config,\n            q_embed_dim=dec_embed_dim,\n            kv_embed_dim=enc_embed_dim,  # Note kv_embed_dim\n            num_query_heads=dec_config.cross_query_heads,\n            num_kv_heads=dec_config.cross_query_heads,\n            head_dim=dec_config.cross_head_dim,\n            compute_dtype=compute_dtype,\n            out_embed_dim=dec_embed_dim,\n        )\n        # MLP\n        self.mlp = MlpBlock(\n            embed_dim=dec_embed_dim,\n            intermediate_dim=dec_config.n_hidden,\n            compute_dtype=compute_dtype,\n        )\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        state: DecoderInferenceState,\n        self_attn_cache: KVCache | None = None,\n        cross_attn_cache: KVCache | None = None,\n        prefill: bool = False,\n        current_idx: int = 0,\n    ) -> torch.Tensor:\n        residual = x\n        x_norm = self.pre_sa_norm(x).to(self.compute_dtype)\n\n        self_attn_mask = ", "metadata": {"task_id": "dia/5", "ground_truth": "state.casual_attn_mask[None, None, current_idx]\n", "fpath_tuple": ["dia", "dia", "layers.py"], "context_start_lineno": 510, "line_no": 704, "col_no": 25, "import_no": [8, 8]}}
{"prompt": "        Args:\n            model_name: The Hugging Face Hub repository ID (e.g., \"nari-labs/Dia-1.6B\").\n            compute_dtype: The computation dtype to use.\n            device: The device to load the model onto. If None, will automatically select the best available device.\n            load_dac: Whether to load the DAC model.\n\n        Returns:\n            An instance of the Dia model loaded with weights and set to eval mode.\n\n        Raises:\n            FileNotFoundError: If config or checkpoint download/loading fails.\n            RuntimeError: If there is an error loading the checkpoint.\n        \"\"\"\n        if isinstance(compute_dtype, str):\n            compute_dtype = ComputeDtype(compute_dtype)\n\n        # Load model directly using DiaModel's from_pretrained which handles HF download\n        try:\n            loaded_model = DiaModel.from_pretrained(model_name, compute_dtype=compute_dtype.to_dtype())\n        except Exception as e:\n            raise RuntimeError(f\"Error loading model from Hugging Face Hub ({model_name})\") from e\n\n        config = loaded_model.config  # Get config from the loaded model\n        dia = cls(config, compute_dtype, device, load_dac)\n\n        dia.model = loaded_model  # Assign the already loaded model\n        dia.model.to(dia.device)\n        dia.model.eval()\n        if load_dac:\n            dia._load_dac_model()\n        return dia\n\n    def _load_dac_model(self):\n        \"\"\"Loads the Descript Audio Codec (DAC) model.\n\n        Downloads the DAC model if necessary and loads it onto the specified device.\n        Sets the DAC model to evaluation mode.\n\n        Raises:\n            RuntimeError: If downloading or loading the DAC model fails.\n        \"\"\"\n        import dac\n\n        try:\n            dac_model_path = dac.utils.download()\n            dac_model = dac.DAC.load(dac_model_path).to(self.device)\n            dac_model.eval()  # Ensure DAC is in eval mode\n        except Exception as e:\n            raise RuntimeError(\"Failed to load DAC model\") from e\n        self.dac_model = dac_model\n\n    def _encode_text(self, text: str) -> torch.Tensor:\n        \"\"\"Encodes the input text string into a tensor of token IDs using byte-level encoding.\n\n        Special tokens [S1] and [S2] are replaced by their byte values. The resulting\n        sequence is truncated to the maximum configured text length.\n\n        Args:\n            text: The input text string.\n\n        Returns:\n            A tensor containing the encoded byte token IDs.\n        \"\"\"\n        max_len = self.config.data.text_length\n\n        byte_text = text.encode(\"utf-8\")\n        # Replace special tokens with their byte values if needed by the specific tokenizer/config\n        # Assuming byte values 1 and 2 are correct placeholders based on original code\n        replaced_bytes = byte_text.replace(b\"[S1]\", b\"\\x01\").replace(b\"[S2]\", b\"\\x02\")\n        text_tokens = list(replaced_bytes)\n        return torch.tensor(\n            text_tokens[:max_len],\n            dtype=torch.long,\n            device=self.device,\n        )\n\n    def _pad_text_input(self, text_tokens: list[torch.Tensor]) -> torch.Tensor:\n        \"\"\"Pads the text input to the maximum length.\"\"\"\n        text_pad_value = self.config.data.text_pad_value\n        max_len = self.config.data.text_length\n        batch_size = len(text_tokens)\n\n        src_tokens = torch.full(\n            (batch_size, 1, max_len),\n            fill_value=text_pad_value,\n            dtype=torch.long,\n            device=self.device,\n        )\n        for i in range(batch_size):\n            current_len = len(text_tokens[i])\n            src_tokens[i, 0, :current_len] = text_tokens[i]\n        return src_tokens\n\n    def _prepare_audio_prompt(self, audio_prompts: list[torch.Tensor | None]) -> tuple[torch.Tensor, list[int]]:\n        \"\"\"Prepares the audio prompt tensor for the decoder.\n\n        Handles padding, adds the beginning-of-sequence (BOS) token, applies the\n        delay pattern, and determines the number of prefill steps for each item\n        in the batch.\n\n        Args:\n            audio_prompts: A list of audio prompt tensors (encoded DAC frames) or None.\n                           Each tensor should have shape [T, C].\n\n        Returns:\n            A tuple containing:\n                - delayed_batch (torch.Tensor): The prepared audio prompt tensor with\n                  delays applied, shape [B, T_max_padded, C].\n                - prefill_steps (list[int]): A list containing the number of valid\n                  tokens (including BOS) for each prompt in the batch.\n        \"\"\"\n        num_channels = self.config.data.channels\n        audio_bos_value = self.config.data.audio_bos_value\n        delay_pattern = self.config.data.delay_pattern\n        max_delay_pattern = max(delay_pattern)\n        batch_size = len(audio_prompts)\n\n        max_len = max(p.shape[0] if p is not None else 0 for p in audio_prompts) + max_delay_pattern\n        prefill_steps = []\n\n        prefill = torch.full(\n            (batch_size, max_len, num_channels),\n            fill_value=-1,\n            dtype=torch.int,\n            device=self.device,\n        )\n\n        prefill[:, 0, :] = audio_bos_value\n\n        for i in range(batch_size):\n            prompt = audio_prompts[i]\n            if prompt is not None:\n                prompt = prompt.to(device=self.device, dtype=torch.int)\n                prefill[i, 1 : prompt.shape[0] + 1, :] = prompt\n                prefill_steps.append(prompt.shape[0] + 1)\n            else:\n                prefill_steps.append(1)\n\n        delay_precomp = build_delay_indices(\n            B=batch_size,\n            T=max_len,\n            C=num_channels,\n            delay_pattern=delay_pattern,\n        )\n\n        delayed_batch = apply_audio_delay(\n            audio_BxTxC=prefill,\n            pad_value=-1,\n            bos_value=audio_bos_value,\n            precomp=delay_precomp,\n        )\n\n        return delayed_batch, prefill_steps\n\n    def _prepare_generation(\n        self,\n        text: torch.Tensor,\n        audio_prompts: list[torch.Tensor | None],\n        max_tokens: int | None = None,\n    ):\n        \"\"\"Initializes the model state for generation.\n\n        Encodes the text input (conditional and unconditional), prepares the\n        encoder and decoder states (including KV caches and cross-attention),\n        prepares the audio prompt, and performs the initial decoder prefill steps\n        based on the audio prompts.\n\n        Args:\n            text: The padded text input tensor, shape [B, 1, T_text].\n            audio_prompts: A list of prepared audio prompt tensors or None.\n\n        Returns:\n            A tuple containing:\n                - dec_state (DecoderInferenceState): The initialized decoder state.\n                - dec_output (DecoderOutput): The initialized decoder output manager,\n                  containing the prefilled audio tokens.\n        \"\"\"\n        batch_size = text.shape[0]\n\n        enc_input_uncond = torch.zeros_like(text)\n        enc_input_cond = text\n        stacked_inputs = torch.stack([enc_input_uncond, enc_input_cond], dim=1)\n        enc_input = stacked_inputs.view(2 * batch_size, -1)\n\n        enc_state = ", "metadata": {"task_id": "dia/6", "ground_truth": "EncoderInferenceState.new(self.config, enc_input_cond)\n", "fpath_tuple": ["dia", "dia", "model.py"], "context_start_lineno": 182, "line_no": 367, "col_no": 20, "import_no": [11, 11]}}
{"prompt": "import time\nfrom enum import Enum\n\nimport numpy as np\nimport torch\nimport torchaudio\n\n# Assuming these imports are relative to the package structure\nfrom .audio import apply_audio_delay, build_delay_indices, build_revert_indices, revert_audio_delay\nfrom .config import DiaConfig\nfrom .state import DecoderInferenceState, DecoderOutput, EncoderInferenceState\n\n\nDEFAULT_SAMPLE_RATE = 44100\nSAMPLE_RATE_RATIO = 512\n\n\ndef _get_default_device():\n    if torch.cuda.is_available():\n        return torch.device(\"cuda\")\n    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n        return torch.device(\"mps\")\n    return torch.device(\"cpu\")\n\n\ndef _sample_next_token(\n    logits_BCxV: torch.Tensor,\n    temperature: float,\n    top_p: float,\n    top_k: int | None,\n    audio_eos_value: int,\n) -> torch.Tensor:\n    if temperature == 0.0:\n        return torch.argmax(logits_BCxV, dim=-1)\n\n    logits_BCxV = logits_BCxV / temperature\n\n    if audio_eos_value is not None and audio_eos_value >= 0:\n        top_logit_indices_BC = torch.argmax(logits_BCxV, dim=-1)\n        eos_not_highest_mask_BC = top_logit_indices_BC != audio_eos_value\n        mask_eos_unless_highest_BCxV = torch.zeros_like(logits_BCxV, dtype=torch.bool)\n        mask_eos_unless_highest_BCxV[eos_not_highest_mask_BC, audio_eos_value] = True\n        logits_BCxV = logits_BCxV.masked_fill(mask_eos_unless_highest_BCxV, -torch.inf)\n\n    if top_k is not None:\n        _, top_k_indices_BCxV = torch.topk(logits_BCxV, k=top_k, dim=-1)\n        mask = torch.ones_like(logits_BCxV, dtype=torch.bool)\n        mask = mask.scatter(dim=-1, index=top_k_indices_BCxV, value=False)\n        logits_BCxV = logits_BCxV.masked_fill(mask, -torch.inf)\n\n    if top_p < 1.0:\n        probs_BCxV = torch.softmax(logits_BCxV, dim=-1)\n        sorted_probs_BCxV, sorted_indices_BCxV = torch.sort(probs_BCxV, dim=-1, descending=True)\n        cumulative_probs_BCxV = torch.cumsum(sorted_probs_BCxV, dim=-1)\n\n        sorted_indices_to_remove_BCxV = cumulative_probs_BCxV > top_p\n        sorted_indices_to_remove_BCxV = torch.roll(sorted_indices_to_remove_BCxV, shifts=1, dims=-1)\n        sorted_indices_to_remove_BCxV[..., 0] = torch.zeros_like(sorted_indices_to_remove_BCxV[..., 0])\n\n        indices_to_remove_BCxV = torch.zeros_like(sorted_indices_to_remove_BCxV)\n        indices_to_remove_BCxV = indices_to_remove_BCxV.scatter(\n            dim=-1, index=sorted_indices_BCxV, src=sorted_indices_to_remove_BCxV\n        )\n        logits_BCxV = logits_BCxV.masked_fill(indices_to_remove_BCxV, -torch.inf)\n\n    final_probs_BCxV = torch.softmax(logits_BCxV, dim=-1)\n\n    sampled_indices_BC = torch.multinomial(final_probs_BCxV, num_samples=1)\n    sampled_indices_C = sampled_indices_BC.squeeze(-1)\n    return sampled_indices_C\n\n\nclass ComputeDtype(str, Enum):\n    FLOAT32 = \"float32\"\n    FLOAT16 = \"float16\"\n    BFLOAT16 = \"bfloat16\"\n\n    def to_dtype(self) -> torch.dtype:\n        if self == ComputeDtype.FLOAT32:\n            return torch.float32\n        elif self == ComputeDtype.FLOAT16:\n            return torch.float16\n        elif self == ComputeDtype.BFLOAT16:\n            return torch.bfloat16\n        else:\n            raise ValueError(f\"Unsupported compute dtype: {self}\")\n\n\nclass Dia:\n    def __init__(\n        self,\n        config: DiaConfig,\n        compute_dtype: str | ComputeDtype = ComputeDtype.FLOAT32,\n        device: torch.device | None = None,\n        load_dac: bool = True,\n    ):\n        \"\"\"Initializes the Dia model.\n\n        Args:\n            config: The configuration object for the model.\n            compute_dtype: The computation dtype to use.\n            device: The device to load the model onto. If None, will automatically select the best available device.\n            load_dac: Whether to load the DAC model.\n\n        Raises:\n            RuntimeError: If there is an error loading the DAC model.\n        \"\"\"\n        super().__init__()\n        self.config = config\n        self.device = device if device is not None else _get_default_device()\n        if isinstance(compute_dtype, str):\n            compute_dtype = ComputeDtype(compute_dtype)\n        self.compute_dtype = compute_dtype.to_dtype()\n", "metadata": {"task_id": "dia/7", "ground_truth": "        self.model: DiaModel = DiaModel(config, self.compute_dtype)\n", "fpath_tuple": ["dia", "dia", "model.py"], "context_start_lineno": 0, "line_no": 114, "col_no": -1, "import_no": [10, 10]}}
{"prompt": "import argparse\nimport os\nimport random\n\nimport numpy as np\nimport soundfile as sf\nimport torch\n\n\n\ndef set_seed(seed: int):\n    \"\"\"Sets the random seed for reproducibility.\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    # Ensure deterministic behavior for cuDNN (if used)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Generate audio using the Dia model.\")\n\n    parser.add_argument(\"text\", type=str, help=\"Input text for speech generation.\")\n    parser.add_argument(\n        \"--output\", type=str, required=True, help=\"Path to save the generated audio file (e.g., output.wav).\"\n    )\n\n    parser.add_argument(\n        \"--repo-id\",\n        type=str,\n        default=\"nari-labs/Dia-1.6B\",\n        help=\"Hugging Face repository ID (e.g., nari-labs/Dia-1.6B).\",\n    )\n    parser.add_argument(\n        \"--local-paths\", action=\"store_true\", help=\"Load model from local config and checkpoint files.\"\n    )\n\n    parser.add_argument(\n        \"--config\", type=str, help=\"Path to local config.json file (required if --local-paths is set).\"\n    )\n    parser.add_argument(\n        \"--checkpoint\", type=str, help=\"Path to local model checkpoint .pth file (required if --local-paths is set).\"\n    )\n    parser.add_argument(\n        \"--audio-prompt\", type=str, default=None, help=\"Path to an optional audio prompt WAV file for voice cloning.\"\n    )\n\n    gen_group = parser.add_argument_group(\"Generation Parameters\")\n    gen_group.add_argument(\n        \"--max-tokens\",\n        type=int,\n        default=None,\n        help=\"Maximum number of audio tokens to generate (defaults to config value).\",\n    )\n    gen_group.add_argument(\n        \"--cfg-scale\", type=float, default=3.0, help=\"Classifier-Free Guidance scale (default: 3.0).\"\n    )\n    gen_group.add_argument(\n        \"--temperature\", type=float, default=1.3, help=\"Sampling temperature (higher is more random, default: 0.7).\"\n    )\n    gen_group.add_argument(\"--top-p\", type=float, default=0.95, help=\"Nucleus sampling probability (default: 0.95).\")\n\n    infra_group = parser.add_argument_group(\"Infrastructure\")\n    infra_group.add_argument(\"--seed\", type=int, default=None, help=\"Random seed for reproducibility.\")\n    infra_group.add_argument(\n        \"--device\",\n        type=str,\n        default=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n        help=\"Device to run inference on (e.g., 'cuda', 'cpu', default: auto).\",\n    )\n\n    args = parser.parse_args()\n\n    # Validation for local paths\n    if args.local_paths:\n        if not args.config:\n            parser.error(\"--config is required when --local-paths is set.\")\n        if not args.checkpoint:\n            parser.error(\"--checkpoint is required when --local-paths is set.\")\n        if not os.path.exists(args.config):\n            parser.error(f\"Config file not found: {args.config}\")\n        if not os.path.exists(args.checkpoint):\n            parser.error(f\"Checkpoint file not found: {args.checkpoint}\")\n\n    # Set seed if provided\n    if args.seed is not None:\n        set_seed(args.seed)\n        print(f\"Using random seed: {args.seed}\")\n\n    # Determine device\n    device = torch.device(args.device)\n    print(f\"Using device: {device}\")\n\n    # Load model\n    print(\"Loading model...\")\n    if args.local_paths:\n        print(f\"Loading from local paths: config='{args.config}', checkpoint='{args.checkpoint}'\")\n        try:\n            model = Dia.from_local(args.config, args.checkpoint, device=device)\n        except Exception as e:\n            print(f\"Error loading local model: {e}\")\n            exit(1)\n    else:\n        print(f\"Loading from Hugging Face Hub: repo_id='{args.repo_id}'\")\n        try:\n            model = ", "metadata": {"task_id": "dia/8", "ground_truth": "Dia.from_pretrained(args.repo_id, device=device)\n", "fpath_tuple": ["dia", "cli.py"], "context_start_lineno": 0, "line_no": 110, "col_no": 20, "import_no": [8, 8]}}
{"prompt": "\n\nmodel = Dia.from_pretrained(\"nari-labs/Dia-1.6B\", compute_dtype=\"float16\")\n\ntext = \"[S1] Dia is an open weights text to dialogue model. [S2] You get full control over scripts and voices. [S1] Wow. Amazing. (laughs) [S2] Try it now on Git hub or Hugging Face.\"\n\n", "metadata": {"task_id": "dia/9", "ground_truth": "output = model.generate(text, use_torch_compile=True, verbose=True)\n", "fpath_tuple": ["dia", "example", "simple.py"], "context_start_lineno": 0, "line_no": 7, "col_no": -1, "import_no": [0, 0]}}
{"prompt": "        try:\n            loaded_model = DiaModel.from_pretrained(model_name, compute_dtype=compute_dtype.to_dtype())\n        except Exception as e:\n            raise RuntimeError(f\"Error loading model from Hugging Face Hub ({model_name})\") from e\n\n        config = loaded_model.config  # Get config from the loaded model\n        dia = cls(config, compute_dtype, device, load_dac)\n\n        dia.model = loaded_model  # Assign the already loaded model\n        dia.model.to(dia.device)\n        dia.model.eval()\n        if load_dac:\n            dia._load_dac_model()\n        return dia\n\n    def _load_dac_model(self):\n        \"\"\"Loads the Descript Audio Codec (DAC) model.\n\n        Downloads the DAC model if necessary and loads it onto the specified device.\n        Sets the DAC model to evaluation mode.\n\n        Raises:\n            RuntimeError: If downloading or loading the DAC model fails.\n        \"\"\"\n        import dac\n\n        try:\n            dac_model_path = dac.utils.download()\n            dac_model = dac.DAC.load(dac_model_path).to(self.device)\n            dac_model.eval()  # Ensure DAC is in eval mode\n        except Exception as e:\n            raise RuntimeError(\"Failed to load DAC model\") from e\n        self.dac_model = dac_model\n\n    def _encode_text(self, text: str) -> torch.Tensor:\n        \"\"\"Encodes the input text string into a tensor of token IDs using byte-level encoding.\n\n        Special tokens [S1] and [S2] are replaced by their byte values. The resulting\n        sequence is truncated to the maximum configured text length.\n\n        Args:\n            text: The input text string.\n\n        Returns:\n            A tensor containing the encoded byte token IDs.\n        \"\"\"\n        max_len = self.config.data.text_length\n\n        byte_text = text.encode(\"utf-8\")\n        # Replace special tokens with their byte values if needed by the specific tokenizer/config\n        # Assuming byte values 1 and 2 are correct placeholders based on original code\n        replaced_bytes = byte_text.replace(b\"[S1]\", b\"\\x01\").replace(b\"[S2]\", b\"\\x02\")\n        text_tokens = list(replaced_bytes)\n        return torch.tensor(\n            text_tokens[:max_len],\n            dtype=torch.long,\n            device=self.device,\n        )\n\n    def _pad_text_input(self, text_tokens: list[torch.Tensor]) -> torch.Tensor:\n        \"\"\"Pads the text input to the maximum length.\"\"\"\n        text_pad_value = self.config.data.text_pad_value\n        max_len = self.config.data.text_length\n        batch_size = len(text_tokens)\n\n        src_tokens = torch.full(\n            (batch_size, 1, max_len),\n            fill_value=text_pad_value,\n            dtype=torch.long,\n            device=self.device,\n        )\n        for i in range(batch_size):\n            current_len = len(text_tokens[i])\n            src_tokens[i, 0, :current_len] = text_tokens[i]\n        return src_tokens\n\n    def _prepare_audio_prompt(self, audio_prompts: list[torch.Tensor | None]) -> tuple[torch.Tensor, list[int]]:\n        \"\"\"Prepares the audio prompt tensor for the decoder.\n\n        Handles padding, adds the beginning-of-sequence (BOS) token, applies the\n        delay pattern, and determines the number of prefill steps for each item\n        in the batch.\n\n        Args:\n            audio_prompts: A list of audio prompt tensors (encoded DAC frames) or None.\n                           Each tensor should have shape [T, C].\n\n        Returns:\n            A tuple containing:\n                - delayed_batch (torch.Tensor): The prepared audio prompt tensor with\n                  delays applied, shape [B, T_max_padded, C].\n                - prefill_steps (list[int]): A list containing the number of valid\n                  tokens (including BOS) for each prompt in the batch.\n        \"\"\"\n        num_channels = self.config.data.channels\n        audio_bos_value = self.config.data.audio_bos_value\n        delay_pattern = self.config.data.delay_pattern\n        max_delay_pattern = max(delay_pattern)\n        batch_size = len(audio_prompts)\n\n        max_len = max(p.shape[0] if p is not None else 0 for p in audio_prompts) + max_delay_pattern\n        prefill_steps = []\n\n        prefill = torch.full(\n            (batch_size, max_len, num_channels),\n            fill_value=-1,\n            dtype=torch.int,\n            device=self.device,\n        )\n\n        prefill[:, 0, :] = audio_bos_value\n\n        for i in range(batch_size):\n            prompt = audio_prompts[i]\n            if prompt is not None:\n                prompt = prompt.to(device=self.device, dtype=torch.int)\n                prefill[i, 1 : prompt.shape[0] + 1, :] = prompt\n                prefill_steps.append(prompt.shape[0] + 1)\n            else:\n                prefill_steps.append(1)\n\n        delay_precomp = build_delay_indices(\n            B=batch_size,\n            T=max_len,\n            C=num_channels,\n            delay_pattern=delay_pattern,\n        )\n\n        delayed_batch = apply_audio_delay(\n            audio_BxTxC=prefill,\n            pad_value=-1,\n            bos_value=audio_bos_value,\n            precomp=delay_precomp,\n        )\n\n        return delayed_batch, prefill_steps\n\n    def _prepare_generation(\n        self,\n        text: torch.Tensor,\n        audio_prompts: list[torch.Tensor | None],\n        max_tokens: int | None = None,\n    ):\n        \"\"\"Initializes the model state for generation.\n\n        Encodes the text input (conditional and unconditional), prepares the\n        encoder and decoder states (including KV caches and cross-attention),\n        prepares the audio prompt, and performs the initial decoder prefill steps\n        based on the audio prompts.\n\n        Args:\n            text: The padded text input tensor, shape [B, 1, T_text].\n            audio_prompts: A list of prepared audio prompt tensors or None.\n\n        Returns:\n            A tuple containing:\n                - dec_state (DecoderInferenceState): The initialized decoder state.\n                - dec_output (DecoderOutput): The initialized decoder output manager,\n                  containing the prefilled audio tokens.\n        \"\"\"\n        batch_size = text.shape[0]\n\n        enc_input_uncond = torch.zeros_like(text)\n        enc_input_cond = text\n        stacked_inputs = torch.stack([enc_input_uncond, enc_input_cond], dim=1)\n        enc_input = stacked_inputs.view(2 * batch_size, -1)\n\n        enc_state = EncoderInferenceState.new(self.config, enc_input_cond)\n        encoder_out = self.model.encoder(enc_input, enc_state)\n\n        dec_cross_attn_cache = self.model.decoder.precompute_cross_attn_cache(\n            encoder_out, enc_state.positions, enc_state.padding_mask\n        )\n        dec_state = DecoderInferenceState.new(\n            self.config,\n            enc_state,\n            encoder_out,\n            dec_cross_attn_cache,\n            self.compute_dtype,\n            max_generation_length=max_tokens,\n        )\n        prefill, prefill_steps = self._prepare_audio_prompt(audio_prompts)\n\n        dec_output = ", "metadata": {"task_id": "dia/10", "ground_truth": "DecoderOutput.new(batch_size, self.config, self.device)\n", "fpath_tuple": ["dia", "dia", "model.py"], "context_start_lineno": 199, "line_no": 383, "col_no": 21, "import_no": [11, 11]}}
{"prompt": "#\n# For licensing see accompanying LICENSE file.\n# Copyright (C) 2025 Apple Inc. All Rights Reserved.\n#\nimport os\nimport json\nimport copy\nimport argparse\n\nimport torch\nimport numpy as np\nimport coremltools\n\nfrom llava.model.builder import load_pretrained_model\nfrom llava.utils import disable_torch_init\n\n\ndef export(args):\n    # Load model\n    disable_torch_init()\n    model_path = os.path.expanduser(args.model_path)\n", "metadata": {"task_id": "ml-fastvlm/1", "ground_truth": "    model_name = get_model_name_from_path(model_path)\n", "fpath_tuple": ["ml-fastvlm", "model_export", "export_vision_encoder.py"], "context_start_lineno": 0, "line_no": 22, "col_no": -1, "import_no": [15, 15]}}
{"prompt": "#\n# Modified from LLaVA/predict.py\n# Please see ACKNOWLEDGEMENTS for details about LICENSE\n#\nimport os\nimport argparse\n\nimport torch\nfrom PIL import Image\n\nfrom llava.utils import disable_torch_init\nfrom llava.conversation import conv_templates\nfrom llava.model.builder import load_pretrained_model\nfrom llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n\n\ndef predict(args):\n    # Remove generation config from model folder\n    # to read generation parameters from args\n    model_path = os.path.expanduser(args.model_path)\n    generation_config = None\n    if os.path.exists(os.path.join(model_path, 'generation_config.json')):\n        generation_config = os.path.join(model_path, '.generation_config.json')\n        os.rename(os.path.join(model_path, 'generation_config.json'),\n                  generation_config)\n\n    # Load model\n    disable_torch_init()\n    model_name = get_model_name_from_path(model_path)\n    tokenizer, model, image_processor, context_len = load_pretrained_model(model_path, args.model_base, model_name, device=\"mps\")\n\n    # Construct prompt\n    qs = args.prompt\n    if model.config.mm_use_im_start_end:\n        qs = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + '\\n' + qs\n    else:\n        qs = DEFAULT_IMAGE_TOKEN + '\\n' + qs\n    conv = conv_templates[args.conv_mode].copy()\n    conv.append_message(conv.roles[0], qs)\n    conv.append_message(conv.roles[1], None)\n    prompt = conv.get_prompt()\n\n    # Set the pad token id for generation\n    model.generation_config.pad_token_id = tokenizer.pad_token_id\n\n    # Tokenize prompt\n    input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).to(torch.device(\"mps\"))\n\n    # Load and preprocess image\n    image = Image.open(args.image_file).convert('RGB')\n    image_tensor = ", "metadata": {"task_id": "ml-fastvlm/2", "ground_truth": "process_images([image], image_processor, model.config)[0]\n", "fpath_tuple": ["ml-fastvlm", "predict.py"], "context_start_lineno": 0, "line_no": 51, "col_no": 19, "import_no": [13, 13]}}
{"prompt": "\"\"\"\nA model worker executes the model.\n\"\"\"\nimport argparse\nimport asyncio\nfrom concurrent.futures import ThreadPoolExecutor\nimport json\nimport time\nimport threading\nimport uuid\n\nfrom fastapi import FastAPI, Request, BackgroundTasks\nfrom fastapi.responses import StreamingResponse\nimport requests\nimport re\nimport uvicorn\nfrom functools import partial\n\nfrom llava.constants import WORKER_HEART_BEAT_INTERVAL\nfrom llava.mm_utils import process_images, load_image_from_base64, tokenizer_image_token, expand2square\nfrom llava.constants import DEFAULT_IMAGE_TOKEN\n\nimport sglang as sgl\nfrom sglang.backend.runtime_endpoint import RuntimeEndpoint\n\n\nGB = 1 << 30\n\nworker_id = str(uuid.uuid4())[:6]\n", "metadata": {"task_id": "ml-fastvlm/3", "ground_truth": "logger = build_logger(\"model_worker\", f\"model_worker_{worker_id}.log\")\n", "fpath_tuple": ["ml-fastvlm", "llava", "serve", "sglang_worker.py"], "context_start_lineno": 0, "line_no": 31, "col_no": -1, "import_no": [19, 20]}}
{"prompt": "\"\"\"\nA model worker executes the model.\n\"\"\"\nimport argparse\nimport asyncio\nfrom concurrent.futures import ThreadPoolExecutor\nimport json\nimport time\nimport threading\nimport uuid\n\nfrom fastapi import FastAPI, Request, BackgroundTasks\nfrom fastapi.responses import StreamingResponse\nimport requests\nimport re\nimport uvicorn\nfrom functools import partial\n\nfrom llava.constants import WORKER_HEART_BEAT_INTERVAL\nfrom llava.utils import (build_logger, server_error_msg,\n                         pretty_print_semaphore)\nfrom llava.constants import DEFAULT_IMAGE_TOKEN\n\nimport sglang as sgl\nfrom sglang.backend.runtime_endpoint import RuntimeEndpoint\n\n\nGB = 1 << 30\n\nworker_id = str(uuid.uuid4())[:6]\nlogger = build_logger(\"model_worker\", f\"model_worker_{worker_id}.log\")\nglobal_counter = 0\n\nmodel_semaphore = None\n\n\ndef heart_beat_worker(controller):\n    while True:\n        time.sleep(WORKER_HEART_BEAT_INTERVAL)\n        controller.send_heart_beat()\n\n\n@sgl.function\ndef pipeline(s, prompt, max_tokens):\n    for p in prompt:\n        if type(p) is str:\n            s += p\n        else:\n            s += sgl.image(p)\n    s += sgl.gen(\"response\", max_tokens=max_tokens)\n\n\nclass ModelWorker:\n    def __init__(self, controller_addr, worker_addr, sgl_endpoint,\n                 worker_id, no_register, model_name):\n        self.controller_addr = controller_addr\n        self.worker_addr = worker_addr\n        self.worker_id = worker_id\n\n        # Select backend\n        backend = RuntimeEndpoint(sgl_endpoint)\n        sgl.set_default_backend(backend)\n        model_path = backend.model_info[\"model_path\"]\n\n        if model_path.endswith(\"/\"):\n            model_path = model_path[:-1]\n        if model_name is None:\n            model_paths = model_path.split(\"/\")\n            if model_paths[-1].startswith('checkpoint-'):\n                self.model_name = model_paths[-2] + \"_\" + model_paths[-1]\n            else:\n                self.model_name = model_paths[-1]\n        else:\n            self.model_name = model_name\n\n        logger.info(f\"Loading the SGLANG model {self.model_name} on worker {worker_id} ...\")\n\n        if not no_register:\n            self.register_to_controller()\n            self.heart_beat_thread = threading.Thread(\n                target=heart_beat_worker, args=(self,), daemon=True)\n            self.heart_beat_thread.start()\n\n    def register_to_controller(self):\n        logger.info(\"Register to controller\")\n\n        url = self.controller_addr + \"/register_worker\"\n        data = {\n            \"worker_name\": self.worker_addr,\n            \"check_heart_beat\": True,\n            \"worker_status\": self.get_status()\n        }\n        r = requests.post(url, json=data)\n        assert r.status_code == 200\n\n    def send_heart_beat(self):\n        logger.info(f\"Send heart beat. Models: {[self.model_name]}. \"\n                    f\"Semaphore: {pretty_print_semaphore(model_semaphore)}. \"\n                    f\"global_counter: {global_counter}\")\n\n        url = self.controller_addr + \"/receive_heart_beat\"\n\n        while True:\n            try:\n                ret = requests.post(url, json={\n                    \"worker_name\": self.worker_addr,\n                    \"queue_length\": self.get_queue_length()}, timeout=5)\n                exist = ret.json()[\"exist\"]\n                break\n            except requests.exceptions.RequestException as e:\n                logger.error(f\"heart beat error: {e}\")\n            time.sleep(5)\n\n        if not exist:\n            self.register_to_controller()\n\n    def get_queue_length(self):\n        if model_semaphore is None:\n            return 0\n        else:\n            return args.limit_model_concurrency - model_semaphore._value + (len(\n                model_semaphore._waiters) if model_semaphore._waiters is not None else 0)\n\n    def get_status(self):\n        return {\n            \"model_names\": [self.model_name],\n            \"speed\": 1,\n            \"queue_length\": self.get_queue_length(),\n        }\n\n    async def generate_stream(self, params):\n        ori_prompt = prompt = params[\"prompt\"]\n        images = params.get(\"images\", None)\n        if images is not None and len(images) > 0:\n            if len(images) > 0:\n                if len(images) != prompt.count(DEFAULT_IMAGE_TOKEN):\n                    raise ValueError(\"Number of images does not match number of <image> tokens in prompt\")\n\n                images = [", "metadata": {"task_id": "ml-fastvlm/4", "ground_truth": "load_image_from_base64(image) for image in images]\n", "fpath_tuple": ["ml-fastvlm", "llava", "serve", "sglang_worker.py"], "context_start_lineno": 0, "line_no": 139, "col_no": 26, "import_no": [21, 21]}}
{"prompt": "\"\"\"\nA model worker executes the model.\n\"\"\"\nimport argparse\nimport asyncio\nimport json\nimport time\nimport threading\nimport uuid\n\nfrom fastapi import FastAPI, Request, BackgroundTasks\nfrom fastapi.responses import StreamingResponse\nimport requests\nimport torch\nimport uvicorn\nfrom functools import partial\n\nfrom llava.constants import WORKER_HEART_BEAT_INTERVAL\nfrom llava.utils import (build_logger, server_error_msg,\n                         pretty_print_semaphore)\nfrom llava.model.builder import load_pretrained_model\nfrom llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\nfrom transformers import TextIteratorStreamer\nfrom threading import Thread\n\n\nGB = 1 << 30\n\nworker_id = str(uuid.uuid4())[:6]\nlogger = build_logger(\"model_worker\", f\"model_worker_{worker_id}.log\")\nglobal_counter = 0\n\nmodel_semaphore = None\n\n\ndef heart_beat_worker(controller):\n\n    while True:\n        time.sleep(WORKER_HEART_BEAT_INTERVAL)\n        controller.send_heart_beat()\n\n\nclass ModelWorker:\n    def __init__(self, controller_addr, worker_addr,\n                 worker_id, no_register,\n                 model_path, model_base, model_name,\n                 load_8bit, load_4bit, device, use_flash_attn=False):\n        self.controller_addr = controller_addr\n        self.worker_addr = worker_addr\n        self.worker_id = worker_id\n        if model_path.endswith(\"/\"):\n            model_path = model_path[:-1]\n        if model_name is None:\n            model_paths = model_path.split(\"/\")\n            if model_paths[-1].startswith('checkpoint-'):\n                self.model_name = model_paths[-2] + \"_\" + model_paths[-1]\n            else:\n                self.model_name = model_paths[-1]\n        else:\n            self.model_name = model_name\n\n        self.device = device\n        logger.info(f\"Loading the model {self.model_name} on worker {worker_id} ...\")\n        self.tokenizer, self.model, self.image_processor, self.context_len = load_pretrained_model(\n            model_path, model_base, self.model_name, load_8bit, load_4bit, device=self.device, use_flash_attn=use_flash_attn)\n        self.is_multimodal = 'llava' in self.model_name.lower()\n\n        if not no_register:\n            self.register_to_controller()\n            self.heart_beat_thread = threading.Thread(\n                target=heart_beat_worker, args=(self,), daemon=True)\n            self.heart_beat_thread.start()\n\n    def register_to_controller(self):\n        logger.info(\"Register to controller\")\n\n        url = self.controller_addr + \"/register_worker\"\n        data = {\n            \"worker_name\": self.worker_addr,\n            \"check_heart_beat\": True,\n            \"worker_status\": self.get_status()\n        }\n        r = requests.post(url, json=data)\n        assert r.status_code == 200\n\n    def send_heart_beat(self):\n        logger.info(f\"Send heart beat. Models: {[self.model_name]}. \"\n                    f\"Semaphore: {pretty_print_semaphore(model_semaphore)}. \"\n                    f\"global_counter: {global_counter}\")\n\n        url = self.controller_addr + \"/receive_heart_beat\"\n\n        while True:\n            try:\n                ret = requests.post(url, json={\n                    \"worker_name\": self.worker_addr,\n                    \"queue_length\": self.get_queue_length()}, timeout=5)\n                exist = ret.json()[\"exist\"]\n                break\n            except requests.exceptions.RequestException as e:\n                logger.error(f\"heart beat error: {e}\")\n            time.sleep(5)\n\n        if not exist:\n            self.register_to_controller()\n\n    def get_queue_length(self):\n        if model_semaphore is None:\n            return 0\n        else:\n            return args.limit_model_concurrency - model_semaphore._value + (len(\n                model_semaphore._waiters) if model_semaphore._waiters is not None else 0)\n\n    def get_status(self):\n        return {\n            \"model_names\": [self.model_name],\n            \"speed\": 1,\n            \"queue_length\": self.get_queue_length(),\n        }\n\n    @torch.inference_mode()\n    def generate_stream(self, params):\n        tokenizer, model, image_processor = self.tokenizer, self.model, self.image_processor\n\n        prompt = params[\"prompt\"]\n        ori_prompt = prompt\n        images = params.get(\"images\", None)\n        num_image_tokens = 0\n        if images is not None and len(images) > 0 and self.is_multimodal:\n            if len(images) > 0:\n                if len(images) != prompt.count(DEFAULT_IMAGE_TOKEN):\n                    raise ValueError(\"Number of images does not match number of <image> tokens in prompt\")\n\n                images = [load_image_from_base64(image) for image in images]\n                image_sizes = [image.size for image in images]\n                images = process_images(images, image_processor, model.config)\n\n                if type(images) is list:\n                    images = [image.to(self.model.device, dtype=torch.float16) for image in images]\n                else:\n                    images = images.to(self.model.device, dtype=torch.float16)\n\n                replace_token = DEFAULT_IMAGE_TOKEN\n                if getattr(self.model.config, 'mm_use_im_start_end', False):\n                    replace_token = DEFAULT_IM_START_TOKEN + replace_token + DEFAULT_IM_END_TOKEN\n                prompt = prompt.replace(DEFAULT_IMAGE_TOKEN, replace_token)\n\n                num_image_tokens = prompt.count(replace_token) * model.get_vision_tower().num_patches\n            else:\n                images = None\n                image_sizes = None\n            image_args = {\"images\": images, \"image_sizes\": image_sizes}\n        else:\n            images = None\n            image_args = {}\n\n        temperature = float(params.get(\"temperature\", 1.0))\n        top_p = float(params.get(\"top_p\", 1.0))\n        max_context_length = getattr(model.config, 'max_position_embeddings', 2048)\n        max_new_tokens = min(int(params.get(\"max_new_tokens\", 256)), 1024)\n        stop_str = params.get(\"stop\", None)\n        do_sample = True if temperature > 0.001 else False\n\n        input_ids = ", "metadata": {"task_id": "ml-fastvlm/5", "ground_truth": "tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).to(self.device)\n", "fpath_tuple": ["ml-fastvlm", "llava", "serve", "model_worker.py"], "context_start_lineno": 0, "line_no": 164, "col_no": 20, "import_no": [21, 21]}}
{"prompt": "import argparse\nimport datetime\nimport json\nimport os\nimport time\n\nimport gradio as gr\nimport requests\n\nfrom llava.conversation import (default_conversation, conv_templates,\n                                SeparatorStyle)\nfrom llava.constants import LOGDIR\nimport hashlib\n\n\nlogger = build_logger(\"gradio_web_server\", \"gradio_web_server.log\")\n\nheaders = {\"User-Agent\": \"LLaVA Client\"}\n\nno_change_btn = gr.Button()\nenable_btn = gr.Button(interactive=True)\ndisable_btn = gr.Button(interactive=False)\n\npriority = {\n    \"vicuna-13b\": \"aaaaaaa\",\n    \"koala-13b\": \"aaaaaab\",\n}\n\n\ndef get_conv_log_filename():\n    t = datetime.datetime.now()\n    name = os.path.join(LOGDIR, f\"{t.year}-{t.month:02d}-{t.day:02d}-conv.json\")\n    return name\n\n\ndef get_model_list():\n    ret = requests.post(args.controller_url + \"/refresh_all_workers\")\n    assert ret.status_code == 200\n    ret = requests.post(args.controller_url + \"/list_models\")\n    models = ret.json()[\"models\"]\n    models.sort(key=lambda x: priority.get(x, x))\n    logger.info(f\"Models: {models}\")\n    return models\n\n\nget_window_url_params = \"\"\"\nfunction() {\n    const params = new URLSearchParams(window.location.search);\n    url_params = Object.fromEntries(params);\n    console.log(url_params);\n    return url_params;\n    }\n\"\"\"\n\n\ndef load_demo(url_params, request: gr.Request):\n    logger.info(f\"load_demo. ip: {request.client.host}. params: {url_params}\")\n\n    dropdown_update = gr.Dropdown(visible=True)\n    if \"model\" in url_params:\n        model = url_params[\"model\"]\n        if model in models:\n            dropdown_update = gr.Dropdown(value=model, visible=True)\n\n    state = default_conversation.copy()\n    return state, dropdown_update\n\n\ndef load_demo_refresh_model_list(request: gr.Request):\n    logger.info(f\"load_demo. ip: {request.client.host}\")\n    models = get_model_list()\n    state = default_conversation.copy()\n    dropdown_update = gr.Dropdown(\n        choices=models,\n        value=models[0] if len(models) > 0 else \"\"\n    )\n    return state, dropdown_update\n\n\ndef vote_last_response(state, vote_type, model_selector, request: gr.Request):\n    with open(get_conv_log_filename(), \"a\") as fout:\n        data = {\n            \"tstamp\": round(time.time(), 4),\n            \"type\": vote_type,\n            \"model\": model_selector,\n            \"state\": state.dict(),\n            \"ip\": request.client.host,\n        }\n        fout.write(json.dumps(data) + \"\\n\")\n\n\ndef upvote_last_response(state, model_selector, request: gr.Request):\n    logger.info(f\"upvote. ip: {request.client.host}\")\n    vote_last_response(state, \"upvote\", model_selector, request)\n    return (\"\",) + (disable_btn,) * 3\n\n\ndef downvote_last_response(state, model_selector, request: gr.Request):\n    logger.info(f\"downvote. ip: {request.client.host}\")\n    vote_last_response(state, \"downvote\", model_selector, request)\n    return (\"\",) + (disable_btn,) * 3\n\n\ndef flag_last_response(state, model_selector, request: gr.Request):\n    logger.info(f\"flag. ip: {request.client.host}\")\n    vote_last_response(state, \"flag\", model_selector, request)\n    return (\"\",) + (disable_btn,) * 3\n\n\ndef regenerate(state, image_process_mode, request: gr.Request):\n    logger.info(f\"regenerate. ip: {request.client.host}\")\n    state.messages[-1][-1] = None\n    prev_human_msg = state.messages[-2]\n    if type(prev_human_msg[1]) in (tuple, list):\n        prev_human_msg[1] = (*prev_human_msg[1][:2], image_process_mode)\n    state.skip_next = False\n    return (state, state.to_gradio_chatbot(), \"\", None) + (disable_btn,) * 5\n\n\ndef clear_history(request: gr.Request):\n    logger.info(f\"clear_history. ip: {request.client.host}\")\n    state = default_conversation.copy()\n    return (state, state.to_gradio_chatbot(), \"\", None) + (disable_btn,) * 5\n\n\ndef add_text(state, text, image, image_process_mode, request: gr.Request):\n    logger.info(f\"add_text. ip: {request.client.host}. len: {len(text)}\")\n    if len(text) <= 0 and image is None:\n        state.skip_next = True\n        return (state, state.to_gradio_chatbot(), \"\", None) + (no_change_btn,) * 5\n    if args.moderate:\n", "metadata": {"task_id": "ml-fastvlm/6", "ground_truth": "        flagged = violates_moderation(text)\n", "fpath_tuple": ["ml-fastvlm", "llava", "serve", "gradio_web_server.py"], "context_start_lineno": 0, "line_no": 133, "col_no": -1, "import_no": [12, 13]}}
{"prompt": "        input_ids=input_ids,\n        labels=targets,\n    )\n\n\ndef preprocess_plain(\n    sources: Sequence[str],\n    tokenizer: transformers.PreTrainedTokenizer,\n) -> Dict:\n    # add end signal and concatenate together\n    conversations = []\n    for source in sources:\n        assert len(source) == 2\n        assert DEFAULT_IMAGE_TOKEN in source[0]['value']\n        source[0]['value'] = DEFAULT_IMAGE_TOKEN\n        conversation = source[0]['value'] + source[1]['value'] + conversation_lib.default_conversation.sep\n        conversations.append(conversation)\n    # tokenize conversations\n    input_ids = [tokenizer_image_token(prompt, tokenizer, return_tensors='pt') for prompt in conversations]\n    targets = copy.deepcopy(input_ids)\n    for target, source in zip(targets, sources):\n        tokenized_len = len(tokenizer_image_token(source[0]['value'], tokenizer))\n        target[:tokenized_len] = IGNORE_INDEX\n\n    return dict(input_ids=input_ids, labels=targets)\n\n\ndef preprocess(\n    sources: Sequence[str],\n    tokenizer: transformers.PreTrainedTokenizer,\n    has_image: bool = False\n) -> Dict:\n    \"\"\"\n    Given a list of sources, each is a conversation list. This transform:\n    1. Add signal '### ' at the beginning each sentence, with end signal '\\n';\n    2. Concatenate conversations together;\n    3. Tokenize the concatenated conversation;\n    4. Make a deepcopy as the target. Mask human words with IGNORE_INDEX.\n    \"\"\"\n    # print(\"conversation:\",conversation_lib.default_conversation.version)\n    # conversation_lib.default_conversation.version == \"qwen_v2\"\n\n    if conversation_lib.default_conversation.sep_style == conversation_lib.SeparatorStyle.PLAIN:\n        return preprocess_plain(sources, tokenizer)\n    if conversation_lib.default_conversation.sep_style == conversation_lib.SeparatorStyle.LLAMA_2:\n        return preprocess_llama_2(sources, tokenizer, has_image=has_image)\n    if conversation_lib.default_conversation.version.startswith(\"v1\"):\n        # print('--v1--')\n        return preprocess_v1(sources, tokenizer, has_image=has_image)\n    if conversation_lib.default_conversation.version == \"mpt\":\n        # print('--mpt--')\n        return preprocess_mpt(sources, tokenizer, has_image=has_image)\n    # fix: add qwen2\n    if conversation_lib.default_conversation.version.startswith(\"qwen_v2\"):\n        # print('--qwen_v2--')\n        return preprocess_qwen_2(sources, tokenizer, has_image=has_image)\n    # add end signal and concatenate together\n    conversations = []\n    for source in sources:\n        header = f\"{conversation_lib.default_conversation.system}\\n\\n\"\n        conversation = _add_speaker_and_signal(header, source)\n        conversations.append(conversation)\n    # tokenize conversations\n\n    def get_tokenize_len(prompts):\n        return [len(tokenizer_image_token(prompt, tokenizer)) for prompt in prompts]\n\n    if has_image:\n        input_ids = [tokenizer_image_token(prompt, tokenizer, return_tensors='pt') for prompt in conversations]\n    else:\n        conversations_tokenized = _tokenize_fn(conversations, tokenizer)\n        input_ids = conversations_tokenized[\"input_ids\"]\n\n    targets = copy.deepcopy(input_ids)\n    for target, source in zip(targets, sources):\n        if has_image:\n            tokenized_lens = get_tokenize_len([header] + [s[\"value\"] for s in source])\n        else:\n            tokenized_lens = _tokenize_fn([header] + [s[\"value\"] for s in source], tokenizer)[\"input_ids_lens\"]\n        speakers = [sentence[\"from\"] for sentence in source]\n        _mask_targets(target, tokenized_lens, speakers)\n\n    return dict(input_ids=input_ids, labels=targets)\n\n\nclass LazySupervisedDataset(Dataset):\n    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n\n    def __init__(self, data_path: List[str],\n                 tokenizer: transformers.PreTrainedTokenizer,\n                 data_args: DataArguments):\n        super(LazySupervisedDataset, self).__init__()\n        #list_data_dict = json.load(open(data_path, \"r\"))\n        list_data_dict = []\n        for i, _data_path in enumerate(data_path):\n            data = json.load(open(_data_path, \"r\"))\n            data = [{**entry, 'img_path_idx': i} for entry in data]\n            list_data_dict += data\n\n        self.tokenizer = tokenizer\n        self.list_data_dict = list_data_dict\n        self.data_args = data_args\n\n    def __len__(self):\n        return len(self.list_data_dict)\n\n    @property\n    def lengths(self):\n        length_list = []\n        for sample in self.list_data_dict:\n            img_tokens = 128 if 'image' in sample else 0\n            length_list.append(sum(len(conv['value'].split()) for conv in sample['conversations']) + img_tokens)\n        return length_list\n\n    @property\n    def modality_lengths(self):\n        length_list = []\n        for sample in self.list_data_dict:\n            cur_len = sum(len(conv['value'].split()) for conv in sample['conversations'])\n            cur_len = cur_len if 'image' in sample else -cur_len\n            length_list.append(cur_len)\n        return length_list\n\n    def get_sample(self, i) -> Dict[str, torch.Tensor]:\n        sources = self.list_data_dict[i]\n        if isinstance(i, int):\n            sources = [sources]\n        assert len(sources) == 1, \"Don't know why it is wrapped to a list\"  # FIXME\n        if 'image' in sources[0]:\n            image_file = self.list_data_dict[i]['image']\n            img_path_idx = self.list_data_dict[i]['img_path_idx']\n            image_folder = self.data_args.image_folder[img_path_idx]\n            processor = self.data_args.image_processor\n            image = Image.open(os.path.join(image_folder, image_file)).convert('RGB')\n            image_size = image.size\n            if self.data_args.image_aspect_ratio == 'pad':\n                def expand2square(pil_img, background_color):\n                    width, height = pil_img.size\n                    if width == height:\n                        return pil_img\n                    elif width > height:\n                        result = Image.new(pil_img.mode, (width, width), background_color)\n                        result.paste(pil_img, (0, (width - height) // 2))\n                        return result\n                    else:\n                        result = Image.new(pil_img.mode, (height, height), background_color)\n                        result.paste(pil_img, ((height - width) // 2, 0))\n                        return result\n\n                image = expand2square(image, tuple(int(x * 255) for x in processor.image_mean))\n                image = processor.preprocess(image, return_tensors='pt')['pixel_values'][0]\n            elif self.data_args.image_aspect_ratio == \"anyres\" or \"anyres_max\" in self.data_args.image_aspect_ratio:\n                image = ", "metadata": {"task_id": "ml-fastvlm/7", "ground_truth": "process_anyres_image(image, self.data_args.image_processor, self.data_args.image_grid_pinpoints)\n", "fpath_tuple": ["ml-fastvlm", "llava", "train", "train_qwen.py"], "context_start_lineno": 804, "line_no": 957, "col_no": 24, "import_no": [36, 36]}}
{"prompt": "\"\"\"\nUsage:\npython3 -m llava.model.make_delta --base ~/model_weights/llama-7b --target ~/model_weights/llava-7b --delta ~/model_weights/llava-7b-delta --hub-repo-id liuhaotian/llava-7b-delta\n\"\"\"\nimport argparse\n\nimport torch\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n\ndef make_delta(base_model_path, target_model_path, delta_path, hub_repo_id):\n    print(\"Loading base model\")\n    base = AutoModelForCausalLM.from_pretrained(\n        base_model_path, torch_dtype=torch.float16, low_cpu_mem_usage=True)\n\n    print(\"Loading target model\")\n", "metadata": {"task_id": "ml-fastvlm/8", "ground_truth": "    auto_upgrade(target_model_path)\n", "fpath_tuple": ["ml-fastvlm", "llava", "model", "make_delta.py"], "context_start_lineno": 0, "line_no": 18, "col_no": -1, "import_no": [9, 9]}}
{"prompt": "#    Copyright 2023 Haotian Liu\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\n\nfrom abc import ABC, abstractmethod\n\nimport torch\nimport torch.nn as nn\n\nfrom .multimodal_projector.builder import build_vision_projector\n\nfrom llava.constants import IGNORE_INDEX, IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_PATCH_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n\nfrom llava.mm_utils import get_anyres_image_grid_shape\n\n\nclass LlavaMetaModel:\n\n    def __init__(self, config):\n        super(LlavaMetaModel, self).__init__(config)\n\n        if hasattr(config, \"mm_vision_tower\"):\n            self.vision_tower = ", "metadata": {"task_id": "ml-fastvlm/9", "ground_truth": "build_vision_tower(config, delay_load=True)\n", "fpath_tuple": ["ml-fastvlm", "llava", "model", "llava_arch.py"], "context_start_lineno": 0, "line_no": 34, "col_no": 32, "import_no": [20, 20]}}
{"prompt": "#\n# For licensing see accompanying LICENSE file.\n# Copyright (C) 2025 Apple Inc. All Rights Reserved.\n#\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom transformers import CLIPImageProcessor\n\n\nclass MobileCLIPVisionTower(nn.Module):\n    def __init__(self, vision_tower, args, delay_load=False):\n        super().__init__()\n\n        self.is_loaded = False\n        self.vision_tower_name = vision_tower\n        self.tune_vision_tower = getattr(args, 'unfreeze_mm_vision_tower', False)\n        self.input_image_size = int(vision_tower.split(\"_\")[-1])\n\n        # Delay load is disabled for now\n        if not delay_load:\n            self.load_model()\n        elif getattr(args, 'unfreeze_mm_vision_tower', False):\n            self.load_model()\n        else:\n            model_cfg = ", "metadata": {"task_id": "ml-fastvlm/10", "ground_truth": "mobileclip.load_model_config(self.vision_tower_name)\n", "fpath_tuple": ["ml-fastvlm", "llava", "model", "multimodal_encoder", "mobileclip_encoder.py"], "context_start_lineno": 0, "line_no": 27, "col_no": 24, "import_no": [9, 9]}}
{"prompt": "import gradio as gr\nimport numpy as np\nimport os\nimport torch\nimport random\n\nfrom accelerate import infer_auto_device_map, load_checkpoint_and_dispatch, init_empty_weights\nfrom PIL import Image\n\nfrom data.transforms import ImageTransform\nfrom inferencer import InterleaveInferencer\nfrom modeling.autoencoder import load_ae\nfrom modeling.bagel.qwen2_navit import NaiveCache\nfrom modeling.bagel import (\n    BagelConfig, Bagel, Qwen2Config, Qwen2ForCausalLM,\n    SiglipVisionConfig, SiglipVisionModel\n)\nfrom modeling.qwen2 import Qwen2Tokenizer\n\nimport argparse\nfrom accelerate.utils import BnbQuantizationConfig, load_and_quantize_model\n\n\nparser = argparse.ArgumentParser() \nparser.add_argument(\"--server_name\", type=str, default=\"127.0.0.1\")\nparser.add_argument(\"--server_port\", type=int, default=7860)\nparser.add_argument(\"--share\", action=\"store_true\")\nparser.add_argument(\"--model_path\", type=str, default=\"models/BAGEL-7B-MoT\")\nparser.add_argument(\"--mode\", type=int, default=1)\nparser.add_argument(\"--zh\", action=\"store_true\")\nargs = parser.parse_args()\n\n# Model Initialization\nmodel_path = args.model_path #Download from https://huggingface.co/ByteDance-Seed/BAGEL-7B-MoT to models/BAGEL-7B-MoT\n\nmodel_path = args.model_path \n\nllm_config = Qwen2Config.from_json_file(os.path.join(model_path, \"llm_config.json\"))\nllm_config.qk_norm = True\nllm_config.tie_word_embeddings = False\nllm_config.layer_module = \"Qwen2MoTDecoderLayer\"\n\nvit_config = SiglipVisionConfig.from_json_file(os.path.join(model_path, \"vit_config.json\"))\nvit_config.rope = False\nvit_config.num_hidden_layers -= 1\n\nvae_model, vae_config = load_ae(local_path=os.path.join(model_path, \"ae.safetensors\"))\n\nconfig = BagelConfig(\n    visual_gen=True,\n    visual_und=True,\n    llm_config=llm_config, \n    vit_config=vit_config,\n    vae_config=vae_config,\n    vit_max_num_patch_per_side=70,\n    connector_act='gelu_pytorch_tanh',\n    latent_patch_size=2,\n    max_latent_size=64,\n)\n\nwith init_empty_weights():\n    language_model = Qwen2ForCausalLM(llm_config)\n    vit_model      = SiglipVisionModel(vit_config)\n    model          = Bagel(language_model, vit_model, config)\n    model.vit_model.vision_model.embeddings.convert_conv2d_to_linear(vit_config, meta=True)\n\ntokenizer = Qwen2Tokenizer.from_pretrained(model_path)\ntokenizer, new_token_ids, _ = ", "metadata": {"task_id": "Bagel/1", "ground_truth": "add_special_tokens(tokenizer)\n", "fpath_tuple": ["Bagel", "app.py"], "context_start_lineno": 0, "line_no": 68, "col_no": 30, "import_no": [9, 9]}}
{"prompt": "llm_config.qk_norm = True\nllm_config.tie_word_embeddings = False\nllm_config.layer_module = \"Qwen2MoTDecoderLayer\"\n\nvit_config = SiglipVisionConfig.from_json_file(os.path.join(model_path, \"vit_config.json\"))\nvit_config.rope = False\nvit_config.num_hidden_layers -= 1\n\nvae_model, vae_config = load_ae(local_path=os.path.join(model_path, \"ae.safetensors\"))\n\nconfig = BagelConfig(\n    visual_gen=True,\n    visual_und=True,\n    llm_config=llm_config, \n    vit_config=vit_config,\n    vae_config=vae_config,\n    vit_max_num_patch_per_side=70,\n    connector_act='gelu_pytorch_tanh',\n    latent_patch_size=2,\n    max_latent_size=64,\n)\n\nwith init_empty_weights():\n    language_model = Qwen2ForCausalLM(llm_config)\n    vit_model      = SiglipVisionModel(vit_config)\n    model          = Bagel(language_model, vit_model, config)\n    model.vit_model.vision_model.embeddings.convert_conv2d_to_linear(vit_config, meta=True)\n\ntokenizer = Qwen2Tokenizer.from_pretrained(model_path)\ntokenizer, new_token_ids, _ = add_special_tokens(tokenizer)\n\nvae_transform = ImageTransform(1024, 512, 16)\nvit_transform = ImageTransform(980, 224, 14)\n\n# Model Loading and Multi GPU Infernece Preparing\ndevice_map = infer_auto_device_map(\n    model,\n    max_memory={i: \"80GiB\" for i in range(torch.cuda.device_count())},\n    no_split_module_classes=[\"Bagel\", \"Qwen2MoTDecoderLayer\"],\n)\n\nsame_device_modules = [\n    'language_model.model.embed_tokens',\n    'time_embedder',\n    'latent_pos_embed',\n    'vae2llm',\n    'llm2vae',\n    'connector',\n    'vit_pos_embed'\n]\n\nif torch.cuda.device_count() == 1:\n    first_device = device_map.get(same_device_modules[0], \"cuda:0\")\n    for k in same_device_modules:\n        if k in device_map:\n            device_map[k] = first_device\n        else:\n            device_map[k] = \"cuda:0\"\nelse:\n    first_device = device_map.get(same_device_modules[0])\n    for k in same_device_modules:\n        if k in device_map:\n            device_map[k] = first_device\n\nif args.mode == 1:\n    model = load_checkpoint_and_dispatch(\n        model,\n        checkpoint=os.path.join(model_path, \"ema.safetensors\"),\n        device_map=device_map,\n        offload_buffers=True,\n        offload_folder=\"offload\",\n        dtype=torch.bfloat16,\n        force_hooks=True,\n    ).eval()\nelif args.mode == 2: # NF4\n    bnb_quantization_config = BnbQuantizationConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_use_double_quant=False, bnb_4bit_quant_type=\"nf4\")\n    model = load_and_quantize_model(\n        model, \n        weights_location=os.path.join(model_path, \"ema.safetensors\"), \n        bnb_quantization_config=bnb_quantization_config,\n        device_map=device_map,\n        offload_folder=\"offload\",\n    ).eval()\nelif args.mode == 3: # INT8\n    bnb_quantization_config = BnbQuantizationConfig(load_in_8bit=True, torch_dtype=torch.float32)\n    model = load_and_quantize_model(\n        model, \n        weights_location=os.path.join(model_path, \"ema.safetensors\"), \n        bnb_quantization_config=bnb_quantization_config,\n        device_map=device_map,\n        offload_folder=\"offload\",\n    ).eval()\nelse:\n    raise NotImplementedError\n\n# Inferencer Preparing \ninferencer = InterleaveInferencer(\n    model=model,\n    vae_model=vae_model,\n    tokenizer=tokenizer,\n    vae_transform=vae_transform,\n    vit_transform=vit_transform,\n    new_token_ids=new_token_ids,\n)\n\n\ndef set_seed(seed):\n    \"\"\"Set random seeds for reproducibility\"\"\"\n    if seed > 0:\n        random.seed(seed)\n        np.random.seed(seed)\n        torch.manual_seed(seed)\n        if torch.cuda.is_available():\n            torch.cuda.manual_seed(seed)\n            torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n    return seed\n\n\n# Text to Image function with thinking option and hyperparameters\ndef text_to_image(prompt, show_thinking=False, cfg_text_scale=4.0, cfg_interval=0.4, \n                 timestep_shift=3.0, num_timesteps=50, \n                 cfg_renorm_min=0.0, cfg_renorm_type=\"global\", \n                 max_think_token_n=1024, do_sample=False, text_temperature=0.3,\n                 seed=0, image_ratio=\"1:1\"):\n    # Set seed for reproducibility\n    set_seed(seed)\n\n    if image_ratio == \"1:1\":\n        image_shapes = (1024, 1024)\n    elif image_ratio == \"4:3\":\n        image_shapes = (768, 1024)\n    elif image_ratio == \"3:4\":\n        image_shapes = (1024, 768) \n    elif image_ratio == \"16:9\":\n        image_shapes = (576, 1024)\n    elif image_ratio == \"9:16\":\n        image_shapes = (1024, 576) \n    \n    # Set hyperparameters\n    inference_hyper = dict(\n        max_think_token_n=max_think_token_n if show_thinking else 1024,\n        do_sample=do_sample if show_thinking else False,\n        text_temperature=text_temperature if show_thinking else 0.3,\n        cfg_text_scale=cfg_text_scale,\n        cfg_interval=[cfg_interval, 1.0],  # End fixed at 1.0\n        timestep_shift=timestep_shift,\n        num_timesteps=num_timesteps,\n        cfg_renorm_min=cfg_renorm_min,\n        cfg_renorm_type=cfg_renorm_type,\n        image_shapes=image_shapes,\n    )\n    \n    # Call inferencer with or without think parameter based on user choice\n    result = inferencer(text=prompt, think=show_thinking, **inference_hyper)\n    return result[\"image\"], result.get(\"text\", None)\n\n\n# Image Understanding function with thinking option and hyperparameters\ndef image_understanding(image: Image.Image, prompt: str, show_thinking=False, \n                        do_sample=False, text_temperature=0.3, max_new_tokens=512):\n    if image is None:\n        return \"Please upload an image.\"\n\n    if isinstance(image, np.ndarray):\n        image = Image.fromarray(image)\n\n    image = ", "metadata": {"task_id": "Bagel/2", "ground_truth": "pil_img2rgb(image)\n", "fpath_tuple": ["Bagel", "app.py"], "context_start_lineno": 38, "line_no": 207, "col_no": 12, "import_no": [9, 9]}}
{"prompt": "        metadata={\"help\": \"W&B resume mode: 'allow', 'must', or 'never'.\"}\n    )\n    wandb_offline: bool = field(\n        default=False,\n        metadata={\"help\": \"Run W&B in offline mode (logs locally, sync later).\"}\n    )\n\n    # --- reproducibility & resume ---\n    global_seed: int = field(\n        default=4396,\n        metadata={\"help\": \"Base random seed; actual seed is offset by rank for DDP.\"}\n    )\n    auto_resume: bool = field(\n        default=False,\n        metadata={\"help\": \"Automatically pick up the latest checkpoint found in checkpoint_dir.\"}\n    )\n    resume_from: str = field(\n        default=None,\n        metadata={\"help\": \"Explicit checkpoint path to resume from (overrides auto_resume).\" }\n    )\n    resume_model_only: bool = field(\n        default=False,\n        metadata={\"help\": \"Load only model weights, ignoring optimizer/scheduler states.\"}\n    )\n    finetune_from_ema: bool = field(\n        default=False,\n        metadata={\"help\": \"When resume_model_only=True, load the EMA (exponential moving average) weights instead of raw weights.\"}\n    )\n    finetune_from_hf: bool = field(\n        default=False,\n        metadata={\"help\": \"Whether finetune from HugginFace model.\"}\n    )\n\n    # --- reporting frequency ---\n    log_every: int = field(\n        default=10,\n        metadata={\"help\": \"Print / log every N training steps.\"}\n    )\n    save_every: int = field(\n        default=2000,\n        metadata={\"help\": \"Save a checkpoint every N training steps.\"}\n    )\n    total_steps: int = field(\n        default=500_000,\n        metadata={\"help\": \"Total number of optimizer steps to train for.\"}\n    )\n\n    # --- optimization & scheduler ---\n    warmup_steps: int = field(\n        default=2000,\n        metadata={\"help\": \"Linear warm-up steps before applying the main LR schedule.\"}\n    )\n    lr_scheduler: str = field(\n        default=\"constant\",\n        metadata={\"help\": \"Type of LR schedule: 'constant' or 'cosine'.\"}\n    )\n    lr: float = field(\n        default=1e-4,\n        metadata={\"help\": \"Peak learning rate after warm-up.\"}\n    )\n    min_lr: float = field(\n        default=1e-7,\n        metadata={\"help\": \"Minimum learning rate for cosine schedule (ignored for constant).\"}\n    )\n    beta1: float = field(\n        default=0.9,\n        metadata={\"help\": \"AdamW \u03b2\u2081 coefficient.\"}\n    )\n    beta2: float = field(\n        default=0.95,\n        metadata={\"help\": \"AdamW \u03b2\u2082 coefficient.\"}\n    )\n    eps: float = field(\n        default=1e-15,\n        metadata={\"help\": \"AdamW \u03b5 for numerical stability.\"}\n    )\n    ema: float = field(\n        default=0.9999,\n        metadata={\"help\": \"Decay rate for the exponential moving average of model weights.\"}\n    )\n    max_grad_norm: int = field(\n        default=1.0,\n        metadata={\"help\": \"Gradient clipping threshold (L2 norm).\"}\n    )\n    timestep_shift: float = field(\n        default=1.0,\n        metadata={\"help\": \"Shift applied to diffusion timestep indices (for latent prediction).\"}\n    )\n    mse_weight: float = field(\n        default=1.0,\n        metadata={\"help\": \"Scaling factor for the image-reconstruction MSE loss term.\"}\n    )\n    ce_weight: float = field(\n        default=1.0,\n        metadata={\"help\": \"Scaling factor for the language cross-entropy loss term.\"}\n    )\n    ce_loss_reweighting: bool = field(\n        default=False,\n        metadata={\"help\": \"Reweight CE loss by token importance (provided via ce_loss_weights).\"}\n    )\n    expected_num_tokens: int = field(\n        default=32768,\n        metadata={\"help\": \"Soft target token count; yield the batch once it reaches or exceeds this size.\"}\n    )\n\n    # --- distributed training / FSDP ---\n    num_replicate: int = field(\n        default=1,\n        metadata={\"help\": \"Number of model replicas per GPU rank for tensor parallelism.\"}\n    )\n    num_shard: int = field(\n        default=8,\n        metadata={\"help\": \"Number of parameter shards when using FSDP HYBRID_SHARD.\"}\n    )\n    sharding_strategy: str = field(\n        default=\"HYBRID_SHARD\",\n        metadata={\"help\": \"FSDP sharding strategy: FULL_SHARD, SHARD_GRAD_OP, HYBRID_SHARD, etc.\"}\n    )\n    backward_prefetch: str = field(\n        default=\"BACKWARD_PRE\",\n        metadata={\"help\": \"FSDP backward prefetch strategy (BACKWARD_PRE or NO_PREFETCH).\"}\n    )\n    cpu_offload: bool = field(\n        default=False,\n        metadata={\"help\": \"Enable FSDP parameter offload to CPU.\"}\n    )\n\n    # --- module freezing ---\n    freeze_llm: bool = field(\n        default=False,\n        metadata={\"help\": \"Keep language-model weights fixed (no gradient updates).\"}\n    )\n    freeze_vit: bool = field(\n        default=False,\n        metadata={\"help\": \"Keep ViT weights fixed during training.\"}\n    )\n    freeze_vae: bool = field(\n        default=True,\n        metadata={\"help\": \"Keep VAE weights fixed; only predict latents, don\u2019t fine-tune encoder/decoder.\"}\n    )\n    freeze_und: bool = field(\n        default=False,\n        metadata={\"help\": \"Freeze the visual understanding connector layers.\"}\n    )\n    copy_init_moe: bool = field(\n        default=True,\n        metadata={\"help\": \"Duplicate initial MoE experts so each has identical initialisation.\"}\n    )\n    use_flex: bool = field(\n        default=False,\n        metadata={\"help\": \"Enable FLEX (flash-ext friendly) packing algorithm for sequence data.\"}\n    )\n\n\ndef main():\n    assert torch.cuda.is_available()\n    dist.init_process_group(\"nccl\")\n    device = dist.get_rank() % torch.cuda.device_count()\n    torch.cuda.set_device(device)\n    parser = HfArgumentParser((ModelArguments, DataArguments, TrainingArguments))\n    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n\n    # Setup logging:\n    if dist.get_rank() == 0:\n        os.makedirs(training_args.results_dir, exist_ok=True)\n        os.makedirs(training_args.checkpoint_dir, exist_ok=True)\n        logger = create_logger(training_args.results_dir, dist.get_rank())\n        wandb.init(\n            project=training_args.wandb_project, \n            id=f\"{training_args.wandb_name}-run{training_args.wandb_runid}\", \n            name=training_args.wandb_name, \n            resume=training_args.wandb_resume,\n            mode=\"offline\" if training_args.wandb_offline else \"online\"\n        )\n        wandb.config.update(training_args)\n        wandb.config.update(model_args)\n        wandb.config.update(data_args)\n    else:\n        logger = create_logger(None, dist.get_rank())\n    dist.barrier()\n    logger.info(f'Training arguments {training_args}')\n    logger.info(f'Model arguments {model_args}')\n    logger.info(f'Data arguments {data_args}')\n\n    # prepare auto resume logic:\n    if training_args.auto_resume:\n", "metadata": {"task_id": "Bagel/3", "ground_truth": "        resume_from = get_latest_ckpt(training_args.checkpoint_dir)\n", "fpath_tuple": ["Bagel", "train", "pretrain_unified_navit.py"], "context_start_lineno": 186, "line_no": 373, "col_no": -1, "import_no": [32, 32]}}
{"prompt": "    device = dist.get_rank() % torch.cuda.device_count()\n    torch.cuda.set_device(device)\n    parser = HfArgumentParser((ModelArguments, DataArguments, TrainingArguments))\n    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n\n    # Setup logging:\n    if dist.get_rank() == 0:\n        os.makedirs(training_args.results_dir, exist_ok=True)\n        os.makedirs(training_args.checkpoint_dir, exist_ok=True)\n        logger = create_logger(training_args.results_dir, dist.get_rank())\n        wandb.init(\n            project=training_args.wandb_project, \n            id=f\"{training_args.wandb_name}-run{training_args.wandb_runid}\", \n            name=training_args.wandb_name, \n            resume=training_args.wandb_resume,\n            mode=\"offline\" if training_args.wandb_offline else \"online\"\n        )\n        wandb.config.update(training_args)\n        wandb.config.update(model_args)\n        wandb.config.update(data_args)\n    else:\n        logger = create_logger(None, dist.get_rank())\n    dist.barrier()\n    logger.info(f'Training arguments {training_args}')\n    logger.info(f'Model arguments {model_args}')\n    logger.info(f'Data arguments {data_args}')\n\n    # prepare auto resume logic:\n    if training_args.auto_resume:\n        resume_from = get_latest_ckpt(training_args.checkpoint_dir)\n        if resume_from is None:\n            resume_from = training_args.resume_from\n            resume_model_only = training_args.resume_model_only\n            if resume_model_only:\n                finetune_from_ema = training_args.finetune_from_ema\n            else:\n                finetune_from_ema = False\n        else:\n            resume_model_only = False\n            finetune_from_ema = False\n    else:\n        resume_from = training_args.resume_from\n        resume_model_only = training_args.resume_model_only\n        if resume_model_only:\n            finetune_from_ema = training_args.finetune_from_ema\n        else:\n            finetune_from_ema = False\n\n    # Set seed:\n    seed = training_args.global_seed * dist.get_world_size() + dist.get_rank()\n    set_seed(seed)\n\n    # Setup model:\n    if training_args.finetune_from_hf:\n        llm_config = Qwen2Config.from_json_file(os.path.join(model_args.model_path, \"llm_config.json\"))\n    else:\n        llm_config = Qwen2Config.from_pretrained(model_args.llm_path)\n    llm_config.layer_module = model_args.layer_module\n    llm_config.qk_norm = model_args.llm_qk_norm\n    llm_config.tie_word_embeddings = model_args.tie_word_embeddings\n    llm_config.freeze_und = training_args.freeze_und\n    if training_args.finetune_from_hf:\n        language_model = Qwen2ForCausalLM(llm_config)\n    else:\n        language_model = Qwen2ForCausalLM.from_pretrained(model_args.llm_path, config=llm_config)\n    if training_args.copy_init_moe:\n        language_model.init_moe()\n\n    if training_args.visual_und:  \n        if training_args.finetune_from_hf:\n            vit_config = SiglipVisionConfig.from_json_file(os.path.join(model_args.model_path, \"vit_config.json\"))\n        else:\n            vit_config = SiglipVisionConfig.from_pretrained(model_args.vit_path)\n        vit_config.num_hidden_layers = vit_config.num_hidden_layers + 1 + model_args.vit_select_layer\n        vit_config.rope = model_args.vit_rope\n        if training_args.finetune_from_hf:\n            vit_model = SiglipVisionModel(vit_config)\n        else:\n            vit_model = SiglipVisionModel.from_pretrained(model_args.vit_path, config=vit_config)\n\n    if training_args.visual_gen:\n        vae_model, vae_config = load_ae(\n            local_path=os.path.join(model_args.model_path, \"ae.safetensors\") \n            if training_args.finetune_from_hf else model_args.vae_path\n        )\n\n    config = BagelConfig(\n        visual_gen=training_args.visual_gen,\n        visual_und=training_args.visual_und,\n        llm_config=llm_config, \n        vit_config=vit_config if training_args.visual_und else None,\n        vae_config=vae_config if training_args.visual_gen else None,\n        latent_patch_size=model_args.latent_patch_size,\n        max_latent_size=model_args.max_latent_size,\n        vit_max_num_patch_per_side=model_args.vit_max_num_patch_per_side,\n        connector_act=model_args.connector_act,\n        interpolate_pos=model_args.interpolate_pos,\n        timestep_shift=training_args.timestep_shift,\n    )\n    model = Bagel(\n        language_model, \n        vit_model if training_args.visual_und else None, \n        config\n    )\n\n    if training_args.visual_und:\n        model.vit_model.vision_model.embeddings.convert_conv2d_to_linear(vit_config)\n\n    # Setup tokenizer for model:\n    tokenizer = Qwen2Tokenizer.from_pretrained(model_args.model_path if training_args.finetune_from_hf else model_args.llm_path)\n    tokenizer, new_token_ids, num_new_tokens = add_special_tokens(tokenizer)\n    if num_new_tokens > 0:\n        model.language_model.resize_token_embeddings(len(tokenizer))\n        model.config.llm_config.vocab_size = len(tokenizer)\n        model.language_model.config.vocab_size = len(tokenizer)\n\n    # maybe freeze something:\n    if training_args.freeze_vae and training_args.visual_gen:\n        for param in vae_model.parameters():\n            param.requires_grad = False\n    if training_args.freeze_llm:\n        model.language_model.eval()\n        for param in model.language_model.parameters():\n            param.requires_grad = False\n    if training_args.freeze_vit and training_args.visual_und:\n        model.vit_model.eval()\n        for param in model.vit_model.parameters():\n            param.requires_grad = False\n\n    # Setup FSDP and load pretrained model:\n    fsdp_config = FSDPConfig(\n        sharding_strategy=training_args.sharding_strategy,\n        backward_prefetch=training_args.backward_prefetch,\n        cpu_offload=training_args.cpu_offload,\n        num_replicate=training_args.num_replicate,\n        num_shard=training_args.num_shard,\n    )\n    ema_model = deepcopy(model)\n    model, ema_model = FSDPCheckpoint.try_load_ckpt(\n        resume_from, logger, model, ema_model, resume_from_ema=finetune_from_ema\n    )\n    ema_model = ", "metadata": {"task_id": "Bagel/4", "ground_truth": "fsdp_ema_setup(ema_model, fsdp_config)\n", "fpath_tuple": ["Bagel", "train", "pretrain_unified_navit.py"], "context_start_lineno": 340, "line_no": 485, "col_no": 16, "import_no": [33, 36]}}
{"prompt": "from transformers.modeling_utils import PreTrainedModel\n\nfrom .qwen2_navit import NaiveCache\nfrom .modeling_utils import MLPconnector, TimestepEmbedder, PositionEmbedding\n\nfrom tqdm import tqdm\n\n\nclass BagelConfig(PretrainedConfig):\n    def __init__(\n        self,\n        visual_gen=True,\n        visual_und=True,\n        llm_config=None,\n        vit_config=None,\n        vae_config=None,\n        latent_patch_size=2,\n        max_latent_size=32,\n        vit_max_num_patch_per_side=70,\n        connector_act=\"gelu_pytorch_tanh\",\n        interpolate_pos=False,\n        timestep_shift=1.0,\n        **kwargs\n    ):\n        super().__init__(**kwargs)\n        self.visual_gen = visual_gen\n        self.visual_und = visual_und\n        self.llm_config = llm_config\n        self.vit_config = vit_config\n        self.vae_config = vae_config\n        self.latent_patch_size = latent_patch_size\n        self.max_latent_size = max_latent_size\n        self.vit_max_num_patch_per_side = vit_max_num_patch_per_side\n        self.connector_act = connector_act\n        self.interpolate_pos = interpolate_pos\n        self.timestep_shift = timestep_shift\n\n\nclass Bagel(PreTrainedModel):\n    config_class = BagelConfig\n    base_model_prefix = 'bagel'\n\n    def __init__(self, language_model, vit_model, config: BagelConfig):\n        super().__init__(config)    \n        self.language_model = language_model\n        self.hidden_size = config.llm_config.hidden_size\n        self.use_moe = \"Mo\" in config.llm_config.layer_module\n        self.num_heads = config.llm_config.num_attention_heads\n\n        if config.visual_gen:\n            self.latent_patch_size = config.latent_patch_size\n            self.timestep_shift = config.timestep_shift\n            self.latent_downsample = config.vae_config.downsample * config.latent_patch_size\n            self.max_latent_size = config.max_latent_size\n            self.latent_channel = config.vae_config.z_channels\n            self.patch_latent_dim = self.latent_patch_size ** 2 * self.latent_channel\n            self.time_embedder = TimestepEmbedder(self.hidden_size)\n            self.vae2llm = nn.Linear(self.patch_latent_dim, self.hidden_size)\n            self.llm2vae = nn.Linear(self.hidden_size, self.patch_latent_dim)\n            self.latent_pos_embed = PositionEmbedding(self.max_latent_size, self.hidden_size)\n\n        if config.visual_und:\n            self.vit_model = vit_model\n            self.vit_patch_size = config.vit_config.patch_size\n            self.vit_max_num_patch_per_side = config.vit_max_num_patch_per_side\n            self.vit_hidden_size = config.vit_config.hidden_size\n            self.connector = MLPconnector(self.vit_hidden_size, self.hidden_size, config.connector_act)\n            self.vit_pos_embed = PositionEmbedding(self.vit_max_num_patch_per_side, self.hidden_size)\n\n        if config.interpolate_pos:\n            self.get_flattened_position_ids = get_flattened_position_ids_interpolate\n        else:\n            self.get_flattened_position_ids = get_flattened_position_ids_extrapolate\n\n        self.config = config\n        self._init_weights()\n\n    def _init_weights(self):\n        if self.config.visual_gen:\n            nn.init.constant_(self.llm2vae.weight, 0)\n            nn.init.constant_(self.llm2vae.bias, 0)\n\n    def forward(\n        self,\n        sequence_length: int,\n        packed_text_ids: torch.LongTensor,\n        packed_text_indexes: torch.LongTensor,\n        sample_lens: List[int],\n        packed_position_ids: torch.LongTensor,\n        nested_attention_masks: List[torch.Tensor] = None,\n        split_lens: List[int] = None,\n        attn_modes: List[str] = None,\n        # for visual understanding\n        ce_loss_indexes: Optional[torch.BoolTensor] = None,\n        packed_label_ids: Optional[torch.LongTensor] = None,\n        packed_vit_tokens: Optional[torch.Tensor] = None,\n        packed_vit_token_indexes: Optional[torch.LongTensor] = None,\n        packed_vit_position_ids: Optional[torch.LongTensor] = None,\n        vit_token_seqlens: Optional[torch.IntTensor] = None,\n        # for visual generation\n        padded_latent: Optional[torch.Tensor] = None,\n        patchified_vae_latent_shapes: Optional[List[Tuple[int, int]]] = None,\n        packed_latent_position_ids: Optional[torch.LongTensor] = None,\n        packed_vae_token_indexes: Optional[torch.LongTensor] = None,\n        packed_timesteps: Optional[torch.LongTensor] = None,\n        mse_loss_indexes: Optional[torch.BoolTensor] = None,\n    ) -> torch.Tensor:\n        \"\"\"\n        Args:\n            sequence_length: length of sequence.\n            packed_text_ids: 1-D int tensor, packed text token ids.\n            packed_text_indexes: 1-D int tensor, packed text token indexes in sequence.\n            sample_lens: A list of N ints, length of each sample in packed_sequence.\n            nested_attention_masks: A list of N 2-D float tensor,  where 0.0 means attention and \n                -inf means ignore.\n            packed_position_ids: packed 1-D positions, an image has only one global position shared\n                by all latent tokens.\n\n            packed_vit_tokens: packed patchified image tokens for vit model.\n            packed_vit_position_ids: 1-D int tensor, the position of each token for vit model.\n            packed_vit_token_indexes: 1-D int tensor, packed vit token indexes in sequence.\n            vit_token_seqlens: 1-D int tensor, the length of each image tokens for vit model.\n            packed_label_ids: 1-D int tensor, packed label token ids.\n            ce_loss_indexes: 1-D bool tensor, where to compute ce loss.\n\n            padded_latent: padded latent from VAE encoder.\n            patchified_vae_latent_shapes: A list of (h, w) tuples, patchfied latent shapes of each image.\n            packed_latent_position_ids: 1-D int tensor, the position of each token for latent.\n            packed_vae_token_indexes: 1-D int tensor, padded image token indexes in sequence.\n            packed_timesteps: 1-D float tensor, flow timesteps. 0 indicates use clean image.\n            mse_loss_indexes: 1-D bool tensor, where to compute mse loss.\n        \"\"\"\n        packed_text_embedding = self.language_model.model.embed_tokens(packed_text_ids)\n        packed_sequence = packed_text_embedding.new_zeros(size=(sequence_length, self.hidden_size))\n        packed_sequence[packed_text_indexes] = packed_text_embedding\n\n        if nested_attention_masks is None:\n", "metadata": {"task_id": "Bagel/5", "ground_truth": "            sparse_mask = create_sparse_mask(sample_lens, split_lens, attn_modes, packed_text_embedding.device)\n", "fpath_tuple": ["Bagel", "modeling", "bagel", "bagel.py"], "context_start_lineno": 11, "line_no": 154, "col_no": -1, "import_no": [13, 18]}}
{"prompt": "            packed_timesteps = torch.sigmoid(packed_timesteps)\n            packed_timesteps = self.timestep_shift * packed_timesteps / (1 + (self.timestep_shift - 1) * packed_timesteps)\n            packed_latent = (1 - packed_timesteps[:, None]) * packed_latent_clean + packed_timesteps[:, None] * noise\n            packed_timestep_embeds = self.time_embedder(packed_timesteps)\n            latent_token_pos_emb = self.latent_pos_embed(packed_latent_position_ids)\n            packed_latent = self.vae2llm(packed_latent) + packed_timestep_embeds + latent_token_pos_emb\n            packed_sequence[packed_vae_token_indexes] = packed_latent\n\n        extra_inputs = {}\n        if self.use_moe:\n            packed_und_token_indexes = packed_text_indexes\n            if packed_vit_token_indexes is not None:\n                packed_und_token_indexes=torch.cat([packed_text_indexes, packed_vit_token_indexes], dim=0)\n            extra_inputs.update(\n                packed_und_token_indexes=packed_und_token_indexes,\n                packed_gen_token_indexes=packed_vae_token_indexes,\n            )\n\n        last_hidden_state = self.language_model(\n            packed_sequence=packed_sequence,\n            sample_lens=sample_lens,\n            attention_mask=attention_mask,\n            packed_position_ids=packed_position_ids,\n            **extra_inputs,\n        )\n\n        mse = None\n        if self.config.visual_gen:\n            packed_mse_preds = self.llm2vae(last_hidden_state[mse_loss_indexes])\n            target = noise - packed_latent_clean # NOTE: v_t=dx_t/dt=x_1-x_0, pointing from data to noise\n            has_mse = packed_timesteps > 0\n            mse = (packed_mse_preds - target[has_mse]) ** 2\n\n        ce = None\n        if ce_loss_indexes is not None:\n            packed_ce_preds = self.language_model.lm_head(last_hidden_state[ce_loss_indexes])\n            ce = F.cross_entropy(packed_ce_preds, packed_label_ids, reduction=\"none\")\n\n        return dict(mse=mse, ce=ce)\n\n\n    def prepare_prompts(self, curr_kvlens, curr_rope, prompts, tokenizer, new_token_ids):\n        packed_text_ids = list()\n        packed_text_position_ids = list()\n        text_token_lens = list()\n        packed_text_indexes = list()\n        packed_key_value_indexes = list()\n\n        curr = 0\n        newlens, new_rope = list(), list()\n        for prompt, curr_kvlen, curr_position_id in zip(prompts, curr_kvlens, curr_rope):\n            packed_key_value_indexes.extend(range(curr, curr + curr_kvlen))\n            curr += curr_kvlen\n\n            text_ids = tokenizer.encode(prompt)\n            text_ids = [new_token_ids['bos_token_id']] + text_ids + [new_token_ids['eos_token_id']]\n            text_token_lens.append(len(text_ids))\n            packed_text_ids.extend(text_ids)\n            packed_text_position_ids.extend(range(curr_position_id, curr_position_id + len(text_ids)))\n            packed_text_indexes.extend(range(curr, curr + len(text_ids)))\n            newlens.append(curr_kvlen + len(text_ids))\n            new_rope.append(curr_position_id + len(text_ids))\n            curr += len(text_ids)\n\n        generation_input = {\n            \"text_token_lens\": torch.tensor(text_token_lens, dtype=torch.int),\n            \"packed_text_ids\": torch.tensor(packed_text_ids, dtype=torch.long),\n            \"packed_text_position_ids\": torch.tensor(packed_text_position_ids, dtype=torch.long),\n            \"packed_text_indexes\": torch.tensor(packed_text_indexes, dtype=torch.long),\n            \"packed_key_value_indexes\": torch.tensor(packed_key_value_indexes, dtype=torch.long),\n            \"key_values_lens\": torch.tensor(curr_kvlens, dtype=torch.int),\n        }\n\n        return generation_input, newlens, new_rope\n\n    @torch.no_grad\n    def forward_cache_update_text(\n        self,\n        past_key_values: NaiveCache,\n        packed_text_ids: torch.IntTensor,\n        packed_text_position_ids: torch.LongTensor,\n        text_token_lens: torch.LongTensor,\n        packed_text_indexes: torch.LongTensor,\n        packed_key_value_indexes: torch.LongTensor,\n        key_values_lens: torch.IntTensor,\n    ):\n        packed_text_embedding = self.language_model.model.embed_tokens(packed_text_ids)\n\n        extra_inputs = {}\n        if self.use_moe:\n            extra_inputs = {\"mode\": \"und\"}\n\n        output = self.language_model.forward_inference(\n            packed_query_sequence=packed_text_embedding,\n            query_lens=text_token_lens,\n            packed_query_position_ids=packed_text_position_ids,\n            packed_query_indexes=packed_text_indexes,\n            past_key_values=past_key_values,\n            packed_key_value_indexes=packed_key_value_indexes,\n            key_values_lens=key_values_lens,\n            update_past_key_values=True,\n            is_causal=True,\n            **extra_inputs,\n        )\n        past_key_values = output.past_key_values\n\n        return past_key_values\n\n    def prepare_vit_images(self, curr_kvlens, curr_rope, images, transforms, new_token_ids):\n        packed_vit_token_indexes = list()\n        vit_token_seqlens, packed_vit_tokens, packed_vit_position_ids = list(), list(), list()\n        packed_text_ids, packed_text_indexes = list(), list()\n        packed_seqlens, packed_position_ids, packed_indexes = list(), list(), list()\n        packed_key_value_indexes = list()\n\n        _curr = curr = 0\n        newlens, new_rope = list(), list()\n        for image, curr_kvlen, curr_position_id in zip(images, curr_kvlens, curr_rope):\n            packed_key_value_indexes.extend(range(curr, curr + curr_kvlen))\n            curr += curr_kvlen\n\n            packed_text_ids.append(new_token_ids['start_of_image'])\n            packed_text_indexes.append(_curr)\n            packed_indexes.append(curr)\n            curr += 1\n            _curr += 1\n\n            image_tensor = transforms(image)\n            vit_position_ids = self.get_flattened_position_ids(\n                image_tensor.size(1), image_tensor.size(2), \n                self.vit_patch_size, \n                max_num_patches_per_side=self.vit_max_num_patch_per_side\n            )\n            vit_tokens = ", "metadata": {"task_id": "Bagel/6", "ground_truth": "patchify(image_tensor, self.vit_patch_size)\n", "fpath_tuple": ["Bagel", "modeling", "bagel", "bagel.py"], "context_start_lineno": 183, "line_no": 322, "col_no": 25, "import_no": [13, 18]}}
{"prompt": "# Copyright 2025 Bytedance Ltd. and/or its affiliates.\n# SPDX-License-Identifier: Apache-2.0\n\nimport os\nimport json\nimport argparse\nfrom safetensors.torch import load_file\n\nimport torch\nimport torch.distributed as dist\nfrom data.data_utils import add_special_tokens\nfrom modeling.bagel import (\n    BagelConfig, Bagel, Qwen2Config, Qwen2ForCausalLM, SiglipVisionConfig, SiglipVisionModel\n)\nfrom modeling.qwen2 import Qwen2Tokenizer\nfrom modeling.autoencoder import load_ae\n\nfrom PIL import Image\n\n\ndef move_generation_input_to_device(generation_input, device):\n    # Utility to move all tensors in generation_input to device\n    for k, v in generation_input.items():\n        if isinstance(v, torch.Tensor):\n            generation_input[k] = v.to(device)\n    return generation_input\n\n\ndef setup_distributed():\n    dist.init_process_group(backend=\"nccl\")\n    torch.cuda.set_device(int(os.environ[\"LOCAL_RANK\"]))\n\n\ndef generate_image(prompt, num_timesteps=50, cfg_scale=10.0, cfg_interval=[0, 1.0], cfg_renorm_min=0., timestep_shift=1.0, num_images=4, resolution=512, device=None):  # \u6dfb\u52a0device\u53c2\u6570\n", "metadata": {"task_id": "Bagel/7", "ground_truth": "    past_key_values = NaiveCache(gen_model.config.llm_config.num_hidden_layers)\n", "fpath_tuple": ["Bagel", "eval", "gen", "gen_images_mp.py"], "context_start_lineno": 0, "line_no": 35, "col_no": -1, "import_no": [18, 18]}}
{"prompt": "# Copyright 2025 Bytedance Ltd. and/or its affiliates.\n# SPDX-License-Identifier: Apache-2.0\n\nimport io\nimport json\nimport pyarrow.parquet as pq\nimport random\nfrom PIL import Image\n\nfrom .data_utils import pil_img2rgb\nfrom .distributed_iterable_dataset import DistributedIterableDataset\n\nImage.MAX_IMAGE_PIXELS = 20_000_000\n\n\nclass T2IIterableDataset(DistributedIterableDataset):\n    def __init__(\n        self, dataset_name, transform, tokenizer, data_dir_list, num_used_data, \n        local_rank=0, world_size=1, num_workers=8, data_status=None,\n    ):\n        \"\"\"\n        data_dir_list: list of data directories contains parquet files\n        num_used_data: list of number of sampled data paths for each data directory\n        \"\"\"\n        super().__init__(dataset_name, local_rank, world_size, num_workers)\n        self.transform = transform\n        self.tokenizer = tokenizer\n        self.data_status = data_status\n        self.data_paths = self.get_data_paths(data_dir_list, num_used_data)\n        self.set_epoch()\n\n    def get_data_paths(self, data_dir_list, num_used_data):\n        return get_parquet_data_paths(data_dir_list, num_used_data)\n\n    def __iter__(self):\n        data_paths_per_worker, worker_id = self.get_data_paths_per_worker()\n        if self.data_status is not None:\n            parquet_start_id = self.data_status[worker_id][0]\n            row_group_start_id = self.data_status[worker_id][1]\n            row_start_id = self.data_status[worker_id][2] + 1\n        else:\n            parquet_start_id = 0\n            row_group_start_id = 0\n            row_start_id = 0\n        transform_stride = self.transform.stride\n\n        print(\n            f\"rank-{self.local_rank} worker-{worker_id} dataset-{self.dataset_name}: \"\n            f\"resuming data at parquet#{parquet_start_id}, rg#{row_group_start_id}, row#{row_start_id}\"\n        )\n\n        while True:\n            data_paths_per_worker_ = data_paths_per_worker[parquet_start_id:]\n            for parquet_idx, parquet_file_path in enumerate(data_paths_per_worker_, start=parquet_start_id):\n                fs = ", "metadata": {"task_id": "Bagel/8", "ground_truth": "init_arrow_pf_fs(parquet_file_path)\n", "fpath_tuple": ["Bagel", "data", "t2i_dataset.py"], "context_start_lineno": 0, "line_no": 55, "col_no": 21, "import_no": [11, 11]}}
{"prompt": "# Copyright 2025 Bytedance Ltd. and/or its affiliates.\n# SPDX-License-Identifier: Apache-2.0\n\nimport io\nimport json\nimport pyarrow.parquet as pq\nimport random\nfrom PIL import Image\n\nfrom .data_utils import pil_img2rgb\nfrom .distributed_iterable_dataset import DistributedIterableDataset\n\nImage.MAX_IMAGE_PIXELS = 20_000_000\n\n\nclass T2IIterableDataset(DistributedIterableDataset):\n    def __init__(\n        self, dataset_name, transform, tokenizer, data_dir_list, num_used_data, \n        local_rank=0, world_size=1, num_workers=8, data_status=None,\n    ):\n        \"\"\"\n        data_dir_list: list of data directories contains parquet files\n        num_used_data: list of number of sampled data paths for each data directory\n        \"\"\"\n        super().__init__(dataset_name, local_rank, world_size, num_workers)\n        self.transform = transform\n        self.tokenizer = tokenizer\n        self.data_status = data_status\n        self.data_paths = self.get_data_paths(data_dir_list, num_used_data)\n        self.set_epoch()\n\n    def get_data_paths(self, data_dir_list, num_used_data):\n", "metadata": {"task_id": "Bagel/9", "ground_truth": "        return get_parquet_data_paths(data_dir_list, num_used_data)\n", "fpath_tuple": ["Bagel", "data", "t2i_dataset.py"], "context_start_lineno": 0, "line_no": 33, "col_no": -1, "import_no": [11, 11]}}
{"prompt": "# Copyright 2025 Bytedance Ltd. and/or its affiliates.\n# SPDX-License-Identifier: Apache-2.0\n\nimport io\nimport random\nfrom PIL import Image, ImageFile, PngImagePlugin\n\nfrom .interleave_t2i_dataset import InterleavedBaseIterableDataset, ParquetStandardIterableDataset\n\n\nImage.MAX_IMAGE_PIXELS = 200000000\nImageFile.LOAD_TRUNCATED_IMAGES = True\nMaximumDecompressedSize = 1024\nMegaByte = 2 ** 20\nPngImagePlugin.MAX_TEXT_CHUNK = MaximumDecompressedSize * MegaByte\n\n\nclass UnifiedEditIterableDataset(InterleavedBaseIterableDataset, ParquetStandardIterableDataset):\n\n    def parse_row(self, row):\n        image_num = len(row[\"image_list\"])\n        # randomly choose start and end, return [0, 1] when only two images\n        start_idx = random.choice(range(image_num - 1))\n        max_end = min(start_idx + 3, image_num)\n        end_idx = random.choice(range(start_idx + 1, max_end))\n\n        data = self._init_data()\n        data = self._add_image(\n            data, \n            ", "metadata": {"task_id": "Bagel/10", "ground_truth": "pil_img2rgb(Image.open(io.BytesIO(row[\"image_list\"][start_idx]))),\n", "fpath_tuple": ["Bagel", "data", "interleave_datasets", "edit_dataset.py"], "context_start_lineno": 0, "line_no": 30, "col_no": 12, "import_no": [8, 8]}}
{"prompt": "\n                adj_lr_mp = get_lr(global_step, train_cfg.lr_mp, len(train_loader) * train_cfg.epochs // train_cfg.gradient_accumulation_steps)\n                adj_lr_backbones = get_lr(global_step, train_cfg.lr_backbones, len(train_loader) * train_cfg.epochs // train_cfg.gradient_accumulation_steps)\n                optimizer.param_groups[0]['lr'] = adj_lr_mp\n                optimizer.param_groups[1]['lr'] = adj_lr_backbones\n                optimizer.step()\n                optimizer.zero_grad()\n\n            batch_loss = loss.item()\n            if train_cfg.gradient_accumulation_steps > 1:\n                batch_loss = batch_loss * train_cfg.gradient_accumulation_steps\n            total_train_loss += batch_loss\n\n            num_tokens = torch.sum(attention_mask).item() # Sum of attention mask gives number of tokens\n            total_tokens_processed += num_tokens\n\n            batch_end_time = time.time()\n            batch_duration = batch_end_time - batch_start_time\n            tokens_per_second = num_tokens / batch_duration \n\n            # gather loss and t/s from all ranks if DDP\n            batch_loss = mean(dist_gather(batch_loss)) if is_dist() else batch_loss  \n            tokens_per_second = sum(dist_gather(tokens_per_second)) if is_dist() else tokens_per_second  \n\n            if train_cfg.eval_in_epochs and global_step % train_cfg.eval_interval == 0 and is_update_step:\n                model.eval()\n                if device == \"cuda\":\n                    torch.cuda.empty_cache()\n                with torch.no_grad():\n                    total_val_loss = 0\n                    for batch in val_loader:\n                        images = batch[\"image\"].to(device)\n                        input_ids = batch[\"input_ids\"].to(device)\n                        labels = batch[\"labels\"].to(device)\n                        attention_mask = batch[\"attention_mask\"].to(device)\n\n                        with autocast_context:\n                            _, loss = model(input_ids, images, attention_mask=attention_mask, targets=labels)\n\n                        total_val_loss += loss.item()\n                    avg_val_loss = total_val_loss / len(val_loader)\n                    avg_val_loss = mean(dist_gather(avg_val_loss)) if is_dist() else avg_val_loss\n                    if train_cfg.log_wandb and is_master():\n                        run.log({\"val_loss\": avg_val_loss}, step=global_step)\n\n                    if is_master() and global_step % (train_cfg.eval_interval*2) == 0:\n                        eval_model = model.module if is_dist() else model  # unwrap the model for eval if DDP\n                        epoch_accuracy = test_mmstar(eval_model, tokenizer, test_loader, device)\n                        if epoch_accuracy > best_accuracy:\n                            best_accuracy = epoch_accuracy\n                            eval_model.save_pretrained(save_directory=os.path.join(vlm_cfg.vlm_checkpoint_path, run_name))\n                        if train_cfg.log_wandb and is_master():    \n                            run.log({\"accuracy\": epoch_accuracy}, step=global_step)\n                        print(f\"Step: {global_step}, Loss: {batch_loss:.4f}, Tokens/s: {tokens_per_second:.2f}, Accuracy: {epoch_accuracy:.4f}\")\n                    elif is_master() and not global_step % (train_cfg.eval_interval*4) == 0:\n                        print(f\"Step: {global_step}, Loss: {batch_loss:.4f}, Tokens/s: {tokens_per_second:.2f}\")\n\n                model.train()          \n\n            if train_cfg.log_wandb and is_master():\n                run.log({\n                    \"batch_loss\": batch_loss,\n                    \"tokens_per_second\": tokens_per_second,\n                    **({\"grad_norm\": grad_norm} if train_cfg.max_grad_norm is not None and is_update_step else {})\n                }, step=global_step)\n                \n            if is_update_step:\n                global_step += 1\n\n        avg_train_loss = total_train_loss / len(train_loader)\n        # gather average batch loss from all ranks if DDP\n        avg_train_loss = mean(dist_gather(avg_train_loss)) if is_dist() else avg_train_loss  \n\n        epoch_end_time = time.time()\n        epoch_duration = epoch_end_time - epoch_start_time\n        epoch_times.append(epoch_duration)\n\n        # gather and sum total_tokens_processed accross all ranks if DDP\n        total_tokens_processed = sum(dist_gather(total_tokens_processed)) if is_dist() else total_tokens_processed  \n        epoch_tokens_per_second = total_tokens_processed / epoch_duration\n\n        if is_master():\n            if train_cfg.log_wandb:\n                run.log({\"epoch_loss\": avg_train_loss,\n                         \"epoch_duration\": epoch_duration,\n                         \"epoch_tokens_per_second\": epoch_tokens_per_second})\n\n            print(f\"Epoch {epoch+1}/{train_cfg.epochs}, Train Loss: {avg_train_loss:.4f} | Time: {epoch_duration:.2f}s | T/s: {epoch_tokens_per_second:.2f}\")\n\n    # Summary Statistics\n    if is_master():\n        avg_epoch_time = sum(epoch_times) / len(epoch_times)\n        total_training_time = sum(epoch_times)\n        total_samples_processed = len(train_loader.dataset) * train_cfg.epochs\n        avg_time_per_sample = total_training_time / total_samples_processed\n        print(f\"Average time per epoch: {avg_epoch_time:.2f}s\")\n        print(f\"Average time per sample: {avg_time_per_sample:.4f}s\")\n\n        # Push the best model to the hub (Please set your user name in the config!)\n        if vlm_cfg.hf_repo_name is not None:\n            print(\"Training complete. Pushing model to Hugging Face Hub...\")\n            hf_model = VisionLanguageModel.from_pretrained(os.path.join(vlm_cfg.vlm_checkpoint_path, run_name))\n            hf_model.push_to_hub(vlm_cfg.hf_repo_name)\n\n        if train_cfg.log_wandb:\n            run.summary[\"avg_epoch_time\"] = avg_epoch_time\n            run.summary[\"avg_time_per_sample\"] = avg_time_per_sample\n            run.summary[\"mmstar_acc\"] = best_accuracy\n            run.finish()\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--lr_mp', type=float, help='Learning rate for the mapping network')\n    parser.add_argument('--lr_backbones', type=float, help='Learning rate for the backbones')\n    parser.add_argument('--vlm_checkpoint_path', type=str, help='Path to the VLM checkpoint for loading or saving')\n    parser.add_argument('--compile', type=bool, help='Use torch.compile to optimize the model')\n    parser.add_argument('--resume_from_vlm_checkpoint', type=bool, default=False, help='Resume training from VLM checkpoint specified by vlm_checkpoint_path (or default if not provided)')\n\n    args = parser.parse_args()\n\n", "metadata": {"task_id": "nanoVLM/1", "ground_truth": "    vlm_cfg = config.VLMConfig()\n", "fpath_tuple": ["nanoVLM", "train.py"], "context_start_lineno": 307, "line_no": 428, "col_no": -1, "import_no": [24, 24]}}
{"prompt": "import torch\nimport argparse\nimport torch.optim as optim\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader\n\ntorch.manual_seed(0)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(0)\n\nfrom data.collators import VQACollator\nfrom data.processors import get_image_processor, get_tokenizer\nfrom models.vision_language_model import VisionLanguageModel\nimport models.config as config\n\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\ndef measure_vram(args, vlm_cfg, train_cfg_defaults):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    if not torch.cuda.is_available():\n        print(\"CUDA not available. VRAM measurement requires a CUDA-enabled GPU.\")\n        return\n\n    # --- Model Initialization ---\n    torch.cuda.reset_peak_memory_stats(device)\n    print(f\"Using VLMConfig defaults: load_backbone_weights={vlm_cfg.vlm_load_backbone_weights}\")\n    model = VisionLanguageModel(vlm_cfg, load_backbone=vlm_cfg.vlm_load_backbone_weights)\n\n    if args.compile:\n        print(\"Compiling the model with torch.compile...\")\n        model = torch.compile(model)\n        print(\"Model compiled.\")\n    \n    model.to(device)\n\n    # Measure VRAM after model is loaded to device\n    torch.cuda.synchronize() # Ensure all operations are complete\n    initial_vram_allocated_bytes = torch.cuda.memory_allocated(device)\n    initial_vram_allocated_mb = initial_vram_allocated_bytes / (1024 ** 2)\n    print(f\"VRAM allocated after loading model to device: {initial_vram_allocated_mb:.2f} MB\")\n\n    print(f\"Model initialized with {sum(p.numel() for p in model.parameters()):,} parameters\")\n\n    # --- Dataset Preparation ---\n    image_processor = get_image_processor(vlm_cfg.vit_img_size)\n    tokenizer = get_tokenizer(vlm_cfg.lm_tokenizer, vlm_cfg.vlm_extra_tokens)\n\n    dataset_path = train_cfg_defaults.train_dataset_path\n    # train_cfg_defaults.train_dataset_name is a list, use the first if not specified\n    dataset_name = train_cfg_defaults.train_dataset_name[0] if train_cfg_defaults.train_dataset_name else None\n\n    batch_sizes_to_test = [int(bs) for bs in args.batch_sizes.split()]\n    if not batch_sizes_to_test:\n        print(\"Error: No batch sizes provided or parsed correctly.\")\n        return\n    \n    num_iterations_for_vram = args.num_iterations\n    max_bs_to_test = max(batch_sizes_to_test)\n    required_samples_for_base_ds = max_bs_to_test * num_iterations_for_vram\n\n    try:\n        print(f\"Loading dataset: {dataset_path}, name: {dataset_name}\")\n        # Attempt to load only the 'train' split, adjust if dataset has different split names\n        available_splits = load_dataset(dataset_path, dataset_name).keys()\n        split_to_use = 'train' if 'train' in available_splits else list(available_splits)[0]\n        \n        base_ds_full = load_dataset(dataset_path, dataset_name, split=split_to_use)\n        \n        if len(base_ds_full) < required_samples_for_base_ds:\n            print(f\"Warning: Dataset '{dataset_name}' (split: {split_to_use}) has {len(base_ds_full)} samples, \"\n                  f\"but {required_samples_for_base_ds} are recommended for max batch size {max_bs_to_test} \"\n                  f\"and {num_iterations_for_vram} iterations. Using all available samples.\")\n            base_ds_for_vram_test = base_ds_full\n        else:\n            base_ds_for_vram_test = base_ds_full.select(range(required_samples_for_base_ds))\n        print(f\"Using {len(base_ds_for_vram_test)} samples for VRAM testing.\")\n    except Exception as e:\n        print(f\"Error loading dataset: {dataset_path}, name: {dataset_name}. Error: {e}\")\n        print(\"Please ensure the dataset path and name are correct.\")\n        return\n\n    processed_base_dataset = ", "metadata": {"task_id": "nanoVLM/2", "ground_truth": "VQADataset(base_ds_for_vram_test, tokenizer, image_processor)\n", "fpath_tuple": ["nanoVLM", "measure_vram.py"], "context_start_lineno": 0, "line_no": 83, "col_no": 29, "import_no": [11, 11]}}
{"prompt": "import argparse\nimport torch\nfrom PIL import Image\n\ntorch.manual_seed(0)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(0)\n\nfrom data.processors import get_tokenizer, get_image_processor\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description=\"Generate text from an image with nanoVLM\")\n    parser.add_argument(\n        \"--checkpoint\", type=str, default=None,\n        help=\"Path to a local checkpoint (directory or safetensors/pth). If omitted, we pull from HF.\"\n    )\n    parser.add_argument(\n        \"--hf_model\", type=str, default=\"lusxvr/nanoVLM-450M\",\n        help=\"HuggingFace repo ID to download from incase --checkpoint isnt set.\"\n    )\n    parser.add_argument(\"--image\", type=str, default=\"assets/image.png\",\n                        help=\"Path to input image\")\n    parser.add_argument(\"--prompt\", type=str, default=\"What is this?\",\n                        help=\"Text prompt to feed the model\")\n    parser.add_argument(\"--generations\", type=int, default=5,\n                        help=\"Num. of outputs to generate\")\n    parser.add_argument(\"--max_new_tokens\", type=int, default=20,\n                        help=\"Maximum number of tokens per output\")\n    return parser.parse_args()\n\n\ndef main():\n    args = parse_args()\n\n    if torch.cuda.is_available():\n        device = torch.device(\"cuda\")\n    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n        device = torch.device(\"mps\")\n    else:\n        device = torch.device(\"cpu\")\n    print(f\"Using device: {device}\")\n\n    source = args.checkpoint if args.checkpoint else args.hf_model\n    print(f\"Loading weights from: {source}\")\n", "metadata": {"task_id": "nanoVLM/3", "ground_truth": "    model = VisionLanguageModel.from_pretrained(source).to(device)\n", "fpath_tuple": ["nanoVLM", "generate.py"], "context_start_lineno": 0, "line_no": 47, "col_no": -1, "import_no": [8, 8]}}
{"prompt": "import json\nimport os\nimport tempfile\nfrom dataclasses import asdict\nfrom typing import Optional\n\n\nfrom models.vision_transformer import ViT\nfrom models.language_model import LanguageModel\nfrom models.modality_projector import ModalityProjector\nfrom models.config import VLMConfig\n\nfrom data.processors import get_tokenizer\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom safetensors.torch import load_model, save_model\n\nclass VisionLanguageModel(nn.Module):\n    def __init__(self, cfg: VLMConfig, load_backbone=True):\n        super().__init__()\n        self.cfg = cfg\n        if load_backbone:\n            print(\"Loading from backbone weights\")\n            self.vision_encoder = ViT.from_pretrained(cfg)\n            self.decoder = LanguageModel.from_pretrained(cfg)\n        else:\n            self.vision_encoder = ViT(cfg)\n            self.decoder = LanguageModel(cfg)\n        self.MP = ModalityProjector(cfg)\n        self.load_backbone = load_backbone\n        self.tokenizer = get_tokenizer(cfg.lm_tokenizer, cfg.vlm_extra_tokens)\n\n    def _replace_img_tokens_with_embd(self, input_ids, token_embd, image_embd):\n        # Vectorized replacement of image token placeholders.\n        # This assumes that `input_ids` contains `self.cfg.mp_image_token_length` placeholders\n        # for image tokens, and `image_embd` has the corresponding features.\n        updated_token_embd = token_embd.clone()\n\n        # Find the start index of image tokens for each item in the batch.\n        # `torch.argmax` on the boolean mask (cast to int) returns the first index of `True` (value 1).\n        # This relies on image_token_id being present and marking the start of the block.\n        start_indices = torch.argmax((input_ids == self.tokenizer.image_token_id).int(), dim=1) # Shape: [batch_size]\n\n        # Create batch indices for advanced indexing.\n        # This tensor will be like [[0,0,..0], [1,1,..1], ..., [B-1,B-1,..B-1]]\n        # where each inner list has length `image_token_len`.\n        batch_idx_fill = torch.arange(input_ids.size(0), device=input_ids.device).unsqueeze(1).expand(-1, self.cfg.mp_image_token_length) # Shape: [batch_size, image_token_len]\n\n        # Create sequence indices for replacement.\n        # `seq_offsets` will be [0, 1, ..., image_token_len-1]\n        seq_offsets = torch.arange(self.cfg.mp_image_token_length, device=input_ids.device).unsqueeze(0) # Shape: [1, image_token_len]\n\n        # `start_indices.unsqueeze(1)` results in shape [batch_size, 1].\n        # Broadcasting with `seq_offsets` gives, for each batch item `b`:\n        # [start_indices[b], start_indices[b]+1, ..., start_indices[b]+image_token_len-1]\n        sequence_idx_fill = start_indices.unsqueeze(1) + seq_offsets # Shape: [batch_size, image_token_len]\n\n        # Perform the replacement using advanced indexing.\n        # `updated_token_embd[batch_idx_fill, sequence_idx_fill]` selects slices of shape [batch_size, image_token_len, D_lm]\n        # `image_embd` also has shape [batch_size, image_token_len, D_lm] (or [B, mp_image_token_length, D_lm])\n        updated_token_embd[batch_idx_fill, sequence_idx_fill] = image_embd.to(updated_token_embd.dtype)\n        \n        return updated_token_embd\n\n    def forward(self, input_ids, image, attention_mask=None, targets=None):\n        image_embd = self.vision_encoder(image)\n        image_embd = self.MP(image_embd) # [B, mp_image_token_length, D_lm]\n\n        token_embd = self.decoder.token_embedding(input_ids) # [B, T_sequence, D_lm]\n        \n        updated_token_embd = self._replace_img_tokens_with_embd(input_ids, token_embd, image_embd)\n\n        # The updated_token_embd is now the token_embd with image parts replaced.\n        # The attention_mask comes from the collator and should already cover the full sequence.\n        logits, _ = self.decoder(updated_token_embd, attention_mask=attention_mask)\n\n        loss = None\n        if targets is not None:\n            logits = self.decoder.head(logits) # Apply LM head\n            # Loss is calculated over all tokens, but `targets` (labels) will have -100 for non-answer tokens.\n            # No need to slice logits based on image embedding size here, as the target mask handles it.\n            loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), targets.reshape(-1), ignore_index=-100)\n\n        return logits, loss\n\n    @torch.inference_mode()\n    def generate(self, input_ids, image, attention_mask=None, max_new_tokens=5, top_k=50, top_p=0.9, temperature=0.5, greedy=False):\n\n        # 1. Process image\n        image_embd = self.vision_encoder(image) # [B, T_img_feat, D_model]\n        image_embd = self.MP(image_embd)      # [B, mp_image_token_length, D_lm]\n\n        # 2. Embed initial text prompt tokens\n        prompt_token_embeds = self.decoder.token_embedding(input_ids) # [B, T_prompt_text, D_lm]\n\n        # 3. Combine image and text embeddings\n        initial_combined_embeds = self._replace_img_tokens_with_embd(input_ids, prompt_token_embeds, image_embd)\n\n        current_total_seq_len = initial_combined_embeds.size(1)\n        batch_size = input_ids.size(0) # Or initial_combined_embeds.size(0)\n        \n        # --- Multimodal Prefill Phase ---\n        prefill_output, kv_cache_list = self.decoder(\n            initial_combined_embeds,\n            attention_mask=attention_mask, # Use the provided attention mask\n            kv_cache=None,\n            start_pos=0\n        )\n        \n        last_token_output_from_prefill = prefill_output[:, -1, :] \n        \n        if not self.decoder.lm_use_tokens:\n            current_logits = self.decoder.head(last_token_output_from_prefill) \n        else:\n            current_logits = last_token_output_from_prefill \n\n        # Store newly generated token IDs\n        newly_generated_ids_list = []\n\n        # --- Decode Phase by sampling tokens autoregressively using the kv-cache ---\n        for _ in range(max_new_tokens):\n            if greedy:\n                next_token_id = torch.argmax(current_logits, dim=-1, keepdim=True)\n            else:\n                filtered_logits = ", "metadata": {"task_id": "nanoVLM/4", "ground_truth": "top_k_top_p_filtering(current_logits, top_k=top_k, top_p=top_p)\n", "fpath_tuple": ["nanoVLM", "models", "vision_language_model.py"], "context_start_lineno": 0, "line_no": 127, "col_no": 34, "import_no": [7, 7]}}
{"prompt": "import json\nimport os\nimport tempfile\nfrom dataclasses import asdict\nfrom typing import Optional\n\n\nfrom models.utils import top_k_top_p_filtering\nfrom models.language_model import LanguageModel\nfrom models.modality_projector import ModalityProjector\nfrom models.config import VLMConfig\n\nfrom data.processors import get_tokenizer\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom safetensors.torch import load_model, save_model\n\nclass VisionLanguageModel(nn.Module):\n    def __init__(self, cfg: VLMConfig, load_backbone=True):\n        super().__init__()\n        self.cfg = cfg\n        if load_backbone:\n            print(\"Loading from backbone weights\")\n", "metadata": {"task_id": "nanoVLM/5", "ground_truth": "            self.vision_encoder = ViT.from_pretrained(cfg)\n", "fpath_tuple": ["nanoVLM", "models", "vision_language_model.py"], "context_start_lineno": 0, "line_no": 26, "col_no": -1, "import_no": [8, 8]}}
{"prompt": "import math\nimport time\nimport torch\nimport wandb\nimport numpy\nimport random\nimport argparse\nimport contextlib\nimport torch.optim as optim\nfrom statistics import mean\nfrom dataclasses import asdict\nfrom datasets import load_dataset, concatenate_datasets\nfrom torch.utils.data import DataLoader, RandomSampler, DistributedSampler\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel\n\ntorch.manual_seed(0)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(0)\n\nfrom data.datasets import MMStarDataset, VQADataset\nfrom data.processors import get_image_processor, get_tokenizer\nfrom models.vision_language_model import VisionLanguageModel\nimport models.config as config\nimport models.utils as utils\n\n#Otherwise, the tokenizer will through a warning\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\ndef seed_worker(worker_id):\n    worker_seed = torch.initial_seed() % 2**32\n    numpy.random.seed(worker_seed)\n    random.seed(worker_seed)\n\ndef init_dist():\n    dist.init_process_group(backend='nccl')\n    torch.cuda.set_device(dist.get_rank())\n\ndef destroy_dist():\n    dist.destroy_process_group()\n\ndef is_dist():\n    return dist.is_available() and dist.is_initialized()\n\ndef is_master():\n    return dist.get_rank() == 0 if is_dist() else True\n\ndef get_world_size():\n    return dist.get_world_size() if is_dist() else 1\n\ndef get_rank():\n    return dist.get_rank() if is_dist() else 0\n\ndef dist_gather(o):\n    o_all = [None for _ in range(dist.get_world_size())]\n    dist.all_gather_object(o_all, o)\n    return o_all\n\ndef wrap_model(model):\n    return DistributedDataParallel(model, device_ids=[dist.get_rank()])\n\ndef get_run_name(train_cfg, vlm_cfg):\n    dataset_size = \"full_ds\" if train_cfg.data_cutoff_idx is None else f\"{train_cfg.data_cutoff_idx}samples\"\n    batch_size = f\"bs{int(train_cfg.batch_size*get_world_size()*train_cfg.gradient_accumulation_steps)}\"\n    epochs = f\"ep{train_cfg.epochs}\"\n    learning_rate = f\"lr{train_cfg.lr_backbones}-{train_cfg.lr_mp}\"\n    num_gpus = f\"{get_world_size()}xGPU\"\n    date = time.strftime(\"%m%d-%H%M%S\")\n    vit = f\"{vlm_cfg.vit_model_type.split('/')[-1]}\"\n    mp = f\"mp{vlm_cfg.mp_pixel_shuffle_factor}\"\n    llm = f\"{vlm_cfg.lm_model_type.split('/')[-1]}\"\n\n    return f\"nanoVLM_{vit}_{mp}_{llm}_{num_gpus}_{dataset_size}_{batch_size}_{epochs}_{learning_rate}_{date}\"\n\ndef get_dataloaders(train_cfg, vlm_cfg):\n    # Create datasets\n    image_processor = get_image_processor(vlm_cfg.vit_img_size)\n    tokenizer = get_tokenizer(vlm_cfg.lm_tokenizer, vlm_cfg.vlm_extra_tokens)\n\n    # Load and combine all training datasets\n    combined_train_data = []\n    for dataset_name in train_cfg.train_dataset_name:\n        train_ds = load_dataset(train_cfg.train_dataset_path, dataset_name)\n        combined_train_data.append(train_ds['train'])\n    train_ds = concatenate_datasets(combined_train_data)\n    \n    test_ds = load_dataset(train_cfg.test_dataset_path)\n    train_ds = train_ds.shuffle(seed=0) # Shuffle the training dataset, so train and val get equal contributions from all concatinated datasets\n\n    # Apply cutoff if specified\n    if train_cfg.data_cutoff_idx is None:\n        total_samples = len(train_ds)  # Use the entire dataset\n    else:\n        total_samples = min(len(train_ds), train_cfg.data_cutoff_idx)\n\n    val_size = int(total_samples * train_cfg.val_ratio)\n    train_size = total_samples - val_size\n\n    train_dataset = VQADataset(train_ds.select(range(train_size)), tokenizer, image_processor)\n    val_dataset = VQADataset(train_ds.select(range(train_size, total_samples)), tokenizer, image_processor)\n    test_dataset = MMStarDataset(test_ds['val'], tokenizer, image_processor)\n\n    # Create collators\n    vqa_collator = VQACollator(tokenizer, vlm_cfg.lm_max_length, vlm_cfg.mp_image_token_length)\n    mmstar_collator = ", "metadata": {"task_id": "nanoVLM/6", "ground_truth": "MMStarCollator(tokenizer, vlm_cfg.mp_image_token_length)\n", "fpath_tuple": ["nanoVLM", "train.py"], "context_start_lineno": 0, "line_no": 106, "col_no": 22, "import_no": [20, 20]}}
{"prompt": "import math\nimport time\nimport torch\nimport wandb\nimport numpy\nimport random\nimport argparse\nimport contextlib\nimport torch.optim as optim\nfrom statistics import mean\nfrom dataclasses import asdict\nfrom datasets import load_dataset, concatenate_datasets\nfrom torch.utils.data import DataLoader, RandomSampler, DistributedSampler\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel\n\ntorch.manual_seed(0)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(0)\n\nfrom data.collators import VQACollator, MMStarCollator\nfrom data.processors import get_image_processor, get_tokenizer\nfrom models.vision_language_model import VisionLanguageModel\nimport models.config as config\nimport models.utils as utils\n\n#Otherwise, the tokenizer will through a warning\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\ndef seed_worker(worker_id):\n    worker_seed = torch.initial_seed() % 2**32\n    numpy.random.seed(worker_seed)\n    random.seed(worker_seed)\n\ndef init_dist():\n    dist.init_process_group(backend='nccl')\n    torch.cuda.set_device(dist.get_rank())\n\ndef destroy_dist():\n    dist.destroy_process_group()\n\ndef is_dist():\n    return dist.is_available() and dist.is_initialized()\n\ndef is_master():\n    return dist.get_rank() == 0 if is_dist() else True\n\ndef get_world_size():\n    return dist.get_world_size() if is_dist() else 1\n\ndef get_rank():\n    return dist.get_rank() if is_dist() else 0\n\ndef dist_gather(o):\n    o_all = [None for _ in range(dist.get_world_size())]\n    dist.all_gather_object(o_all, o)\n    return o_all\n\ndef wrap_model(model):\n    return DistributedDataParallel(model, device_ids=[dist.get_rank()])\n\ndef get_run_name(train_cfg, vlm_cfg):\n    dataset_size = \"full_ds\" if train_cfg.data_cutoff_idx is None else f\"{train_cfg.data_cutoff_idx}samples\"\n    batch_size = f\"bs{int(train_cfg.batch_size*get_world_size()*train_cfg.gradient_accumulation_steps)}\"\n    epochs = f\"ep{train_cfg.epochs}\"\n    learning_rate = f\"lr{train_cfg.lr_backbones}-{train_cfg.lr_mp}\"\n    num_gpus = f\"{get_world_size()}xGPU\"\n    date = time.strftime(\"%m%d-%H%M%S\")\n    vit = f\"{vlm_cfg.vit_model_type.split('/')[-1]}\"\n    mp = f\"mp{vlm_cfg.mp_pixel_shuffle_factor}\"\n    llm = f\"{vlm_cfg.lm_model_type.split('/')[-1]}\"\n\n    return f\"nanoVLM_{vit}_{mp}_{llm}_{num_gpus}_{dataset_size}_{batch_size}_{epochs}_{learning_rate}_{date}\"\n\ndef get_dataloaders(train_cfg, vlm_cfg):\n    # Create datasets\n    image_processor = get_image_processor(vlm_cfg.vit_img_size)\n    tokenizer = get_tokenizer(vlm_cfg.lm_tokenizer, vlm_cfg.vlm_extra_tokens)\n\n    # Load and combine all training datasets\n    combined_train_data = []\n    for dataset_name in train_cfg.train_dataset_name:\n        train_ds = load_dataset(train_cfg.train_dataset_path, dataset_name)\n        combined_train_data.append(train_ds['train'])\n    train_ds = concatenate_datasets(combined_train_data)\n    \n    test_ds = load_dataset(train_cfg.test_dataset_path)\n    train_ds = train_ds.shuffle(seed=0) # Shuffle the training dataset, so train and val get equal contributions from all concatinated datasets\n\n    # Apply cutoff if specified\n    if train_cfg.data_cutoff_idx is None:\n        total_samples = len(train_ds)  # Use the entire dataset\n    else:\n        total_samples = min(len(train_ds), train_cfg.data_cutoff_idx)\n\n    val_size = int(total_samples * train_cfg.val_ratio)\n    train_size = total_samples - val_size\n\n    train_dataset = VQADataset(train_ds.select(range(train_size)), tokenizer, image_processor)\n    val_dataset = VQADataset(train_ds.select(range(train_size, total_samples)), tokenizer, image_processor)\n", "metadata": {"task_id": "nanoVLM/7", "ground_truth": "    test_dataset = MMStarDataset(test_ds['val'], tokenizer, image_processor)\n", "fpath_tuple": ["nanoVLM", "train.py"], "context_start_lineno": 0, "line_no": 102, "col_no": -1, "import_no": [21, 21]}}
{"prompt": "import math\nimport time\nimport torch\nimport wandb\nimport numpy\nimport random\nimport argparse\nimport contextlib\nimport torch.optim as optim\nfrom statistics import mean\nfrom dataclasses import asdict\nfrom datasets import load_dataset, concatenate_datasets\nfrom torch.utils.data import DataLoader, RandomSampler, DistributedSampler\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel\n\ntorch.manual_seed(0)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(0)\n\nfrom data.collators import VQACollator, MMStarCollator\nfrom data.processors import get_image_processor, get_tokenizer\nfrom models.vision_language_model import VisionLanguageModel\nimport models.config as config\nimport models.utils as utils\n\n#Otherwise, the tokenizer will through a warning\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\ndef seed_worker(worker_id):\n    worker_seed = torch.initial_seed() % 2**32\n    numpy.random.seed(worker_seed)\n    random.seed(worker_seed)\n\ndef init_dist():\n    dist.init_process_group(backend='nccl')\n    torch.cuda.set_device(dist.get_rank())\n\ndef destroy_dist():\n    dist.destroy_process_group()\n\ndef is_dist():\n    return dist.is_available() and dist.is_initialized()\n\ndef is_master():\n    return dist.get_rank() == 0 if is_dist() else True\n\ndef get_world_size():\n    return dist.get_world_size() if is_dist() else 1\n\ndef get_rank():\n    return dist.get_rank() if is_dist() else 0\n\ndef dist_gather(o):\n    o_all = [None for _ in range(dist.get_world_size())]\n    dist.all_gather_object(o_all, o)\n    return o_all\n\ndef wrap_model(model):\n    return DistributedDataParallel(model, device_ids=[dist.get_rank()])\n\ndef get_run_name(train_cfg, vlm_cfg):\n    dataset_size = \"full_ds\" if train_cfg.data_cutoff_idx is None else f\"{train_cfg.data_cutoff_idx}samples\"\n    batch_size = f\"bs{int(train_cfg.batch_size*get_world_size()*train_cfg.gradient_accumulation_steps)}\"\n    epochs = f\"ep{train_cfg.epochs}\"\n    learning_rate = f\"lr{train_cfg.lr_backbones}-{train_cfg.lr_mp}\"\n    num_gpus = f\"{get_world_size()}xGPU\"\n    date = time.strftime(\"%m%d-%H%M%S\")\n    vit = f\"{vlm_cfg.vit_model_type.split('/')[-1]}\"\n    mp = f\"mp{vlm_cfg.mp_pixel_shuffle_factor}\"\n    llm = f\"{vlm_cfg.lm_model_type.split('/')[-1]}\"\n\n    return f\"nanoVLM_{vit}_{mp}_{llm}_{num_gpus}_{dataset_size}_{batch_size}_{epochs}_{learning_rate}_{date}\"\n\ndef get_dataloaders(train_cfg, vlm_cfg):\n    # Create datasets\n    image_processor = get_image_processor(vlm_cfg.vit_img_size)\n    tokenizer = get_tokenizer(vlm_cfg.lm_tokenizer, vlm_cfg.vlm_extra_tokens)\n\n    # Load and combine all training datasets\n    combined_train_data = []\n    for dataset_name in train_cfg.train_dataset_name:\n        train_ds = load_dataset(train_cfg.train_dataset_path, dataset_name)\n        combined_train_data.append(train_ds['train'])\n    train_ds = concatenate_datasets(combined_train_data)\n    \n    test_ds = load_dataset(train_cfg.test_dataset_path)\n    train_ds = train_ds.shuffle(seed=0) # Shuffle the training dataset, so train and val get equal contributions from all concatinated datasets\n\n    # Apply cutoff if specified\n    if train_cfg.data_cutoff_idx is None:\n        total_samples = len(train_ds)  # Use the entire dataset\n    else:\n        total_samples = min(len(train_ds), train_cfg.data_cutoff_idx)\n\n    val_size = int(total_samples * train_cfg.val_ratio)\n    train_size = total_samples - val_size\n\n", "metadata": {"task_id": "nanoVLM/8", "ground_truth": "    train_dataset = VQADataset(train_ds.select(range(train_size)), tokenizer, image_processor)\n", "fpath_tuple": ["nanoVLM", "train.py"], "context_start_lineno": 0, "line_no": 100, "col_no": -1, "import_no": [21, 21]}}
{"prompt": "import numpy\nimport random\nimport argparse\nimport contextlib\nimport torch.optim as optim\nfrom statistics import mean\nfrom dataclasses import asdict\nfrom datasets import load_dataset, concatenate_datasets\nfrom torch.utils.data import DataLoader, RandomSampler, DistributedSampler\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel\n\ntorch.manual_seed(0)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(0)\n\nfrom data.collators import VQACollator, MMStarCollator\nfrom data.datasets import MMStarDataset, VQADataset\nfrom data.processors import get_image_processor, get_tokenizer\nfrom models.vision_language_model import VisionLanguageModel\nimport models.config as config\n\n#Otherwise, the tokenizer will through a warning\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\ndef seed_worker(worker_id):\n    worker_seed = torch.initial_seed() % 2**32\n    numpy.random.seed(worker_seed)\n    random.seed(worker_seed)\n\ndef init_dist():\n    dist.init_process_group(backend='nccl')\n    torch.cuda.set_device(dist.get_rank())\n\ndef destroy_dist():\n    dist.destroy_process_group()\n\ndef is_dist():\n    return dist.is_available() and dist.is_initialized()\n\ndef is_master():\n    return dist.get_rank() == 0 if is_dist() else True\n\ndef get_world_size():\n    return dist.get_world_size() if is_dist() else 1\n\ndef get_rank():\n    return dist.get_rank() if is_dist() else 0\n\ndef dist_gather(o):\n    o_all = [None for _ in range(dist.get_world_size())]\n    dist.all_gather_object(o_all, o)\n    return o_all\n\ndef wrap_model(model):\n    return DistributedDataParallel(model, device_ids=[dist.get_rank()])\n\ndef get_run_name(train_cfg, vlm_cfg):\n    dataset_size = \"full_ds\" if train_cfg.data_cutoff_idx is None else f\"{train_cfg.data_cutoff_idx}samples\"\n    batch_size = f\"bs{int(train_cfg.batch_size*get_world_size()*train_cfg.gradient_accumulation_steps)}\"\n    epochs = f\"ep{train_cfg.epochs}\"\n    learning_rate = f\"lr{train_cfg.lr_backbones}-{train_cfg.lr_mp}\"\n    num_gpus = f\"{get_world_size()}xGPU\"\n    date = time.strftime(\"%m%d-%H%M%S\")\n    vit = f\"{vlm_cfg.vit_model_type.split('/')[-1]}\"\n    mp = f\"mp{vlm_cfg.mp_pixel_shuffle_factor}\"\n    llm = f\"{vlm_cfg.lm_model_type.split('/')[-1]}\"\n\n    return f\"nanoVLM_{vit}_{mp}_{llm}_{num_gpus}_{dataset_size}_{batch_size}_{epochs}_{learning_rate}_{date}\"\n\ndef get_dataloaders(train_cfg, vlm_cfg):\n    # Create datasets\n    image_processor = get_image_processor(vlm_cfg.vit_img_size)\n    tokenizer = get_tokenizer(vlm_cfg.lm_tokenizer, vlm_cfg.vlm_extra_tokens)\n\n    # Load and combine all training datasets\n    combined_train_data = []\n    for dataset_name in train_cfg.train_dataset_name:\n        train_ds = load_dataset(train_cfg.train_dataset_path, dataset_name)\n        combined_train_data.append(train_ds['train'])\n    train_ds = concatenate_datasets(combined_train_data)\n    \n    test_ds = load_dataset(train_cfg.test_dataset_path)\n    train_ds = train_ds.shuffle(seed=0) # Shuffle the training dataset, so train and val get equal contributions from all concatinated datasets\n\n    # Apply cutoff if specified\n    if train_cfg.data_cutoff_idx is None:\n        total_samples = len(train_ds)  # Use the entire dataset\n    else:\n        total_samples = min(len(train_ds), train_cfg.data_cutoff_idx)\n\n    val_size = int(total_samples * train_cfg.val_ratio)\n    train_size = total_samples - val_size\n\n    train_dataset = VQADataset(train_ds.select(range(train_size)), tokenizer, image_processor)\n    val_dataset = VQADataset(train_ds.select(range(train_size, total_samples)), tokenizer, image_processor)\n    test_dataset = MMStarDataset(test_ds['val'], tokenizer, image_processor)\n\n    # Create collators\n    vqa_collator = VQACollator(tokenizer, vlm_cfg.lm_max_length, vlm_cfg.mp_image_token_length)\n    mmstar_collator = MMStarCollator(tokenizer, vlm_cfg.mp_image_token_length)\n\n    g = torch.Generator()\n    g.manual_seed(0)\n\n    # Create dataloaders\n    train_sampler = DistributedSampler(\n        train_dataset, \n        rank=get_rank(),\n        num_replicas=get_world_size(),\n    )\n\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=train_cfg.batch_size,    # =per device BS in DDP\n        sampler=train_sampler,\n        collate_fn=vqa_collator,\n        num_workers=8,\n        pin_memory=True,\n        drop_last=True,\n        worker_init_fn=seed_worker,\n        generator=g,\n    )\n\n    val_sampler = DistributedSampler(\n        val_dataset,\n        rank=get_rank(),\n        num_replicas=get_world_size(),\n        shuffle=False  # Usually False for validation\n    )\n\n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=train_cfg.batch_size,\n        sampler=val_sampler,\n        collate_fn=vqa_collator,\n        num_workers=8,\n        pin_memory=True,\n        drop_last=True,\n        worker_init_fn=seed_worker,\n        generator=g,\n    )\n\n    test_loader = DataLoader(\n        test_dataset, \n        batch_size=train_cfg.mmstar_batch_size, \n        shuffle=False, \n        collate_fn=mmstar_collator,\n        pin_memory=True,\n        worker_init_fn=seed_worker,\n        generator=g,\n        )\n\n    return train_loader, val_loader, test_loader\n\ndef test_mmstar(model, tokenizer, test_loader, device):\n    total_examples = 0\n    correct_predictions = 0\n    with torch.no_grad():\n        for batch in test_loader:\n            image = batch['images'].to(device)\n            input_ids = batch['input_ids'].to(device)\n            labels = batch['labels'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n\n            correct_answer = tokenizer.batch_decode(labels, skip_special_tokens=True)\n            \n            gen = model.generate(input_ids, image, attention_mask, greedy=True)\n            model_output = tokenizer.batch_decode(gen, skip_special_tokens=True)\n            \n            is_correct = ", "metadata": {"task_id": "nanoVLM/9", "ground_truth": "utils.check_multiple_choice_with_regex(model_output, correct_answer)\n", "fpath_tuple": ["nanoVLM", "train.py"], "context_start_lineno": 4, "line_no": 176, "col_no": 25, "import_no": [25, 25]}}
{"prompt": "    learning_rate = f\"lr{train_cfg.lr_backbones}-{train_cfg.lr_mp}\"\n    num_gpus = f\"{get_world_size()}xGPU\"\n    date = time.strftime(\"%m%d-%H%M%S\")\n    vit = f\"{vlm_cfg.vit_model_type.split('/')[-1]}\"\n    mp = f\"mp{vlm_cfg.mp_pixel_shuffle_factor}\"\n    llm = f\"{vlm_cfg.lm_model_type.split('/')[-1]}\"\n\n    return f\"nanoVLM_{vit}_{mp}_{llm}_{num_gpus}_{dataset_size}_{batch_size}_{epochs}_{learning_rate}_{date}\"\n\ndef get_dataloaders(train_cfg, vlm_cfg):\n    # Create datasets\n    image_processor = get_image_processor(vlm_cfg.vit_img_size)\n    tokenizer = get_tokenizer(vlm_cfg.lm_tokenizer, vlm_cfg.vlm_extra_tokens)\n\n    # Load and combine all training datasets\n    combined_train_data = []\n    for dataset_name in train_cfg.train_dataset_name:\n        train_ds = load_dataset(train_cfg.train_dataset_path, dataset_name)\n        combined_train_data.append(train_ds['train'])\n    train_ds = concatenate_datasets(combined_train_data)\n    \n    test_ds = load_dataset(train_cfg.test_dataset_path)\n    train_ds = train_ds.shuffle(seed=0) # Shuffle the training dataset, so train and val get equal contributions from all concatinated datasets\n\n    # Apply cutoff if specified\n    if train_cfg.data_cutoff_idx is None:\n        total_samples = len(train_ds)  # Use the entire dataset\n    else:\n        total_samples = min(len(train_ds), train_cfg.data_cutoff_idx)\n\n    val_size = int(total_samples * train_cfg.val_ratio)\n    train_size = total_samples - val_size\n\n    train_dataset = VQADataset(train_ds.select(range(train_size)), tokenizer, image_processor)\n    val_dataset = VQADataset(train_ds.select(range(train_size, total_samples)), tokenizer, image_processor)\n    test_dataset = MMStarDataset(test_ds['val'], tokenizer, image_processor)\n\n    # Create collators\n    vqa_collator = VQACollator(tokenizer, vlm_cfg.lm_max_length, vlm_cfg.mp_image_token_length)\n    mmstar_collator = MMStarCollator(tokenizer, vlm_cfg.mp_image_token_length)\n\n    g = torch.Generator()\n    g.manual_seed(0)\n\n    # Create dataloaders\n    train_sampler = DistributedSampler(\n        train_dataset, \n        rank=get_rank(),\n        num_replicas=get_world_size(),\n    )\n\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=train_cfg.batch_size,    # =per device BS in DDP\n        sampler=train_sampler,\n        collate_fn=vqa_collator,\n        num_workers=8,\n        pin_memory=True,\n        drop_last=True,\n        worker_init_fn=seed_worker,\n        generator=g,\n    )\n\n    val_sampler = DistributedSampler(\n        val_dataset,\n        rank=get_rank(),\n        num_replicas=get_world_size(),\n        shuffle=False  # Usually False for validation\n    )\n\n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=train_cfg.batch_size,\n        sampler=val_sampler,\n        collate_fn=vqa_collator,\n        num_workers=8,\n        pin_memory=True,\n        drop_last=True,\n        worker_init_fn=seed_worker,\n        generator=g,\n    )\n\n    test_loader = DataLoader(\n        test_dataset, \n        batch_size=train_cfg.mmstar_batch_size, \n        shuffle=False, \n        collate_fn=mmstar_collator,\n        pin_memory=True,\n        worker_init_fn=seed_worker,\n        generator=g,\n        )\n\n    return train_loader, val_loader, test_loader\n\ndef test_mmstar(model, tokenizer, test_loader, device):\n    total_examples = 0\n    correct_predictions = 0\n    with torch.no_grad():\n        for batch in test_loader:\n            image = batch['images'].to(device)\n            input_ids = batch['input_ids'].to(device)\n            labels = batch['labels'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n\n            correct_answer = tokenizer.batch_decode(labels, skip_special_tokens=True)\n            \n            gen = model.generate(input_ids, image, attention_mask, greedy=True)\n            model_output = tokenizer.batch_decode(gen, skip_special_tokens=True)\n            \n            is_correct = utils.check_multiple_choice_with_regex(model_output, correct_answer)\n            \n            total_examples += len(is_correct)\n            if is_correct:\n                correct_predictions += sum(is_correct)\n    accuracy = correct_predictions / total_examples if total_examples > 0 else 0\n    return accuracy\n\n# Cosine learning rate schedule with warmup (from Karpathy)\n# https://github.com/karpathy/build-nanogpt/blob/master/train_gpt2.py#L353\ndef get_lr(it, max_lr, max_steps):\n    min_lr = max_lr * 0.1\n    warmup_steps = max_steps * 0.03\n    # 1) linear warmup for warmup_iters steps\n    if it < warmup_steps:\n        return max_lr * (it+1) / warmup_steps\n    # 2) if it > lr_decay_iters, return min learning rate\n    if it > max_steps:\n        return min_lr\n    # 3) in between, use cosine decay down to min learning rate\n    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n    assert 0 <= decay_ratio <= 1\n    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n    return min_lr + coeff * (max_lr - min_lr)\n\ndef train(train_cfg, vlm_cfg):\n    train_loader, val_loader, test_loader = get_dataloaders(train_cfg, vlm_cfg)\n    tokenizer = get_tokenizer(vlm_cfg.lm_tokenizer, vlm_cfg.vlm_extra_tokens)\n\n    total_dataset_size = len(train_loader.dataset)\n    if train_cfg.log_wandb and is_master():\n        run_name = get_run_name(train_cfg, vlm_cfg)\n        if train_cfg.data_cutoff_idx is None:\n            run_name = run_name.replace(\"full_ds\", f\"{total_dataset_size}samples\")\n        run = wandb.init(\n            entity=train_cfg.wandb_entity,\n            project=\"nanoVLM\",\n            config={\n                \"VLMConfig\": asdict(vlm_cfg),\n                \"TrainConfig\": asdict(train_cfg)\n            },\n            name=run_name,\n        )\n\n    # Initialize model\n    if train_cfg.resume_from_vlm_checkpoint:\n", "metadata": {"task_id": "nanoVLM/10", "ground_truth": "        model = VisionLanguageModel.from_pretrained(vlm_cfg.vlm_checkpoint_path)\n", "fpath_tuple": ["nanoVLM", "train.py"], "context_start_lineno": 66, "line_no": 222, "col_no": -1, "import_no": [23, 23]}}
{"prompt": "        tensors = {}\n        non_tensors = {}\n\n        for key, val in data.items():\n            if isinstance(val, torch.Tensor):\n                tensors[key] = val\n            elif isinstance(val, np.ndarray):\n                non_tensors[key] = val\n            else:\n                raise ValueError(f'Unsupported type in data {type(val)}')\n\n        return DataProto.from_dict(tensors=tensors, non_tensors=non_tensors, meta_info=meta_info)\n\n    @classmethod\n    def from_dict(cls, tensors: Dict[str, torch.Tensor], non_tensors=None, meta_info=None, num_batch_dims=1):\n        \"\"\"Create a DataProto from a dict of tensors. This assumes that\n        1. All the tensor in tensors have the same dim0\n        2. Only dim0 is the batch dim\n        \"\"\"\n        assert len(tensors) > 0, 'tensors must not be empty'\n        assert num_batch_dims > 0, 'num_batch_dims must be greater than zero'\n        if non_tensors is not None:\n            assert num_batch_dims == 1, 'only support num_batch_dims=1 when non_tensors is not None.'\n\n        if meta_info is None:\n            meta_info = {}\n        if non_tensors is None:\n            non_tensors = {}\n\n        assert isinstance(non_tensors, dict)\n\n        # get and check batch size\n        batch_size = None\n        pivot_key = None\n        for key, tensor in tensors.items():\n            if batch_size is None:\n                batch_size = tensor.shape[:num_batch_dims]\n                pivot_key = key\n            else:\n                current_batch = tensor.shape[:num_batch_dims]\n                assert batch_size == current_batch, \\\n                    f'Not all the tensor in tensors have the same batch size with batch_dims={num_batch_dims}. Got {pivot_key} has {batch_size}, {key} has {current_batch}'\n\n        for key, val in non_tensors.items():\n            non_tensors[key] = np.array(val, dtype=object)\n\n        tensor_dict = TensorDict(source=tensors, batch_size=batch_size)\n        return cls(batch=tensor_dict, non_tensor_batch=non_tensors, meta_info=meta_info)\n\n    def to(self, device) -> 'DataProto':\n        \"\"\"move the batch to device\n\n        Args:\n            device (torch.device, str): torch device\n\n        Returns:\n            DataProto: the current DataProto\n\n        \"\"\"\n        if self.batch is not None:\n            self.batch = self.batch.to(device)\n        return self\n\n    def select(self, batch_keys=None, non_tensor_batch_keys=None, meta_info_keys=None, deepcopy=False) -> 'DataProto':\n        \"\"\"Select a subset of the DataProto via batch_keys and meta_info_keys\n\n        Args:\n            batch_keys (list, optional): a list of strings indicating the keys in batch to select\n            meta_info_keys (list, optional): a list of keys indicating the meta info to select\n\n        Returns:\n            DataProto: the DataProto with the selected batch_keys and meta_info_keys\n        \"\"\"\n        # TODO (zhangchi.usc1992) whether to copy\n        if batch_keys is not None:\n            batch_keys = tuple(batch_keys)\n            sub_batch = self.batch.select(*batch_keys)\n        else:\n            sub_batch = self.batch\n\n        if non_tensor_batch_keys is not None:\n            non_tensor_batch = {key: val for key, val in self.non_tensor_batch.items() if key in non_tensor_batch_keys}\n        else:\n            non_tensor_batch = self.non_tensor_batch\n\n        if deepcopy:\n            non_tensor_batch = copy.deepcopy(non_tensor_batch)\n\n        if meta_info_keys is not None:\n            sub_meta_info = {key: val for key, val in self.meta_info.items() if key in meta_info_keys}\n        else:\n            sub_meta_info = self.meta_info\n\n        if deepcopy:\n            sub_meta_info = copy.deepcopy(sub_meta_info)\n\n        return DataProto(batch=sub_batch, non_tensor_batch=non_tensor_batch, meta_info=sub_meta_info)\n\n    def pop(self, batch_keys=None, non_tensor_batch_keys=None, meta_info_keys=None) -> 'DataProto':\n        \"\"\"Pop a subset of the DataProto via `batch_keys` and `meta_info_keys`\n\n        Args:\n            batch_keys (list, optional): a list of strings indicating the keys in batch to pop\n            meta_info_keys (list, optional): a list of keys indicating the meta info to pop\n\n        Returns:\n            DataProto: the DataProto with the poped batch_keys and meta_info_keys\n        \"\"\"\n        assert batch_keys is not None\n        if meta_info_keys is None:\n            meta_info_keys = []\n        if non_tensor_batch_keys is None:\n            non_tensor_batch_keys = []\n\n        tensors = {}\n        # tensor batch\n        for key in batch_keys:\n            assert key in self.batch.keys()\n            tensors[key] = self.batch.pop(key)\n        non_tensors = {}\n        # non tensor batch\n        for key in non_tensor_batch_keys:\n            assert key in self.non_tensor_batch.keys()\n            non_tensors[key] = self.non_tensor_batch.pop(key)\n        meta_info = {}\n        for key in meta_info_keys:\n            assert key in self.meta_info.keys()\n            meta_info[key] = self.meta_info.pop(key)\n        return DataProto.from_dict(tensors=tensors, non_tensors=non_tensors, meta_info=meta_info)\n\n    def rename(self, old_keys=None, new_keys=None) -> 'DataProto':\n        \"\"\"\n        Note that this function only rename the key in the batch\n        \"\"\"\n\n        def validate_input(keys):\n            if keys is not None:\n                if isinstance(keys, str):\n                    keys = [keys]\n                elif isinstance(keys, list):\n                    pass\n                else:\n                    raise TypeError(f'keys must be a list or a string, but got {type(keys)}')\n            return keys\n\n        old_keys = validate_input(old_keys)\n        new_keys = validate_input(new_keys)\n\n        if len(new_keys) != len(old_keys):\n            raise ValueError(\n                f'new_keys and old_keys must have the same length, but got {len(new_keys)} and {len(old_keys)}')\n\n        self.batch.rename_key_(tuple(old_keys), tuple(new_keys))\n\n        return self\n\n    def union(self, other: 'DataProto') -> 'DataProto':\n        \"\"\"Union with another DataProto. Union batch and meta_info separately.\n        Throw an error if\n        - there are conflict keys in batch and they are not equal\n        - the batch size of two data batch is not the same\n        - there are conflict keys in meta_info and they are not the same.\n\n        Args:\n            other (DataProto): another DataProto to union\n\n        Returns:\n            DataProto: the DataProto after union\n        \"\"\"\n        self.batch = union_tensor_dict(self.batch, other.batch)\n        self.non_tensor_batch = union_numpy_dict(self.non_tensor_batch, other.non_tensor_batch)\n        self.meta_info = ", "metadata": {"task_id": "ZeroSearch/1", "ground_truth": "union_two_dict(self.meta_info, other.meta_info)\n", "fpath_tuple": ["ZeroSearch", "verl", "protocol.py"], "context_start_lineno": 265, "line_no": 437, "col_no": 25, "import_no": [29, 29]}}
{"prompt": "            'eos_token_id': self.tokenizer.eos_token_id,\n            'pad_token_id': self.tokenizer.pad_token_id,\n        }\n        override_config_kwargs.update(override_model_config)\n        update_model_config(actor_model_config, override_config_kwargs=override_config_kwargs)\n\n        if self.rank == 0:\n            print(f'Model config after override: {actor_model_config}')\n\n        def megatron_actor_model_provider(pre_process, post_process):\n            from verl.utils.model import get_parallel_model_from_config\n            # vpp is not supported yet because it will hang for some reason. Need debugging\n            vpp_rank = mpu.get_virtual_pipeline_model_parallel_rank()  # this will be set inside get_model\n            # this_megatron_config = copy.deepcopy(megatron_config)\n            # this_megatron_config.virtual_pipeline_model_parallel_rank = vpp_rank\n            parallel_model = get_parallel_model_from_config(config=actor_model_config,\n                                                            megatron_config=megatron_config,\n                                                            pre_process=pre_process,\n                                                            post_process=post_process,\n                                                            value=False)\n            parallel_model.cuda()\n            return parallel_model\n\n        # Step 3: initialize the megatron model\n        if self._is_actor and self._is_rollout:\n            # Initialize the 3D HybridEngine\n            hybrid_engine = AllGatherPPModel(model_provider=megatron_actor_model_provider)\n            # Fetch the model at current rank\n            actor_module = hybrid_engine.this_rank_models\n            if isinstance(actor_module, nn.ModuleList):\n                actor_module = [actor_module[0]]\n            if self.config.actor.load_weight:\n                load_megatron_model_weights(self.config,\n                                            actor_model_config,\n                                            actor_module,\n                                            params_dtype=megatron_config.params_dtype,\n                                            is_value_model=False)\n\n            if self.rank == 0:\n                print_model_size(actor_module[0])\n            log_gpu_memory_usage('After AllGatherPPModel init', logger=logger)\n        elif self._is_ref:\n            print(f'self.config.ref.load_weight: {self.config.ref.load_weight}')\n            ref_module = get_model(model_provider_func=megatron_actor_model_provider,\n                                   model_type=ModelType.encoder_or_decoder,\n                                   wrap_with_ddp=False)\n            # ref_module = nn.ModuleList(ref_module)\n\n            if self.config.ref.load_weight:  # should align with the actor:\n                assert self.config.actor.load_weight == self.config.ref.load_weight\n                print(f'load ref weight start')\n                load_megatron_model_weights(self.config,\n                                            actor_model_config,\n                                            ref_module,\n                                            params_dtype=megatron_config.params_dtype,\n                                            is_value_model=False)\n            log_gpu_memory_usage('After ref module init', logger=logger)\n            return ref_module, actor_model_config\n\n        # TODO: add more optimizer args into config\n        if self._is_actor:\n            optim_config = init_megatron_optim_config(optim_config)\n            actor_optimizer = get_megatron_optimizer(model=actor_module, config=optim_config)\n        else:\n            optim_config = None\n            actor_optimizer = None\n\n        log_gpu_memory_usage('After actor optimizer init', logger=logger)\n\n        return actor_module, hybrid_engine, actor_optimizer, actor_model_config, optim_config\n\n    def _build_rollout(self):\n        if self.config.rollout.name == 'vllm':\n            from verl.workers.rollout.vllm_rollout import vLLMRollout\n            from verl.workers.sharding_manager import MegatronVLLMShardingManager\n            from verl.utils.model import normalize_pp_vpp_params\n\n            # NOTE(sgm): If the QKV and gate_up projection layer are concate together in actor,\n            # we will reorganize their weight format when resharding from actor to rollout.\n            layer_name_mapping = {\n                \"qkv_layer_name\":\n                    self.config.rollout.layer_name_map.get(\"qkv_layer_name\", \"qkv\"),\n                \"gate_proj_layer_name\":\n                    self.config.rollout.layer_name_map.get(\"gate_proj_layer_name\", \"linear_fc1.weight\"),\n            }\n\n            # reshard the weight partition from actor to rollout to initialize the rollout class\n            # create a new cuda space for parameters not in this pp rank\n            self.hybrid_engine.load_params_to_cuda()\n            # broadcast the parameters from pp rank to other ranks\n            self.hybrid_engine.allgather_params()\n            # obtain name to parameters in pp/vpp\n            params = self.hybrid_engine.get_all_params()\n            # update the param name for the\n            params = normalize_pp_vpp_params(params=params,\n                                             num_hidden_layers=self.actor_model_config.num_hidden_layers,\n                                             layer_name='layers')\n            rollout = vLLMRollout(actor_module=params,\n                                  config=self.config.rollout,\n                                  tokenizer=self.tokenizer,\n                                  model_hf_config=self.actor_model_config,\n                                  train_tp=mpu.get_tensor_model_parallel_world_size())\n            log_gpu_memory_usage('After building vllm rollout', logger=logger)\n\n            # perform weight resharding between actor and rollout\n            sharding_manager = MegatronVLLMShardingManager(module=self.hybrid_engine,\n                                                           inference_engine=rollout.inference_engine,\n                                                           model_config=self.actor_model_config,\n                                                           layer_name_mapping=layer_name_mapping)\n            log_gpu_memory_usage('After building sharding manager', logger=logger)\n        else:\n            NotImplementedError('Only vllmRollout is supported with Megatron now')\n\n        return rollout, sharding_manager\n\n    @register(dispatch_mode=Dispatch.ONE_TO_ALL)\n    def init_model(self):\n        if self.config.model.get('external_lib', None) is not None:\n            # This is used to import external_lib into the huggingface systems\n            import importlib\n            importlib.import_module(self.config.model.external_lib)\n\n        from omegaconf import OmegaConf\n        from verl.utils.torch_dtypes import PrecisionType\n        override_model_config = OmegaConf.to_container(self.config.model.get('override_config', OmegaConf.create()))\n        torch_dtype = torch.bfloat16\n\n        megatron_config = OmegaConf.create({\n            'sequence_parallel': self.config.actor.megatron.get('sequence_parallel', True),\n            'param_dtype': PrecisionType.to_str(torch_dtype),\n            'tensor_model_parallel_size': mpu.get_tensor_model_parallel_world_size(),\n            'pipeline_model_parallel_rank': mpu.get_pipeline_model_parallel_rank(),\n            'pipeline_model_parallel_size': mpu.get_pipeline_model_parallel_world_size(),\n            'virtual_pipeline_model_parallel_rank': mpu.get_virtual_pipeline_model_parallel_rank(),\n            'virtual_pipeline_model_parallel_size': mpu.get_virtual_pipeline_model_parallel_world_size()\n        })\n\n", "metadata": {"task_id": "ZeroSearch/2", "ground_truth": "        megatron_config = init_model_parallel_config(megatron_config)\n", "fpath_tuple": ["ZeroSearch", "verl", "workers", "megatron_workers.py"], "context_start_lineno": 143, "line_no": 281, "col_no": -1, "import_no": [35, 35]}}
{"prompt": "                                                           inference_engine=rollout.inference_engine,\n                                                           model_config=self.actor_model_config,\n                                                           layer_name_mapping=layer_name_mapping)\n            log_gpu_memory_usage('After building sharding manager', logger=logger)\n        else:\n            NotImplementedError('Only vllmRollout is supported with Megatron now')\n\n        return rollout, sharding_manager\n\n    @register(dispatch_mode=Dispatch.ONE_TO_ALL)\n    def init_model(self):\n        if self.config.model.get('external_lib', None) is not None:\n            # This is used to import external_lib into the huggingface systems\n            import importlib\n            importlib.import_module(self.config.model.external_lib)\n\n        from omegaconf import OmegaConf\n        from verl.utils.torch_dtypes import PrecisionType\n        override_model_config = OmegaConf.to_container(self.config.model.get('override_config', OmegaConf.create()))\n        torch_dtype = torch.bfloat16\n\n        megatron_config = OmegaConf.create({\n            'sequence_parallel': self.config.actor.megatron.get('sequence_parallel', True),\n            'param_dtype': PrecisionType.to_str(torch_dtype),\n            'tensor_model_parallel_size': mpu.get_tensor_model_parallel_world_size(),\n            'pipeline_model_parallel_rank': mpu.get_pipeline_model_parallel_rank(),\n            'pipeline_model_parallel_size': mpu.get_pipeline_model_parallel_world_size(),\n            'virtual_pipeline_model_parallel_rank': mpu.get_virtual_pipeline_model_parallel_rank(),\n            'virtual_pipeline_model_parallel_size': mpu.get_virtual_pipeline_model_parallel_world_size()\n        })\n\n        megatron_config = init_model_parallel_config(megatron_config)\n\n        if self._is_actor or self._is_rollout:\n            # we need the model for actor and rollout\n            if self._is_actor:\n                optim_config = self.config.actor.optim\n            else:\n                optim_config = None\n            self.actor_module, self.hybrid_engine, self.actor_optimizer, \\\n            self.actor_model_config, self.actor_optim_config = self._build_model_optimizer(\n                model_path=self.config.model.path,\n                megatron_config=megatron_config,\n                optim_config=optim_config,\n                override_model_config=override_model_config,\n            )\n\n        if self._is_actor:\n            self.actor = MegatronPPOActor(config=self.config.actor,\n                                          model_config=self.actor_model_config,\n                                          megatron_config=megatron_config,\n                                          actor_module=self.actor_module,\n                                          actor_optimizer=self.actor_optimizer,\n                                          actor_optimizer_config=self.actor_optim_config)\n\n        if self._is_rollout:\n            self.rollout, self.sharding_manager = self._build_rollout()\n\n        if self._is_ref:\n            self.ref_module, self.ref_model_config = self._build_model_optimizer(\n                model_path=self.config.model.path,\n                megatron_config=megatron_config,\n                optim_config=None,\n                override_model_config=override_model_config,\n            )\n            self.ref_policy = MegatronPPOActor(config=self.config.ref,\n                                               model_config=self.ref_model_config,\n                                               megatron_config=megatron_config,\n                                               actor_module=self.ref_module,\n                                               actor_optimizer=None,\n                                               actor_optimizer_config=None)\n\n        torch.cuda.empty_cache()\n\n    @register(dispatch_mode=Dispatch.MEGATRON_COMPUTE_PROTO)\n    def update_actor(self, data: DataProto):\n        assert self._is_actor\n\n        data.batch = data.batch.cuda()\n\n        log_gpu_memory_usage('Before update policy', logger=logger)\n\n        dataloader = self.actor.make_minibatch_iterator(data=data)\n        metrics = self.actor.update_policy(dataloader=dataloader)\n\n        log_gpu_memory_usage('After update policy', logger=logger)\n\n        # TODO: here, we should return all metrics\n        output = DataProto(meta_info={'metrics': metrics})\n        output = output.to('cpu')\n        torch.cuda.empty_cache()\n        return output\n    \n    # @register(dispatch_mode=Dispatch.MEGATRON_PP_AS_DP_PROTO)\n    # def compute_log_prob(self, data: DataProto) -> DataProto:\n    #     assert self._is_rollout\n    #     output = self.actor.compute_log_prob(data=data)\n    #     output = DataProto.from_dict(tensors={'old_log_probs': output})\n    #     torch.cuda.empty_cache()\n    #     return output\n\n    @register(dispatch_mode=Dispatch.MEGATRON_PP_AS_DP_PROTO)\n    def generate_sequences(self, prompts: DataProto):\n        assert self._is_rollout\n\n        prompts.batch = prompts.batch.cuda()\n        meta_info = {'eos_token_id': self.tokenizer.eos_token_id, 'pad_token_id': self.tokenizer.pad_token_id}\n        prompts.meta_info.update(meta_info)\n        with self.sharding_manager:\n            log_gpu_memory_usage('After entering sharding manager', logger=logger)\n\n            prompts = self.sharding_manager.preprocess_data(prompts)\n            output = self.rollout.generate_sequences(prompts=prompts)\n\n            log_gpu_memory_usage('After rollout generation', logger=logger)\n\n            output = self.sharding_manager.postprocess_data(output)\n\n        validate = prompts.meta_info.get('validate', False)\n        if self._is_actor and not validate:\n            # we should always recompute old_log_probs when it is HybridEngine\n            output.meta_info['micro_batch_size'] = self.config.rollout.log_prob_micro_batch_size\n            output.meta_info['temperature'] = self.config.rollout.temperature\n            old_log_probs = self.actor.compute_log_prob(data=output)\n            output.batch['old_log_probs'] = old_log_probs\n\n        output = output.to('cpu')\n        # clear kv cache\n        torch.cuda.empty_cache()\n        log_gpu_memory_usage('After recompute log prob', logger=logger)\n        return output\n\n    @register(dispatch_mode=Dispatch.MEGATRON_COMPUTE_PROTO)\n    def compute_ref_log_prob(self, data: DataProto):\n        data = data.to('cuda')\n\n        assert self._is_ref\n        if self._is_offload_param:\n            load_megatron_param_and_grad(self.ref_module, torch.cuda.current_device(), self._is_offload_grad)\n\n        micro_batch_size = self.config.rollout.log_prob_micro_batch_size\n        data.meta_info['micro_batch_size'] = micro_batch_size\n        data.meta_info['temperature'] = self.config.rollout.temperature\n        output = self.ref_policy.compute_log_prob(data=data)\n        output = DataProto.from_dict(tensors={'ref_log_prob': output})\n        output = output.to('cpu')\n        if self._is_offload_param:\n", "metadata": {"task_id": "ZeroSearch/3", "ground_truth": "            offload_megatron_param_and_grad(self.ref_module, self._is_offload_grad)\n", "fpath_tuple": ["ZeroSearch", "verl", "workers", "megatron_workers.py"], "context_start_lineno": 249, "line_no": 397, "col_no": -1, "import_no": [36, 36]}}
{"prompt": "\n        reward_module = FSDP(\n            reward_module,\n            param_init_fn=init_fn,\n            use_orig_params=False,\n            auto_wrap_policy=auto_wrap_policy,\n            device_id=torch.cuda.current_device(),\n            sharding_strategy=ShardingStrategy.FULL_SHARD,  # zero3\n            sync_module_states=True,\n            cpu_offload=CPUOffload(offload_params=self.config.model.fsdp_config.param_offload),\n            forward_prefetch=False)\n\n        return reward_module\n\n    @register(dispatch_mode=Dispatch.ONE_TO_ALL)\n    def init_model(self):\n        # This is used to import external_lib into the huggingface systems\n        import_external_libs(self.config.model.get('external_lib', None))\n        self.reward_module = self._build_model(config=self.config)\n        torch.cuda.empty_cache()\n\n    def _forward_micro_batch(self, micro_batch):\n        from flash_attn.bert_padding import pad_input, unpad_input, index_first_axis, rearrange\n        from verl.utils.ulysses import ulysses_pad_and_slice_inputs, gather_outpus_and_unpad\n\n        with torch.no_grad(), torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n            input_ids = micro_batch['input_ids']\n            batch_size, seqlen = input_ids.shape\n            attention_mask = micro_batch['attention_mask']\n            position_ids = micro_batch['position_ids']\n\n            if self.use_remove_padding:\n                input_ids_rmpad, indices, *_ = unpad_input(input_ids.unsqueeze(-1),\n                                                           attention_mask)  # input_ids_rmpad (total_nnz, ...)\n                input_ids_rmpad = input_ids_rmpad.transpose(0, 1)  # (1, total_nnz)\n\n                # unpad the position_ids to align the rotary\n                position_ids_rmpad = index_first_axis(rearrange(position_ids.unsqueeze(-1), \"b s ... -> (b s) ...\"),\n                                                      indices).transpose(0, 1)\n\n                # pad and slice the inputs if sp > 1\n                if self.ulysses_sequence_parallel_size > 1:\n                    input_ids_rmpad, position_ids_rmpad, pad_size = ulysses_pad_and_slice_inputs(input_ids_rmpad, \\\n                                                                                                position_ids_rmpad, \\\n                                                                                                sp_size=self.ulysses_sequence_parallel_size)\n\n                # only pass input_ids and position_ids to enable flash_attn_varlen\n                output = self.reward_module(input_ids=input_ids_rmpad,\n                                            attention_mask=None,\n                                            position_ids=position_ids_rmpad,\n                                            use_cache=False)  # prevent model thinks we are generating\n                reward_rmpad = output.logits\n                reward_rmpad = reward_rmpad.squeeze(0)  # (total_nnz)\n\n                # gather output if sp > 1\n                if self.ulysses_sequence_parallel_size > 1:\n                    reward_rmpad = gather_outpus_and_unpad(reward_rmpad,\n                                                           gather_dim=0,\n                                                           unpad_dim=0,\n                                                           padding_size=pad_size)\n\n                # pad it back\n                rm_score = pad_input(reward_rmpad, indices=indices, batch=batch_size, seqlen=seqlen).squeeze(-1)\n            else:\n                output = self.reward_module(input_ids=input_ids,\n                                            attention_mask=attention_mask,\n                                            position_ids=position_ids)\n                rm_score = output.logits  # (batch_size, seq_len, 1)\n                rm_score = rm_score.squeeze(-1)\n\n            # extract the result of the last valid token\n            eos_mask_idx = torch.argmax(position_ids * attention_mask, dim=-1)  # (bsz,)\n            rm_score = rm_score[torch.arange(batch_size), eos_mask_idx]\n            return rm_score\n\n    def _expand_to_token_level(self, data: DataProto, scores: torch.Tensor):\n        batch_size = data.batch.batch_size[0]\n        # expand as token_level_reward\n        attention_mask = data.batch['attention_mask']\n        position_ids = data.batch['position_ids']\n        response_length = data.batch['responses'].shape[-1]\n        eos_mask_idx = torch.argmax(position_ids * attention_mask, dim=-1)  # (bsz,)\n        token_level_scores = torch.zeros_like(attention_mask, dtype=scores.dtype)  # (bsz, seqlen)\n        token_level_scores[torch.arange(batch_size), eos_mask_idx] = scores\n\n        # select the response part\n        token_level_scores = token_level_scores[:, -response_length:]\n\n        return token_level_scores\n\n    def _switch_chat_template(self, data: DataProto):\n        src_max_length = data.batch['attention_mask'].shape[-1]\n\n        src_tokenizer = self.input_tokenizer\n        target_tokenizer = self.tokenizer\n\n        rm_input_ids = []\n        rm_attention_mask = []\n\n        for i in range(data.batch.batch_size[0]):\n            # extract raw prompt\n            chat: list = data.non_tensor_batch['raw_prompt'][i].tolist()\n\n            # extract response\n            response_ids = data.batch['responses'][i]\n            response_length = response_ids.shape[-1]\n            valid_response_length = data.batch['attention_mask'][i][-response_length:].sum()\n            valid_response_ids = response_ids[:valid_response_length]\n\n            # decode\n            response = src_tokenizer.decode(valid_response_ids)\n            # remove bos and eos\n            response = response.replace(src_tokenizer.eos_token, '')\n\n            chat.append({'role': 'assistant', 'content': response})\n\n            prompt_with_chat_template = target_tokenizer.apply_chat_template(chat,\n                                                                             add_generation_prompt=False,\n                                                                             tokenize=False)\n            if self.rank == 0 and i == 0:\n                # for debugging purpose\n                print(f'Switch template. chat: {prompt_with_chat_template}')\n\n            # the maximum length is actually determined by the reward model itself\n            max_length = self.config.get('max_length', src_max_length)\n            if max_length is None:\n                max_length = src_max_length\n            input_ids, attention_mask = verl_F.tokenize_and_postprocess_data(\n                prompt=prompt_with_chat_template,\n                tokenizer=target_tokenizer,\n                max_length=max_length,\n                pad_token_id=target_tokenizer.pad_token_id,\n                left_pad=False,  # right padding\n                truncation=self.config.get('truncation', 'right'))  # truncate from the right\n\n            rm_input_ids.append(input_ids)\n            rm_attention_mask.append(attention_mask)\n\n        rm_input_ids = torch.cat(rm_input_ids, dim=0)\n        rm_attention_mask = torch.cat(rm_attention_mask, dim=0)\n\n        rm_position_ids = ", "metadata": {"task_id": "ZeroSearch/4", "ground_truth": "compute_position_id_with_mask(rm_attention_mask)\n", "fpath_tuple": ["ZeroSearch", "verl", "workers", "fsdp_workers.py"], "context_start_lineno": 865, "line_no": 1007, "col_no": 26, "import_no": [36, 36]}}
{"prompt": "        update_model_config(actor_model_config, override_config_kwargs=override_config_kwargs)\n        if self.rank == 0:\n            print(f'Model config after override: {actor_model_config}')\n\n        # NOTE(fix me): tie_word_embedding causes meta_tensor init to hang\n        init_context = get_init_weight_context_manager(use_meta_tensor=not actor_model_config.tie_word_embeddings)\n\n        with init_context(), warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            actor_module = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=local_path,\n                                                                torch_dtype=torch_dtype,\n                                                                config=actor_model_config,\n                                                                attn_implementation='flash_attention_2',\n                                                                trust_remote_code=trust_remote_code)\n            # some parameters may not in torch_dtype. TODO(zhangchi.usc1992) remove this after we switch to fsdp2\n            actor_module.to(torch_dtype)\n\n            if enable_gradient_checkpointing:\n                actor_module.gradient_checkpointing_enable(gradient_checkpointing_kwargs={'use_reentrant': False})\n        torch.distributed.barrier()\n\n        if self.rank == 0:\n            print_model_size(actor_module)\n\n        log_gpu_memory_usage('After init from HF AutoModel', logger=logger)\n\n        # We wrap FSDP for rollout as well\n        mixed_precision_config = fsdp_config.get('mixed_precision', None)\n        if mixed_precision_config is not None:\n            param_dtype = PrecisionType.to_dtype(mixed_precision_config.get('param_dtype', 'bf16'))\n            reduce_dtype = PrecisionType.to_dtype(mixed_precision_config.get('reduce_dtype', 'fp32'))\n            buffer_dtype = PrecisionType.to_dtype(mixed_precision_config.get('buffer_dtype', 'fp32'))\n        else:\n            param_dtype = torch.bfloat16\n            reduce_dtype = torch.float32\n            buffer_dtype = torch.float32\n\n        mixed_precision = MixedPrecision(param_dtype=param_dtype, reduce_dtype=reduce_dtype, buffer_dtype=buffer_dtype)\n\n        if self._is_ref:\n            mixed_precision = None\n\n        auto_wrap_policy = get_fsdp_wrap_policy(module=actor_module, config=fsdp_config.get('wrap_policy', None))\n\n        if self._is_rollout and self.config.rollout.name == 'hf':\n            # TODO(zhangchi.usc1992, shengguangming) fix me. Current, auto_wrap_policy causes HFRollout to hang in Gemma\n            auto_wrap_policy = None\n\n        print(f'wrap_policy: {auto_wrap_policy}')\n\n        # TODO(sgm): support hybrid\n        if auto_wrap_policy is None:\n            sharding_strategy = ShardingStrategy.SHARD_GRAD_OP\n        else:\n            sharding_strategy = ShardingStrategy.FULL_SHARD\n\n        # TODO: add transformer policy\n        actor_module_fsdp = FSDP(\n            actor_module,\n            param_init_fn=init_fn,\n            use_orig_params=False,\n            auto_wrap_policy=auto_wrap_policy,\n            device_id=torch.cuda.current_device(),\n            sharding_strategy=sharding_strategy,  # zero3\n            mixed_precision=mixed_precision,\n            sync_module_states=True,\n            device_mesh=self.device_mesh,\n            forward_prefetch=False)\n\n        log_gpu_memory_usage('After Actor FSDP init', logger=logger)\n\n        # TODO: add more optimizer args into config\n        if self._is_actor:\n            from verl.utils.torch_functional import get_constant_schedule_with_warmup\n            actor_optimizer = optim.AdamW(actor_module_fsdp.parameters(),\n                                          lr=optim_config.lr,\n                                          betas=optim_config.get('betas', (0.9, 0.999)),\n                                          weight_decay=optim_config.get('weight_decay', 1e-2))\n\n            total_steps = optim_config.get('total_training_steps', 0)\n            num_warmup_steps_ratio = optim_config.get('lr_warmup_steps_ratio', 0.)\n            num_warmup_steps = int(num_warmup_steps_ratio * total_steps)\n\n            print(f'Total steps: {total_steps}, num_warmup_steps: {num_warmup_steps}')\n\n            actor_lr_scheduler = get_constant_schedule_with_warmup(optimizer=actor_optimizer,\n                                                                   num_warmup_steps=num_warmup_steps)\n        else:\n            actor_optimizer = None\n            actor_lr_scheduler = None\n\n        log_gpu_memory_usage('After actor optimizer init', logger=logger)\n\n        return actor_module_fsdp, actor_optimizer, actor_lr_scheduler, actor_model_config\n\n    def _build_rollout(self):\n        from torch.distributed.device_mesh import init_device_mesh\n        # TODO(sgm): support FSDP hybrid shard for larger model\n        infer_tp = self.config.rollout.tensor_model_parallel_size\n        dp = self.world_size // infer_tp\n        assert self.world_size % infer_tp == 0, f'rollout world_size: {self.world_size} is not divisible by infer_tp: {infer_tp}'\n        rollout_device_mesh = init_device_mesh('cuda', mesh_shape=(dp, infer_tp), mesh_dim_names=['dp', 'infer_tp'])\n\n        if self.config.rollout.name == 'hf':\n            from verl.workers.rollout import HFRollout\n            from verl.workers.sharding_manager import BaseShardingManager\n            rollout = HFRollout(module=self.actor_module_fsdp, config=self.config.rollout)\n            rollout_sharding_manager = BaseShardingManager()\n            # TODO: a sharding manager that do nothing?\n        elif self.config.rollout.name == 'vllm':\n            from verl.workers.rollout.vllm_rollout import vLLMRollout\n            from verl.workers.sharding_manager import FSDPVLLMShardingManager\n            log_gpu_memory_usage('Before building vllm rollout', logger=None)\n            rollout = vLLMRollout(actor_module=self.actor_module_fsdp,\n                                  config=self.config.rollout,\n                                  tokenizer=self.tokenizer,\n                                  model_hf_config=self.actor_model_config)\n            log_gpu_memory_usage('After building vllm rollout', logger=None)\n            if torch.distributed.get_world_size() == 1:\n                self.config.rollout.load_format = 'dummy_hf'\n            rollout_sharding_manager = FSDPVLLMShardingManager(module=self.actor_module_fsdp,\n                                                               inference_engine=rollout.inference_engine,\n                                                               model_config=self.actor_model_config,\n                                                               full_params='hf' in self.config.rollout.load_format,\n                                                               device_mesh=rollout_device_mesh)\n            log_gpu_memory_usage('After building sharding manager', logger=None)\n\n        return rollout, rollout_sharding_manager\n\n    @register(dispatch_mode=Dispatch.ONE_TO_ALL)\n    def init_model(self):\n        from verl.workers.actor import DataParallelPPOActor\n        # This is used to import external_lib into the huggingface systems\n", "metadata": {"task_id": "ZeroSearch/5", "ground_truth": "        import_external_libs(self.config.model.get('external_lib', None))\n", "fpath_tuple": ["ZeroSearch", "verl", "workers", "fsdp_workers.py"], "context_start_lineno": 153, "line_no": 287, "col_no": -1, "import_no": [35, 35]}}
{"prompt": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nIn single GPU rollout, the sequences are generated directly by sampling from the model.\nThe output will contain\n1. output_ids\n2. attention_masks (left padding)\n3. eos_masks\n4. log_probs\n\"\"\"\nfrom typing import Iterable, Union\n\nimport torch\nimport torch.nn.functional as F\nfrom tensordict import TensorDict\nfrom torch import nn\n\nfrom verl import DataProto\nfrom ..base import BaseRollout\n\n__all__ = ['NativeRollout']\n\n\nclass NaiveRollout(BaseRollout):\n\n    def __init__(self, module: nn.Module, config):\n        \"\"\"A naive rollout. It requires the module to be compatible with huggingface APIs. That is:\n        The module should define __call__ to receive input_ids, attention_mask and position_ids.\n        It outputs a structure that contains logits field.\n\n        Args:\n            module: module here follows huggingface APIs\n            config: DictConfig\n        \"\"\"\n        super().__init__()\n        self.config = config\n        self.module = module\n\n    @torch.no_grad()\n    def generate_sequences(self, prompts: DataProto) -> DataProto:\n        \"\"\"Generate sequences\"\"\"\n        idx = prompts.batch['input_ids']  # (bs, prompt_length)\n        attention_mask = prompts.batch['attention_mask']  # left-padded attention_mask\n        position_ids = prompts.batch['position_ids']\n\n        # used to construct attention_mask\n        eos_token_id = prompts.meta_info['eos_token_id']\n\n        batch_size = idx.size(0)\n        prompt_length = idx.size(1)\n\n        self.module.eval()\n\n        prev_attention_mask = torch.ones(size=(batch_size, 1), dtype=attention_mask.dtype, device=attention_mask.device)\n\n        logits_lst = []\n        for _ in range(self.config.response_length):\n            # if the sequence context is growing too long we must crop it at block_size\n            # idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n            idx_cond = idx\n            # forward the model to get the logits for the index in the sequence\n            # we use huggingface APIs here\n            output = self.module(input_ids=idx_cond, attention_mask=attention_mask, position_ids=position_ids)\n            logits = output.logits\n            # pluck the logits at the final step and scale by desired temperature\n            logits = logits[:, -1, :] / self.config.temperature  # (bs, vocab_size)\n            # optionally crop the logits to only the top k options\n            if self.config.top_k is not None:\n                v, _ = torch.topk(logits, min(self.config.top_k, logits.size(-1)))\n                logits[logits < v[:, [-1]]] = -float('Inf')\n            # apply softmax to convert logits to (normalized) probabilities\n            probs = F.softmax(logits, dim=-1)\n            # sample from the distribution\n            if self.config.do_sample:\n                idx_next = torch.multinomial(probs, num_samples=1)\n            else:\n                idx_next = torch.argmax(probs, dim=-1, keepdim=True)\n\n            attention_mask = torch.cat((attention_mask, prev_attention_mask), dim=-1)\n\n            prev_attention_mask = torch.logical_and(idx_next != eos_token_id, prev_attention_mask.bool())\n            prev_attention_mask.to(attention_mask.dtype)\n\n            position_ids = torch.cat((position_ids, position_ids[:, -1:] + 1), dim=-1)\n\n            # append sampled index to the running sequence and continue\n            idx = torch.cat((idx, idx_next), dim=1)\n            logits_lst.append(logits)\n\n        logits = torch.stack(logits_lst, dim=1)  # (bs, response_length, vocab_size)\n        prompts = idx[:, :prompt_length]  # (bs, prompt_length)\n        response = idx[:, prompt_length:]  # (bs, response_length)\n        log_probs = ", "metadata": {"task_id": "ZeroSearch/6", "ground_truth": "logprobs_from_logits(logits=logits, labels=response)\n", "fpath_tuple": ["ZeroSearch", "verl", "workers", "rollout", "naive", "naive_rollout.py"], "context_start_lineno": 0, "line_no": 104, "col_no": 20, "import_no": [29, 29]}}
{"prompt": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nMegatron Reward Model.\n\"\"\"\n\nfrom tensordict import TensorDict\nfrom functools import partial\nfrom verl import DataProto\nfrom verl.utils.torch_functional import logprobs_from_logits\nimport torch\nimport torch\nimport torch.distributed\n\nfrom verl.utils.megatron.pipeline_parallel import (compute_transformers_input_shapes, make_batch_generator)\nfrom verl import DataProto\nfrom verl.utils.torch_functional import logprobs_from_logits, broadcast_dict_tensor, split_dict_tensor_into_batches\nfrom verl.utils.torch_dtypes import PrecisionType\nfrom verl.workers.reward_model.base import BasePPORewardModel\nfrom verl.utils.megatron import sequence_parallel as sp_utils\nfrom megatron.core import parallel_state as mpu\nfrom megatron.core.pipeline_parallel import get_forward_backward_func\n\n\nclass MegatronRewardModel(BasePPORewardModel):\n\n    def __init__(self,\n                 config,\n                 model_config,\n                 reward_model_module: torch.nn.ModuleList,\n                 megatron_config,\n                 sft_tokenizer=None,\n                 rm_tokenizer=None):\n        self.config = config\n        self.reward_model_module = reward_model_module\n        self.megatron_config = megatron_config\n        self.model_config = model_config\n        self.device = 'cuda'\n        self.sft_tokenizer = sft_tokenizer\n        self.rm_tokenizer = rm_tokenizer\n        self.use_different_tokenizer = rm_tokenizer is not None\n\n        if self.config.param_offload:\n            self.offload_params_to_cpu()\n\n    def re_encode_by_rm_tokenizer(self, data: DataProto) -> DataProto:\n        assert self.use_different_tokenizer, 're-encode need rm tokenizer not be None!'\n        # need to use rm tokenizer to re-generate input_ids, attention_mask and position_ids\n        # 1. remove pad for each sequence\n        # 2. decode by sft_tokenizer, remove sft system prompts\n        # 3. encode by rm_tokenizer with rm system prompts, get rm_input_ids\n        # 4. generate attention_mask and position_ids\n        input_ids = data.batch['input_ids']  # (bs, seq_len)\n        attention_mask = data.batch['attention_mask']\n        position_ids = data.batch['position_ids']\n        ori_values = {'input_ids': input_ids, 'attention_mask': attention_mask, 'position_ids': position_ids}\n        ori_bs, ori_seqlen = input_ids.size(0), input_ids.size(1)\n        input_ids_for_rm = []\n        attention_mask_for_rm = []\n        position_ids_for_rm = []\n        print_decode = True\n        ori_seqlen = ori_seqlen + 128\n        for id, mask in zip(input_ids, attention_mask):\n            # 1. remove pad for each sequence\n            non_zero_indices = torch.nonzero(mask).view(-1)\n            begin_pos, end_pos = non_zero_indices[0].item(), non_zero_indices[-1].item()\n            valid_id = id[begin_pos:end_pos + 1]\n            # 2. decode by sft_tokenizer, remove sft system prompts\n            decode_result = self.sft_tokenizer.decode(valid_id)\n            # workaround\n            decode_with_rm_chat = decode_result.replace(\"<|user|>\\n\", \"[INST] \").replace(\n                \"</s>\\n<|assistant|>\\n\", \" [/INST]\").replace(\"</s> \\n<|assistant|>\\n\", \" [/INST]\") + \"</s>\"\n\n            print(f\"decode_with_rm_chat: {decode_with_rm_chat}\")\n            \n            if print_decode and torch.distributed.get_rank() == 0:\n                # only print first decode result\n                print(f'device {torch.cuda.current_device()}: sft decode result:\\n{decode_result}\\n \\\n                        \\ndevice {torch.cuda.current_device()}: sft decode result with rm chat template:\\n{decode_with_rm_chat}\\n\\n'\n                     )\n                print_decode = False\n            # 3. encode by rm_tokenizer\n            rm_input_ids = self.rm_tokenizer(decode_with_rm_chat,\n                                             return_tensors='pt')['input_ids'][0].to(input_ids.device)\n            # 4. generate attention_mask and position_ids\n            rm_attention_mask = torch.ones_like(rm_input_ids, device=input_ids.device)\n            cur_seqlen = rm_input_ids.shape[-1]\n            # NOTE(gh): the later reward compute will process the shape (bs, seqlen_pad_128)\n            if cur_seqlen > ori_seqlen:\n                print(f'warninig: rm encode seqlen {cur_seqlen} > sft encode seqlen {ori_seqlen}')\n                rm_input_ids = rm_input_ids[:ori_seqlen]\n                rm_attention_mask = rm_attention_mask[:ori_seqlen]\n            else:\n                # right padding\n                rm_input_ids = ", "metadata": {"task_id": "ZeroSearch/7", "ground_truth": "pad_sequence_to_length(rm_input_ids, ori_seqlen, self.rm_tokenizer.pad_token_id)\n", "fpath_tuple": ["ZeroSearch", "verl", "workers", "reward_model", "megatron", "reward_model.py"], "context_start_lineno": 0, "line_no": 106, "col_no": 31, "import_no": [25, 25]}}
{"prompt": "    \"\"\"\n\n    Args:\n        model_name:\n        override_config_kwargs:\n\n    Returns:\n\n    \"\"\"\n    critic_module: nn.Module = create_huggingface_actor(model_name,\n                                                        override_config_kwargs=override_config_kwargs,\n                                                        automodel_kwargs=automodel_kwargs)\n    if automodel_kwargs is None:\n        automodel_kwargs = {}\n    torch_dtype = automodel_kwargs.get('torch_dtype', torch.float32)\n    critic_module.lm_head = nn.Sequential(nn.Linear(critic_module.config.hidden_size, 1, dtype=torch_dtype),\n                                          LambdaLayer(fn=squeeze))\n    return critic_module\n\n\ndef get_model_size(model: nn.Module, scale='auto'):\n    n_params = sum(p.numel() for p in model.parameters())\n\n    if scale == 'auto':\n        if n_params > 1e9:\n            scale = 'B'\n        elif n_params > 1e6:\n            scale = 'M'\n        elif n_params > 1e3:\n            scale = 'K'\n        else:\n            scale = ''\n\n    if scale == 'B':\n        n_params = n_params / 1e9\n    elif scale == 'M':\n        n_params = n_params / 1e6\n    elif scale == 'K':\n        n_params = n_params / 1e3\n    elif scale == '':\n        pass\n    else:\n        raise NotImplemented(f'Unknown scale {scale}')\n\n    return n_params, scale\n\n\ndef print_model_size(model: nn.Module, name: str = None):\n    n_params, scale = get_model_size(model, scale='auto')\n    if name is None:\n        name = model.__class__.__name__\n    print(f'{name} contains {n_params:.2f}{scale} parameters')\n\n\ndef create_random_mask(input_ids: torch.Tensor,\n                       max_ratio_of_valid_token: float,\n                       max_ratio_of_left_padding: float,\n                       min_ratio_of_valid_token: float = 0):\n    \"\"\"Create a random mask given input_ids. Support left padding and right padding.\n    Process:\n    - Sample valid token length\n    - Sample left_padding length\n    - Generate padding\n\n    Args:\n        input_ids:\n            shape (batch_size, seq_len)\n\n    Returns:\n\n    \"\"\"\n    assert max_ratio_of_valid_token > 0 and max_ratio_of_valid_token <= 1.\n    assert max_ratio_of_left_padding >= 0 and max_ratio_of_left_padding < 1.\n    assert min_ratio_of_valid_token <= max_ratio_of_valid_token\n\n    batch_size, sequence_length = input_ids.shape\n    max_num_valid_tokens = int(sequence_length * max_ratio_of_valid_token)\n    min_num_valid_tokens = max(1, int(sequence_length * min_ratio_of_valid_token))\n    max_left_padding = int(sequence_length * max_ratio_of_left_padding)\n    assert max_num_valid_tokens + max_left_padding <= sequence_length\n    assert max_num_valid_tokens > 0 and max_ratio_of_valid_token <= sequence_length\n    masks = torch.ones_like(input_ids, dtype=torch.int64)\n    # TODO: we can make this faster\n    for i in range(batch_size):\n        num_left_padding = np.random.randint(low=0, high=max_left_padding + 1, dtype=np.int64)\n        num_valid = np.random.randint(low=min_num_valid_tokens, high=max_num_valid_tokens + 1, dtype=np.int64)\n\n        for index in range(num_left_padding):\n            masks[i, index] = 0\n\n        for index in range(num_left_padding + num_valid, sequence_length):\n            masks[i, index] = 0\n    return masks\n\n\ndef compute_position_id_with_mask(mask):\n    return torch.clip(torch.cumsum(mask, dim=-1) - 1, min=0, max=None)\n\n\ndef normalize_pp_vpp_params(params, num_hidden_layers, layer_name='layers'):\n    \"\"\"\n    Normalize the pp vpp params into a complete named parameters. \n    This is useful when gather parameters from pp ranks and passed to a model without pp\n\n    params: List[List[Dict[str, param]]]\n        params contains a list of pp, with a list of vpp named_parameters in each vpp chunk.\n    output: Dict[str, param]\n\n    \"\"\"\n\n    def normalize_model_name(name, pp_rank, vpp_rank, pp_size, vpp_size, num_layers):\n        \"\"\"\n        Transform the model name in each model_chunk in each pp stage into the name in inference engine\n        \"\"\"\n        if vpp_size > 1:\n            # print(f'try to bind vpp params to inference engine...')\n            layers_per_pp = num_layers // pp_size\n            layers_per_vpp = layers_per_pp // vpp_size\n            pp_offset = layers_per_vpp * pp_rank\n            vpp_offset = (layers_per_vpp * pp_size) * vpp_rank\n            layer_offset = pp_offset + vpp_offset\n        else:\n            layers_per_pp = num_layers // pp_size\n            layer_offset = layers_per_pp * pp_rank\n\n        if layer_name in name:  # belong to an intermediate layer\n            split_name = name.split('.')\n            # find the num next to split_name\n            for i, name in enumerate(split_name):\n                if name == layer_name:\n                    break\n            layer_num_idx = i + 1\n            # check the name\n            assert len(split_name) >= layer_num_idx + 1, f'split_name = {split_name}'\n            assert split_name[layer_num_idx].isdigit(), f'split_name = {split_name}'\n            # increment layer_num_idx by layer_offset\n            split_name[layer_num_idx] = str(int(split_name[layer_num_idx]) + layer_offset)\n            name = '.'.join(split_name)  # weight name in inference_tp_model\n        return name\n\n    pp_size = len(params)\n    normalized_name_to_param = {}\n    for pp_rank in range(len(params)):\n        vpp_size = len(params[pp_rank])\n        for vpp_rank in range(vpp_size):\n            for name, param in params[pp_rank][vpp_rank].items():\n                normalized_name = normalize_model_name(name, pp_rank, vpp_rank, pp_size, vpp_size, num_hidden_layers)\n                normalized_name_to_param[normalized_name] = param\n\n    return normalized_name_to_param\n\n\ndef get_parallel_model_from_config(config, megatron_config, pre_process=None, post_process=None, value=False):\n    from megatron.core import ModelParallelConfig\n    assert isinstance(megatron_config, ModelParallelConfig)\n    model_class = _get_parallel_model_architecture_from_config(config, value)\n\n    model = model_class(config, megatron_config, pre_process=pre_process, post_process=post_process)\n    return model\n\n\ndef _get_parallel_model_architecture_from_config(config: PretrainedConfig, value=False) -> Type[nn.Module]:\n    architectures = getattr(config, \"architectures\", [])\n    for arch in architectures:\n        model_cls = ", "metadata": {"task_id": "ZeroSearch/8", "ground_truth": "ModelRegistry.load_model_cls(arch, value)\n", "fpath_tuple": ["ZeroSearch", "verl", "utils", "model.py"], "context_start_lineno": 80, "line_no": 245, "col_no": 20, "import_no": [24, 24]}}
{"prompt": "    for model_module in model:\n        for param in model_module.parameters():\n            tensor_parallel.set_defaults_if_not_set_tensor_model_parallel_attributes(param)\n\n    # Print number of parameters.\n    if mpu.get_data_parallel_rank() == 0:\n        print(' > number of parameters on (tensor, pipeline) '\n              'model parallel rank ({}, {}): {}'.format(\n                  mpu.get_tensor_model_parallel_rank(), mpu.get_pipeline_model_parallel_rank(),\n                  sum([sum([p.nelement() for p in model_module.parameters()]) for model_module in model])),\n              flush=True)\n\n    # GPU allocation.\n    for model_module in model:\n        model_module.cuda(torch.cuda.current_device())\n\n    # Fp16 conversion.\n    config = get_model_config(model[0])\n    if config.fp16 or config.bf16:  # the ModelParallelConfig in GPTModel\n        model = [Float16Module(config, model_module) for model_module in model]\n\n    if wrap_with_ddp:\n        model = [\n            DDP(config=config,\n                module=model_chunk,\n                data_parallel_group=mpu.get_data_parallel_group(with_context_parallel=True),\n                accumulate_allreduce_grads_in_fp32=True,\n                overlap_grad_reduce=False,\n                use_distributed_optimizer=True,\n                disable_bucketing=(model_chunk_idx > 0)) for (model_chunk_idx, model_chunk) in enumerate(model)\n        ]\n        # # Broadcast params from data parallel src rank to other data parallel ranks.\n        # if args.data_parallel_random_init:\n        for model_module in model:\n            model_module.broadcast_params()\n    return model\n\n\nALL_MODULE_WRAPPER_CLASSNAMES = (DDP, Float16Module)\n\n\ndef unwrap_model(model, module_instances=ALL_MODULE_WRAPPER_CLASSNAMES):\n    return_list = True\n    if not isinstance(model, list):\n        model = [model]\n        return_list = False\n    unwrapped_model = []\n    for model_module in model:\n        while isinstance(model_module, module_instances):\n            model_module = model_module.module\n        unwrapped_model.append(model_module)\n    if not return_list:\n        return unwrapped_model[0]\n    return unwrapped_model\n\n\nfrom transformers import PretrainedConfig\n\n\ndef convert_config(hf_config: PretrainedConfig, megatron_config) -> TransformerConfig:\n    print(f'megatron config {megatron_config}')\n    dt = PrecisionType.to_dtype(megatron_config['param_dtype'])\n    print(f'pipeline_dtype=megatron_config {dt}')\n    transformer_config = TransformerConfig(\n        num_layers=hf_config.num_hidden_layers,\n        hidden_size=hf_config.hidden_size,\n        num_attention_heads=hf_config.num_attention_heads,\n        num_query_groups=hf_config.num_key_value_heads,\n        ffn_hidden_size=hf_config.intermediate_size,\n        #    max_position_embeddings=hf_config.max_position_embeddings,\n        activation_func=F.silu,\n        normalization='RMSNorm',\n        #    rotary_percent=False, # default,\n        gated_linear_unit=True,  # for llama\n        use_cpu_initialization=True,\n        apply_residual_connection_post_layernorm=False,  # check what's this mean\n        add_bias_linear=False,\n        tensor_model_parallel_size=mpu.get_tensor_model_parallel_world_size(),\n        pipeline_model_parallel_size=mpu.get_pipeline_model_parallel_world_size(),\n        virtual_pipeline_model_parallel_size=mpu.get_virtual_pipeline_model_parallel_world_size(),\n        pipeline_dtype=PrecisionType.to_dtype(megatron_config['param_dtype']),\n        params_dtype=PrecisionType.to_dtype(megatron_config['param_dtype']),\n        sequence_parallel=megatron_config['sequence_parallel_enabled'],\n        variable_seq_lengths=True,\n        masked_softmax_fusion=True,\n        bf16=PrecisionType.to_dtype(megatron_config['param_dtype']) is torch.bfloat16)\n    if torch.distributed.get_rank() == 0:\n        print(f'tensor_parallel_size={transformer_config.tensor_model_parallel_size} \\n \\\n                pipeline_model_parallel_size={transformer_config.pipeline_model_parallel_size} \\n \\\n                virtual_pipeline_model_parallel_size={transformer_config.virtual_pipeline_model_parallel_size} \\n \\\n                pipeline_dtype={transformer_config.pipeline_dtype} \\n \\\n                params_dtype={transformer_config.params_dtype} \\n \\\n                sequence_parallel={transformer_config.sequence_parallel} \\n \\\n                variable_seq_lengths={transformer_config.variable_seq_lengths} \\n \\\n                masked_softmax_fusion={transformer_config.masked_softmax_fusion} \\n ')\n\n    return transformer_config\n\n\n# from megatron.core.optimizer import OptimizerConfig\n\nfrom verl.utils.megatron.optimizer_config import OptimizerConfig\n\n\ndef init_megatron_optim_config(optim_config: Dict) -> OptimizerConfig:\n    config = OptimizerConfig(\n        optimizer='adam',\n        lr=optim_config.get('lr'),\n        clip_grad=optim_config.get('clip_grad'),\n        weight_decay=1e-2,\n        bf16=True,\n        params_dtype=torch.bfloat16,\n        use_distributed_optimizer=True,\n    )\n    return config\n\n\nfrom megatron.core import ModelParallelConfig\n\n\ndef init_model_parallel_config(config: DictConfig) -> ModelParallelConfig:\n    # TODO(sgm): check how to disable megatron timers\n    timers = FakeTimers()\n    return ModelParallelConfig(tensor_model_parallel_size=config.get('tensor_model_parallel_size'),\n                               pipeline_model_parallel_size=config.get('pipeline_model_parallel_size'),\n                               virtual_pipeline_model_parallel_size=config.get('virtual_pipeline_model_parallel_size'),\n                               sequence_parallel=config.get('sequence_parallel'),\n                               params_dtype=PrecisionType.to_dtype(config.get('param_dtype')),\n                               pipeline_dtype=PrecisionType.to_dtype(config.get('param_dtype')),\n                               bf16=True,\n                               fp16=False,\n                               timers=timers)\n\n\nclass FakeTimers:\n    \"\"\"Disable All Megatron Timing with FakeTimers\"\"\"\n\n    def __init__(self):\n        from megatron.timers import DummyTimer\n        self.dummy_timer = DummyTimer()\n\n    def __call__(self, *args: Any, **kwds: Any) -> Any:\n        return self.dummy_timer\n\n\ndef offload_megatron_param_and_grad(module_list: nn.ModuleList, offload_grad=False, hybrid_engine=None):\n    if hybrid_engine is not None:\n        pp_rank = mpu.get_pipeline_model_parallel_rank()\n        for buffer in hybrid_engine.memory_buffers[pp_rank].values():\n            buffer.data = buffer.data.to('cpu', non_blocking=True)\n", "metadata": {"task_id": "ZeroSearch/9", "ground_truth": "        build_memory_reference_from_module(module_list, hybrid_engine.memory_buffers[pp_rank], maintain_weight=True)\n", "fpath_tuple": ["ZeroSearch", "verl", "utils", "megatron_utils.py"], "context_start_lineno": 79, "line_no": 230, "col_no": -1, "import_no": [19, 19]}}
{"prompt": "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n# Copyright (c) 2024, NVIDIA CORPORATION. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom apex.optimizers import FusedAdam as Adam\nfrom apex.optimizers import FusedSGD as SGD\nfrom megatron.optimizer.distrib_optimizer import DistributedOptimizer\nfrom megatron.optimizer.grad_scaler import ConstantGradScaler, DynamicGradScaler\nfrom megatron.optimizer import Float16OptimizerWithFloat16Params, FP32Optimizer\n\nfrom verl.utils.megatron.optimizer_config import OptimizerConfig\n\n\ndef get_megatron_optimizer(\n        model,\n        config: OptimizerConfig,\n        no_weight_decay_cond=None,\n        scale_lr_cond=None,\n        lr_mult=1.0,\n        check_for_nan_in_loss_and_grad=False,\n        overlap_param_gather=False  # add for verl\n):\n    # Base optimizer.\n", "metadata": {"task_id": "ZeroSearch/10", "ground_truth": "    param_groups = get_param_groups(model, no_weight_decay_cond, scale_lr_cond, lr_mult)\n", "fpath_tuple": ["ZeroSearch", "verl", "utils", "megatron", "optimizer.py"], "context_start_lineno": 0, "line_no": 35, "col_no": -1, "import_no": [20, 20]}}
